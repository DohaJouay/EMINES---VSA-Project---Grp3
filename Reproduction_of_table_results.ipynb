{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YM9HqyFvopHg",
        "outputId": "d10f9369-1e5f-4450-edee-33ea49f0a2f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Hyperdimensional-Computing'...\n",
            "remote: Enumerating objects: 30, done.\u001b[K\n",
            "remote: Counting objects: 100% (3/3), done.\u001b[K\n",
            "remote: Compressing objects: 100% (3/3), done.\u001b[K\n",
            "remote: Total 30 (delta 0), reused 1 (delta 0), pack-reused 27 (from 1)\u001b[K\n",
            "Receiving objects: 100% (30/30), 48.17 MiB | 17.81 MiB/s, done.\n",
            "Resolving deltas: 100% (6/6), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/Cornell-RelaxML/Hyperdimensional-Computing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/Hyperdimensional-Computing"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "el3cYHXLov9p",
        "outputId": "6fa78b6d-6101-4ac0-d3ee-fc5ebb3f3e41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Hyperdimensional-Computing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing librairies and Functions' definition"
      ],
      "metadata": {
        "id": "PnpILxpRzqik"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "from utils import prepare_data, encode_and_save\n",
        "from model import BModel, GModel\n",
        "import argparse\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ],
      "metadata": {
        "id": "wTGgVfPPoyr4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(MODEL, loader, criterion, device, model_='rff-hdc'):\n",
        "    MODEL.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for i, data in enumerate(loader, 0):\n",
        "            inputs, labels = data[0].to(device), data[1].to(device)\n",
        "            if model_ == 'rff-hdc':\n",
        "                outputs = MODEL(2 * inputs - 1)\n",
        "            else:\n",
        "                outputs = MODEL(inputs)\n",
        "            test_loss += criterion(outputs, labels)\n",
        "            preds = outputs.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct += preds.eq(labels.view_as(preds)).sum().item()\n",
        "    test_loss /= len(loader.dataset)\n",
        "    print('\\nTesting Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
        "        test_loss, correct, len(loader.dataset),\n",
        "        100. * correct / len(loader.dataset)))"
      ],
      "metadata": {
        "id": "Jyx8LFTvo0FE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(args):\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    # criterion = torch.nn.NLLLoss()\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    channels = 3 if args.dataset == 'cifar' else 1\n",
        "    if args.dataset == 'isolet':\n",
        "        classes = 26\n",
        "    elif args.dataset == 'ucihar':\n",
        "        classes = 6\n",
        "    else:\n",
        "        classes = 10\n",
        "    if 'hdc' in args.model:\n",
        "        model = BModel(in_dim=channels * args.dim, classes=classes).to(device)\n",
        "    elif 'percep' in args.model:\n",
        "        model = nn.Sequential(nn.Linear(10000, classes)).to(device)\n",
        "    else:\n",
        "        model = GModel(args.gorder, in_dim=channels * args.dim, classes=classes).to(device)\n",
        "\n",
        "    trainloader, testloader = prepare_data(args)\n",
        "    #     optimizer = torch.optim.SGD(model.parameters(), lr=args.lr)#, momentum=0.9, weight_decay=1.0e-5)\n",
        "    #     optimizer = torch.optim.Adadelta(model.parameters(), lr=args.lr)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
        "\n",
        "    for epoch in range(args.epoch):\n",
        "        print(\"Epoch:\", epoch + 1)\n",
        "        model.train()\n",
        "        start_time = time.time()\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            inputs, labels = data[0].to(device), data[1].to(device)\n",
        "            optimizer.zero_grad()\n",
        "            if args.model == 'rff-hdc':\n",
        "                outputs = model(2 * inputs - 1)\n",
        "            else:\n",
        "                outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            _, batch_predicted = torch.max(outputs.data, 1)\n",
        "            batch_accu = 100.0 * (batch_predicted == labels).sum().item() / labels.size(0)\n",
        "            if i % 50 == 49:\n",
        "                print(i, \"{0:.4f}\".format(loss.item()), batch_accu)\n",
        "        print('Start testing on test set')\n",
        "        test(model, testloader, criterion, device, model_=args.model)\n",
        "        print(\"--- %s seconds ---\" % (time.time() - start_time))\n"
      ],
      "metadata": {
        "id": "QXYGutTRo4La"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Perceptron RFF on the different data sets"
      ],
      "metadata": {
        "id": "VHqhPgh_pW-p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ISOLET"
      ],
      "metadata": {
        "id": "CFb7m26vumTZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#isolet epoch 1 percep\n",
        "class ArgumentParser:\n",
        "    def __init__(self):\n",
        "        self.args = {\n",
        "            'lr': 0.01,            # Learning rate for optimizing class representative\n",
        "            'gamma': 0.3,          # Kernel parameter for computing covariance\n",
        "            'epoch': 1,            # Epochs of training\n",
        "            'gorder': 16,           # Order of the cyclic group required for G-VSA (8 ou 16)\n",
        "            'dim': 10000,          # Dimension of hypervectors c le grand D\n",
        "            'seed': 43,            # Random seed for reproducing results\n",
        "            'resume': False,       # Resume from existing encoded hypervectors\n",
        "            'data_dir': './encoded_data', # Directory to save encoded data (hypervectors)\n",
        "            'dataset': 'isolet',    # Dataset choice choices=['mnist', 'fmnist', 'cifar', 'isolet', 'ucihar']\n",
        "            'raw_data_dir': './dataset',   # Raw data directory\n",
        "            'model': 'percep-rff'    # Model type choices=['rff-hdc', 'linear-hdc', 'rff-gvsa']\n",
        "        }\n",
        "        self.seed = self.args['seed']\n",
        "        self.lr = self.args['lr']\n",
        "        self.gamma = self.args['gamma']\n",
        "        self.epoch = self.args['epoch']\n",
        "        self.gorder = self.args['gorder']\n",
        "        self.dim = self.args['dim']\n",
        "        self.resume = self.args['resume']\n",
        "        self.data_dir = self.args['data_dir']\n",
        "        self.dataset = self.args['dataset']\n",
        "        self.raw_data_dir = self.args['raw_data_dir']\n",
        "        self.model = self.args['model']\n",
        "\n",
        "\n",
        "    def get_args(self):\n",
        "        return self.args\n",
        "\n",
        "args = ArgumentParser()\n",
        "torch.manual_seed(args.seed)\n",
        "np.random.seed(args.seed)\n",
        "if 'hdc' in args.model:\n",
        "    args.gorder = 2\n",
        "    print(\"Use binary HDC with random fourier features, ignoring gorder, set to 2.\")\n",
        "args.data_dir = f'{args.data_dir}/{args.dataset}_{args.model}_order{args.gorder}_gamma{args.gamma}_dim{args.dim}'\n",
        "try:\n",
        "    os.makedirs(args.data_dir)\n",
        "except FileExistsError:\n",
        "    print('Encoded data folder already exists')\n",
        "if not args.resume:\n",
        "    print('Encode the dataset into hypervectors and save')\n",
        "    encode_and_save(args)\n",
        "    print('Finish encoding and saving')\n",
        "print(f'Optimizing class representatives for {args.epoch} epochs')\n",
        "train(args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O-p2yJdRtOMI",
        "outputId": "d0b9f95b-c226-4dea-f0c7-1c56764e9b83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoded data folder already exists\n",
            "Encode the dataset into hypervectors and save\n",
            "# of channels of data 1\n",
            "# of training samples and test samples 6238 1559\n",
            "Encoding with random fourier features encoder.\n",
            "the threshold to discretize fourier features to group elements tensor([-1.5343, -1.1505, -0.8872, -0.6746, -0.4888, -0.3187, -0.1573,  0.0000,\n",
            "         0.1573,  0.3187,  0.4888,  0.6746,  0.8872,  1.1505,  1.5343])\n",
            "Encoded pixels to hypervectors with size:  torch.Size([256, 10000])\n",
            "Encoding training data...\n",
            "Start encoding data\n",
            "Finish encoding data\n",
            "Encoding test data...\n",
            "Start encoding data\n",
            "Finish encoding data\n",
            "Finish encoding and saving\n",
            "Optimizing class representatives for 1 epochs\n",
            "Loading encoded training data...\n",
            "Loading encoded test data...\n",
            "Size of encoded training data torch.Size([6238, 10000]) and test data torch.Size([1559, 10000])\n",
            "Epoch: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/Hyperdimensional-Computing/utils.py:116: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  train_hd = torch.load(f'{args.data_dir}/train_hd.pt')\n",
            "/content/Hyperdimensional-Computing/utils.py:117: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  y_train = torch.load(f'{args.data_dir}/y_train.pt')\n",
            "/content/Hyperdimensional-Computing/utils.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  test_hd = torch.load(f'{args.data_dir}/test_hd.pt')\n",
            "/content/Hyperdimensional-Computing/utils.py:121: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  y_test = torch.load(f'{args.data_dir}/y_test.pt')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "49 995858.8750 6.25\n",
            "99 526116.8125 6.25\n",
            "149 809939.2500 6.25\n",
            "199 913873.3750 0.0\n",
            "249 697573.6250 0.0\n",
            "299 453530.5000 0.0\n",
            "349 403353.8125 0.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 48020.4922, Accuracy: 60/1559 (3.85%)\n",
            "\n",
            "--- 2.5029330253601074 seconds ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#isolet Percep epoch 10\n",
        "class ArgumentParser:\n",
        "    def __init__(self):\n",
        "        self.args = {\n",
        "            'lr': 0.01,            # Learning rate for optimizing class representative\n",
        "            'gamma': 0.3,          # Kernel parameter for computing covariance\n",
        "            'epoch': 10,            # Epochs of training\n",
        "            'gorder': 16,           # Order of the cyclic group required for G-VSA (8 ou 16)\n",
        "            'dim': 10000,          # Dimension of hypervectors c le grand D\n",
        "            'seed': 43,            # Random seed for reproducing results\n",
        "            'resume': True,       # Resume from existing encoded hypervectors\n",
        "            'data_dir': './encoded_data', # Directory to save encoded data (hypervectors)\n",
        "            'dataset': 'isolet',    # Dataset choice choices=['mnist', 'fmnist', 'cifar', 'isolet', 'ucihar']\n",
        "            'raw_data_dir': './dataset',   # Raw data directory\n",
        "            'model': 'percep-rff'    # Model type choices=['rff-hdc', 'linear-hdc', 'rff-gvsa']\n",
        "        }\n",
        "        self.seed = self.args['seed']\n",
        "        self.lr = self.args['lr']\n",
        "        self.gamma = self.args['gamma']\n",
        "        self.epoch = self.args['epoch']\n",
        "        self.gorder = self.args['gorder']\n",
        "        self.dim = self.args['dim']\n",
        "        self.resume = self.args['resume']\n",
        "        self.data_dir = self.args['data_dir']\n",
        "        self.dataset = self.args['dataset']\n",
        "        self.raw_data_dir = self.args['raw_data_dir']\n",
        "        self.model = self.args['model']\n",
        "\n",
        "\n",
        "    def get_args(self):\n",
        "        return self.args\n",
        "\n",
        "args = ArgumentParser()\n",
        "torch.manual_seed(args.seed)\n",
        "np.random.seed(args.seed)\n",
        "if 'hdc' in args.model:\n",
        "    args.gorder = 2\n",
        "    print(\"Use binary HDC with random fourier features, ignoring gorder, set to 2.\")\n",
        "args.data_dir = f'{args.data_dir}/{args.dataset}_{args.model}_order{args.gorder}_gamma{args.gamma}_dim{args.dim}'\n",
        "try:\n",
        "    os.makedirs(args.data_dir)\n",
        "except FileExistsError:\n",
        "    print('Encoded data folder already exists')\n",
        "if not args.resume:\n",
        "    print('Encode the dataset into hypervectors and save')\n",
        "    encode_and_save(args)\n",
        "    print('Finish encoding and saving')\n",
        "print(f'Optimizing class representatives for {args.epoch} epochs')\n",
        "train(args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1MBMJ6OwuInV",
        "outputId": "0ceeae74-d75e-45a2-93e6-1e131bdfa587"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoded data folder already exists\n",
            "Optimizing class representatives for 10 epochs\n",
            "Loading encoded training data...\n",
            "Loading encoded test data...\n",
            "Size of encoded training data torch.Size([6238, 10000]) and test data torch.Size([1559, 10000])\n",
            "Epoch: 1\n",
            "49 868007.8750 6.25\n",
            "99 827970.8125 0.0\n",
            "149 425642.5000 0.0\n",
            "199 330255.8125 12.5\n",
            "249 561979.3750 0.0\n",
            "299 686604.0000 0.0\n",
            "349 523490.1250 0.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 36815.3633, Accuracy: 60/1559 (3.85%)\n",
            "\n",
            "--- 1.6314761638641357 seconds ---\n",
            "Epoch: 2\n",
            "49 840174.5000 6.25\n",
            "99 665619.8750 0.0\n",
            "149 400420.2500 6.25\n",
            "199 423796.6250 6.25\n",
            "249 832145.0000 6.25\n",
            "299 762263.2500 0.0\n",
            "349 597408.7500 0.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 41860.8242, Accuracy: 60/1559 (3.85%)\n",
            "\n",
            "--- 1.6185083389282227 seconds ---\n",
            "Epoch: 3\n",
            "49 756467.9375 0.0\n",
            "99 657288.5625 0.0\n",
            "149 369311.8438 0.0\n",
            "199 405602.5938 6.25\n",
            "249 923299.7500 0.0\n",
            "299 692592.8125 12.5\n",
            "349 468610.5625 0.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 37269.1211, Accuracy: 60/1559 (3.85%)\n",
            "\n",
            "--- 1.9676141738891602 seconds ---\n",
            "Epoch: 4\n",
            "49 403068.1875 0.0\n",
            "99 929790.3750 0.0\n",
            "149 835156.6875 0.0\n",
            "199 651944.2500 0.0\n",
            "249 487248.6875 0.0\n",
            "299 596799.1250 0.0\n",
            "349 434238.1875 0.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 35308.1641, Accuracy: 60/1559 (3.85%)\n",
            "\n",
            "--- 2.2608988285064697 seconds ---\n",
            "Epoch: 5\n",
            "49 736283.4375 0.0\n",
            "99 562972.5000 0.0\n",
            "149 373988.5625 0.0\n",
            "199 325681.7188 0.0\n",
            "249 686129.6875 6.25\n",
            "299 472170.0938 0.0\n",
            "349 387502.6875 0.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 34636.8594, Accuracy: 60/1559 (3.85%)\n",
            "\n",
            "--- 1.616708517074585 seconds ---\n",
            "Epoch: 6\n",
            "49 729321.9375 0.0\n",
            "99 701512.3125 6.25\n",
            "149 646254.2500 6.25\n",
            "199 512328.3438 12.5\n",
            "249 711486.5625 0.0\n",
            "299 720646.5000 0.0\n",
            "349 587913.8750 0.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 46199.9023, Accuracy: 60/1559 (3.85%)\n",
            "\n",
            "--- 1.621617078781128 seconds ---\n",
            "Epoch: 7\n",
            "49 923802.7500 12.5\n",
            "99 510471.8125 0.0\n",
            "149 516566.5938 0.0\n",
            "199 576123.1875 0.0\n",
            "249 486904.3438 6.25\n",
            "299 645736.1875 0.0\n",
            "349 1031460.3750 0.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 41058.6289, Accuracy: 60/1559 (3.85%)\n",
            "\n",
            "--- 1.6339702606201172 seconds ---\n",
            "Epoch: 8\n",
            "49 398503.3125 12.5\n",
            "99 428943.7500 12.5\n",
            "149 453774.1562 6.25\n",
            "199 255939.3438 6.25\n",
            "249 666626.2500 0.0\n",
            "299 628760.3750 0.0\n",
            "349 691331.1875 0.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 21807.2676, Accuracy: 60/1559 (3.85%)\n",
            "\n",
            "--- 1.6370809078216553 seconds ---\n",
            "Epoch: 9\n",
            "49 597445.7500 6.25\n",
            "99 587710.6250 0.0\n",
            "149 589442.3125 6.25\n",
            "199 804026.8750 0.0\n",
            "249 642914.7500 0.0\n",
            "299 458772.2500 6.25\n",
            "349 672164.6875 6.25\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 36128.6758, Accuracy: 60/1559 (3.85%)\n",
            "\n",
            "--- 1.6431376934051514 seconds ---\n",
            "Epoch: 10\n",
            "49 795742.6250 12.5\n",
            "99 567053.1250 12.5\n",
            "149 848451.8125 0.0\n",
            "199 818178.1250 0.0\n",
            "249 809749.1250 0.0\n",
            "299 691677.8125 6.25\n",
            "349 628536.9375 6.25\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 54673.2070, Accuracy: 60/1559 (3.85%)\n",
            "\n",
            "--- 1.8069345951080322 seconds ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## UCIHAR\n"
      ],
      "metadata": {
        "id": "xusmekHxu-7f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#UCIHAR Percep epoch 1\n",
        "class ArgumentParser:\n",
        "    def __init__(self):\n",
        "        self.args = {\n",
        "            'lr': 0.01,            # Learning rate for optimizing class representative\n",
        "            'gamma': 0.3,          # Kernel parameter for computing covariance\n",
        "            'epoch': 1,            # Epochs of training\n",
        "            'gorder': 16,           # Order of the cyclic group required for G-VSA (8 ou 16)\n",
        "            'dim': 10000,          # Dimension of hypervectors c le grand D\n",
        "            'seed': 43,            # Random seed for reproducing results\n",
        "            'resume': False,       # Resume from existing encoded hypervectors\n",
        "            'data_dir': './encoded_data', # Directory to save encoded data (hypervectors)\n",
        "            'dataset': 'ucihar',    # Dataset choice choices=['mnist', 'fmnist', 'cifar', 'isolet', 'ucihar']\n",
        "            'raw_data_dir': './dataset',   # Raw data directory\n",
        "            'model': 'percep-rff'    # Model type choices=['rff-hdc', 'linear-hdc', 'rff-gvsa']\n",
        "        }\n",
        "        self.seed = self.args['seed']\n",
        "        self.lr = self.args['lr']\n",
        "        self.gamma = self.args['gamma']\n",
        "        self.epoch = self.args['epoch']\n",
        "        self.gorder = self.args['gorder']\n",
        "        self.dim = self.args['dim']\n",
        "        self.resume = self.args['resume']\n",
        "        self.data_dir = self.args['data_dir']\n",
        "        self.dataset = self.args['dataset']\n",
        "        self.raw_data_dir = self.args['raw_data_dir']\n",
        "        self.model = self.args['model']\n",
        "\n",
        "\n",
        "    def get_args(self):\n",
        "        return self.args\n",
        "\n",
        "args = ArgumentParser()\n",
        "torch.manual_seed(args.seed)\n",
        "np.random.seed(args.seed)\n",
        "if 'hdc' in args.model:\n",
        "    args.gorder = 2\n",
        "    print(\"Use binary HDC with random fourier features, ignoring gorder, set to 2.\")\n",
        "args.data_dir = f'{args.data_dir}/{args.dataset}_{args.model}_order{args.gorder}_gamma{args.gamma}_dim{args.dim}'\n",
        "try:\n",
        "    os.makedirs(args.data_dir)\n",
        "except FileExistsError:\n",
        "    print('Encoded data folder already exists')\n",
        "if not args.resume:\n",
        "    print('Encode the dataset into hypervectors and save')\n",
        "    encode_and_save(args)\n",
        "    print('Finish encoding and saving')\n",
        "print(f'Optimizing class representatives for {args.epoch} epochs')\n",
        "train(args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-XU6ykkJvFIG",
        "outputId": "c6e30f93-29c8-4745-f605-1dde8581b234"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encode the dataset into hypervectors and save\n",
            "# of channels of data 1\n",
            "# of training samples and test samples 7352 2947\n",
            "Encoding with random fourier features encoder.\n",
            "the threshold to discretize fourier features to group elements tensor([-1.5343, -1.1505, -0.8872, -0.6746, -0.4888, -0.3187, -0.1573,  0.0000,\n",
            "         0.1573,  0.3187,  0.4888,  0.6746,  0.8872,  1.1505,  1.5343])\n",
            "Encoded pixels to hypervectors with size:  torch.Size([256, 10000])\n",
            "Encoding training data...\n",
            "Start encoding data\n",
            "Finish encoding data\n",
            "Encoding test data...\n",
            "Start encoding data\n",
            "Finish encoding data\n",
            "Finish encoding and saving\n",
            "Optimizing class representatives for 1 epochs\n",
            "Loading encoded training data...\n",
            "Loading encoded test data...\n",
            "Size of encoded training data torch.Size([7352, 10000]) and test data torch.Size([2947, 10000])\n",
            "Epoch: 1\n",
            "49 246569.7500 25.0\n",
            "99 74794.5000 6.25\n",
            "149 286504.1562 12.5\n",
            "199 158004.5469 25.0\n",
            "249 195212.6250 12.5\n",
            "299 85235.6719 18.75\n",
            "349 95910.5625 18.75\n",
            "399 135563.7812 25.0\n",
            "449 180684.5938 12.5\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 17608.4492, Accuracy: 491/2947 (16.66%)\n",
            "\n",
            "--- 2.1132795810699463 seconds ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#UCIHAR Percep epoch 10\n",
        "class ArgumentParser:\n",
        "    def __init__(self):\n",
        "        self.args = {\n",
        "            'lr': 0.01,            # Learning rate for optimizing class representative\n",
        "            'gamma': 0.3,          # Kernel parameter for computing covariance\n",
        "            'epoch': 10,            # Epochs of training\n",
        "            'gorder': 16,           # Order of the cyclic group required for G-VSA (8 ou 16)\n",
        "            'dim': 10000,          # Dimension of hypervectors c le grand D\n",
        "            'seed': 43,            # Random seed for reproducing results\n",
        "            'resume': True,       # Resume from existing encoded hypervectors\n",
        "            'data_dir': './encoded_data', # Directory to save encoded data (hypervectors)\n",
        "            'dataset': 'ucihar',    # Dataset choice choices=['mnist', 'fmnist', 'cifar', 'isolet', 'ucihar']\n",
        "            'raw_data_dir': './dataset',   # Raw data directory\n",
        "            'model': 'percep-rff'    # Model type choices=['rff-hdc', 'linear-hdc', 'rff-gvsa']\n",
        "        }\n",
        "        self.seed = self.args['seed']\n",
        "        self.lr = self.args['lr']\n",
        "        self.gamma = self.args['gamma']\n",
        "        self.epoch = self.args['epoch']\n",
        "        self.gorder = self.args['gorder']\n",
        "        self.dim = self.args['dim']\n",
        "        self.resume = self.args['resume']\n",
        "        self.data_dir = self.args['data_dir']\n",
        "        self.dataset = self.args['dataset']\n",
        "        self.raw_data_dir = self.args['raw_data_dir']\n",
        "        self.model = self.args['model']\n",
        "\n",
        "\n",
        "    def get_args(self):\n",
        "        return self.args\n",
        "\n",
        "args = ArgumentParser()\n",
        "torch.manual_seed(args.seed)\n",
        "np.random.seed(args.seed)\n",
        "if 'hdc' in args.model:\n",
        "    args.gorder = 2\n",
        "    print(\"Use binary HDC with random fourier features, ignoring gorder, set to 2.\")\n",
        "args.data_dir = f'{args.data_dir}/{args.dataset}_{args.model}_order{args.gorder}_gamma{args.gamma}_dim{args.dim}'\n",
        "try:\n",
        "    os.makedirs(args.data_dir)\n",
        "except FileExistsError:\n",
        "    print('Encoded data folder already exists')\n",
        "if not args.resume:\n",
        "    print('Encode the dataset into hypervectors and save')\n",
        "    encode_and_save(args)\n",
        "    print('Finish encoding and saving')\n",
        "print(f'Optimizing class representatives for {args.epoch} epochs')\n",
        "train(args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ZjModWNvK8O",
        "outputId": "61a26c61-6201-41ee-d983-ae5c8167d3ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoded data folder already exists\n",
            "Optimizing class representatives for 10 epochs\n",
            "Loading encoded training data...\n",
            "Loading encoded test data...\n",
            "Size of encoded training data torch.Size([7352, 10000]) and test data torch.Size([2947, 10000])\n",
            "Epoch: 1\n",
            "49 147516.7031 0.0\n",
            "99 232809.3438 25.0\n",
            "149 165723.8281 37.5\n",
            "199 282995.5625 6.25\n",
            "249 166385.8594 18.75\n",
            "299 301754.4062 18.75\n",
            "349 137537.0156 18.75\n",
            "399 220966.9688 12.5\n",
            "449 168522.3750 37.5\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 17324.5273, Accuracy: 496/2947 (16.83%)\n",
            "\n",
            "--- 2.063068389892578 seconds ---\n",
            "Epoch: 2\n",
            "49 47441.6875 31.25\n",
            "99 224030.3750 6.25\n",
            "149 179364.0625 12.5\n",
            "199 158497.4062 6.25\n",
            "249 72886.6641 12.5\n",
            "299 66122.3203 12.5\n",
            "349 265680.1250 31.25\n",
            "399 237218.3750 18.75\n",
            "449 227155.3750 6.25\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 5479.3911, Accuracy: 532/2947 (18.05%)\n",
            "\n",
            "--- 2.035463333129883 seconds ---\n",
            "Epoch: 3\n",
            "49 170961.6562 25.0\n",
            "99 160176.5000 6.25\n",
            "149 218127.7188 0.0\n",
            "199 259056.4688 6.25\n",
            "249 287386.6250 12.5\n",
            "299 199135.6250 12.5\n",
            "349 131050.4297 6.25\n",
            "399 86548.0312 12.5\n",
            "449 149652.3125 25.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 20758.3145, Accuracy: 537/2947 (18.22%)\n",
            "\n",
            "--- 2.062208414077759 seconds ---\n",
            "Epoch: 4\n",
            "49 65949.1719 25.0\n",
            "99 289686.6875 6.25\n",
            "149 207271.1875 18.75\n",
            "199 219851.0312 25.0\n",
            "249 256612.5312 6.25\n",
            "299 305120.9375 6.25\n",
            "349 318087.1875 12.5\n",
            "399 247089.7969 6.25\n",
            "449 424520.1875 12.5\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 14202.7705, Accuracy: 496/2947 (16.83%)\n",
            "\n",
            "--- 2.7334680557250977 seconds ---\n",
            "Epoch: 5\n",
            "49 80079.9062 43.75\n",
            "99 123758.9453 0.0\n",
            "149 293984.6250 6.25\n",
            "199 166803.3125 6.25\n",
            "249 250043.1562 12.5\n",
            "299 122253.1016 12.5\n",
            "349 294641.7188 25.0\n",
            "399 332270.4375 12.5\n",
            "449 146326.7500 6.25\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 13553.5000, Accuracy: 496/2947 (16.83%)\n",
            "\n",
            "--- 2.168893337249756 seconds ---\n",
            "Epoch: 6\n",
            "49 217893.9219 12.5\n",
            "99 130446.1875 18.75\n",
            "149 131553.5625 37.5\n",
            "199 74485.6328 37.5\n",
            "249 288955.3438 12.5\n",
            "299 275695.5312 12.5\n",
            "349 155891.3125 18.75\n",
            "399 107886.4922 6.25\n",
            "449 215833.8438 12.5\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 17706.2617, Accuracy: 532/2947 (18.05%)\n",
            "\n",
            "--- 2.042599678039551 seconds ---\n",
            "Epoch: 7\n",
            "49 209278.2969 0.0\n",
            "99 190442.5469 31.25\n",
            "149 60840.7656 6.25\n",
            "199 87483.9453 18.75\n",
            "249 214750.4688 12.5\n",
            "299 182512.9844 12.5\n",
            "349 125602.8203 6.25\n",
            "399 297687.0312 12.5\n",
            "449 164785.4531 6.25\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 8393.0850, Accuracy: 496/2947 (16.83%)\n",
            "\n",
            "--- 2.0499260425567627 seconds ---\n",
            "Epoch: 8\n",
            "49 204856.7344 12.5\n",
            "99 183964.0000 12.5\n",
            "149 224191.9531 31.25\n",
            "199 116412.3594 6.25\n",
            "249 156919.3438 25.0\n",
            "299 273313.4688 31.25\n",
            "349 302405.9375 0.0\n",
            "399 184162.1562 18.75\n",
            "449 205224.7188 12.5\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 5129.3125, Accuracy: 491/2947 (16.66%)\n",
            "\n",
            "--- 2.0589709281921387 seconds ---\n",
            "Epoch: 9\n",
            "49 172605.3750 12.5\n",
            "99 296912.2500 31.25\n",
            "149 217500.5156 6.25\n",
            "199 117388.7734 18.75\n",
            "249 248055.2188 6.25\n",
            "299 174283.4062 12.5\n",
            "349 38333.5234 25.0\n",
            "399 130794.7109 18.75\n",
            "449 105837.0078 18.75\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 12326.8955, Accuracy: 491/2947 (16.66%)\n",
            "\n",
            "--- 2.108926773071289 seconds ---\n",
            "Epoch: 10\n",
            "49 99753.7188 37.5\n",
            "99 166708.1719 18.75\n",
            "149 61752.1797 18.75\n",
            "199 225443.5938 12.5\n",
            "249 230357.8125 0.0\n",
            "299 140290.2188 18.75\n",
            "349 200455.1875 12.5\n",
            "399 168934.4688 18.75\n",
            "449 288149.2188 12.5\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 14875.1152, Accuracy: 532/2947 (18.05%)\n",
            "\n",
            "--- 2.885601043701172 seconds ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MNIST"
      ],
      "metadata": {
        "id": "mquJY9S_ugl7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#mnist Percep epoch 1\n",
        "class ArgumentParser:\n",
        "    def __init__(self):\n",
        "        self.args = {\n",
        "            'lr': 0.01,            # Learning rate for optimizing class representative\n",
        "            'gamma': 0.3,          # Kernel parameter for computing covariance\n",
        "            'epoch': 1,            # Epochs of training\n",
        "            'gorder': 16,           # Order of the cyclic group required for G-VSA (8 ou 16)\n",
        "            'dim': 10000,          # Dimension of hypervectors c le grand D\n",
        "            'seed': 43,            # Random seed for reproducing results\n",
        "            'resume': False,       # Resume from existing encoded hypervectors\n",
        "            'data_dir': './encoded_data', # Directory to save encoded data (hypervectors)\n",
        "            'dataset': 'mnist',    # Dataset choice choices=['mnist', 'fmnist', 'cifar', 'isolet', 'ucihar']\n",
        "            'raw_data_dir': './dataset',   # Raw data directory\n",
        "            'model': 'percep-rff'    # Model type choices=['rff-hdc', 'linear-hdc', 'rff-gvsa']\n",
        "        }\n",
        "        self.seed = self.args['seed']\n",
        "        self.lr = self.args['lr']\n",
        "        self.gamma = self.args['gamma']\n",
        "        self.epoch = self.args['epoch']\n",
        "        self.gorder = self.args['gorder']\n",
        "        self.dim = self.args['dim']\n",
        "        self.resume = self.args['resume']\n",
        "        self.data_dir = self.args['data_dir']\n",
        "        self.dataset = self.args['dataset']\n",
        "        self.raw_data_dir = self.args['raw_data_dir']\n",
        "        self.model = self.args['model']\n",
        "\n",
        "\n",
        "    def get_args(self):\n",
        "        return self.args\n",
        "\n",
        "args = ArgumentParser()\n",
        "torch.manual_seed(args.seed)\n",
        "np.random.seed(args.seed)\n",
        "if 'hdc' in args.model:\n",
        "    args.gorder = 2\n",
        "    print(\"Use binary HDC with random fourier features, ignoring gorder, set to 2.\")\n",
        "args.data_dir = f'{args.data_dir}/{args.dataset}_{args.model}_order{args.gorder}_gamma{args.gamma}_dim{args.dim}'\n",
        "try:\n",
        "    os.makedirs(args.data_dir)\n",
        "except FileExistsError:\n",
        "    print('Encoded data folder already exists')\n",
        "if not args.resume:\n",
        "    print('Encode the dataset into hypervectors and save')\n",
        "    encode_and_save(args)\n",
        "    print('Finish encoding and saving')\n",
        "print(f'Optimizing class representatives for {args.epoch} epochs')\n",
        "train(args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bnf8thtBpyO3",
        "outputId": "20f8dca7-4f51-40a2-ac63-63b3782674e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encode the dataset into hypervectors and save\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./dataset/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 17.8MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./dataset/MNIST/raw/train-images-idx3-ubyte.gz to ./dataset/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./dataset/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 486kB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./dataset/MNIST/raw/train-labels-idx1-ubyte.gz to ./dataset/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./dataset/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 4.49MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./dataset/MNIST/raw/t10k-images-idx3-ubyte.gz to ./dataset/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./dataset/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 4.55MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./dataset/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./dataset/MNIST/raw\n",
            "\n",
            "# of channels of data 1\n",
            "# of training samples and test samples 60000 10000\n",
            "Encoding with random fourier features encoder.\n",
            "the threshold to discretize fourier features to group elements tensor([-1.5343, -1.1505, -0.8872, -0.6746, -0.4888, -0.3187, -0.1573,  0.0000,\n",
            "         0.1573,  0.3187,  0.4888,  0.6746,  0.8872,  1.1505,  1.5343])\n",
            "Encoded pixels to hypervectors with size:  torch.Size([256, 10000])\n",
            "Encoding training data...\n",
            "Start encoding data\n",
            "12800 images encoded. Total time elapse = 23.742467164993286\n",
            "25600 images encoded. Total time elapse = 46.900729179382324\n",
            "38400 images encoded. Total time elapse = 69.94405055046082\n",
            "51200 images encoded. Total time elapse = 93.06130862236023\n",
            "Finish encoding data\n",
            "Encoding test data...\n",
            "Start encoding data\n",
            "Finish encoding data\n",
            "Finish encoding and saving\n",
            "Optimizing class representatives for 1 epochs\n",
            "Loading encoded training data...\n",
            "Loading encoded test data...\n",
            "Size of encoded training data torch.Size([60000, 10000]) and test data torch.Size([10000, 10000])\n",
            "Epoch: 1\n",
            "49 281023.0312 18.75\n",
            "99 370968.4375 6.25\n",
            "149 343798.6875 6.25\n",
            "199 259261.1875 31.25\n",
            "249 449003.0938 18.75\n",
            "299 310882.1250 6.25\n",
            "349 740871.0000 0.0\n",
            "399 340309.1250 18.75\n",
            "449 495744.0625 6.25\n",
            "499 255612.7656 18.75\n",
            "549 333220.3750 6.25\n",
            "599 353313.7500 6.25\n",
            "649 433466.5938 18.75\n",
            "699 423346.9375 18.75\n",
            "749 334275.1562 6.25\n",
            "799 428259.7812 6.25\n",
            "849 446439.7500 6.25\n",
            "899 484246.3125 6.25\n",
            "949 349887.6875 6.25\n",
            "999 402979.8438 6.25\n",
            "1049 325053.2188 0.0\n",
            "1099 619084.9375 12.5\n",
            "1149 296657.7188 12.5\n",
            "1199 316529.5938 6.25\n",
            "1249 434739.3438 6.25\n",
            "1299 98600.7188 31.25\n",
            "1349 471027.5312 18.75\n",
            "1399 237134.7500 12.5\n",
            "1449 293519.9375 6.25\n",
            "1499 340649.3750 0.0\n",
            "1549 576613.1250 0.0\n",
            "1599 232429.9062 0.0\n",
            "1649 459922.5625 12.5\n",
            "1699 388267.7812 6.25\n",
            "1749 370760.0625 12.5\n",
            "1799 433358.2500 12.5\n",
            "1849 525457.7500 12.5\n",
            "1899 380118.3750 12.5\n",
            "1949 398188.5625 12.5\n",
            "1999 503741.0625 12.5\n",
            "2049 244651.3594 0.0\n",
            "2099 233490.3125 6.25\n",
            "2149 509261.5000 12.5\n",
            "2199 259964.3438 31.25\n",
            "2249 520270.4375 12.5\n",
            "2299 205144.4062 31.25\n",
            "2349 165247.0156 6.25\n",
            "2399 856985.4375 0.0\n",
            "2449 152807.6562 37.5\n",
            "2499 307206.2500 12.5\n",
            "2549 326692.7500 6.25\n",
            "2599 537632.3125 0.0\n",
            "2649 785541.5000 6.25\n",
            "2699 396615.8750 18.75\n",
            "2749 276544.6875 6.25\n",
            "2799 418624.8438 0.0\n",
            "2849 536008.2500 12.5\n",
            "2899 264172.5000 25.0\n",
            "2949 519053.1562 12.5\n",
            "2999 560517.3125 12.5\n",
            "3049 402179.0312 6.25\n",
            "3099 270165.0625 12.5\n",
            "3149 291207.1250 18.75\n",
            "3199 371922.6250 25.0\n",
            "3249 415996.5625 12.5\n",
            "3299 382710.0625 0.0\n",
            "3349 362375.1875 12.5\n",
            "3399 505503.3750 18.75\n",
            "3449 445465.9375 12.5\n",
            "3499 276364.3125 6.25\n",
            "3549 88574.9062 0.0\n",
            "3599 390316.6250 12.5\n",
            "3649 459941.0625 0.0\n",
            "3699 265922.5625 6.25\n",
            "3749 329476.5000 18.75\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 18331.4453, Accuracy: 958/10000 (9.58%)\n",
            "\n",
            "--- 14.579218864440918 seconds ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#mnist Percep epoch 10\n",
        "class ArgumentParser:\n",
        "    def __init__(self):\n",
        "        self.args = {\n",
        "            'lr': 0.01,            # Learning rate for optimizing class representative\n",
        "            'gamma': 0.3,          # Kernel parameter for computing covariance\n",
        "            'epoch': 10,            # Epochs of training\n",
        "            'gorder': 16,           # Order of the cyclic group required for G-VSA (8 ou 16)\n",
        "            'dim': 10000,          # Dimension of hypervectors c le grand D\n",
        "            'seed': 43,            # Random seed for reproducing results\n",
        "            'resume': True,       # Resume from existing encoded hypervectors\n",
        "            'data_dir': './encoded_data', # Directory to save encoded data (hypervectors)\n",
        "            'dataset': 'mnist',    # Dataset choice choices=['mnist', 'fmnist', 'cifar', 'isolet', 'ucihar']\n",
        "            'raw_data_dir': './dataset',   # Raw data directory\n",
        "            'model': 'percep-rff'    # Model type choices=['rff-hdc', 'linear-hdc', 'rff-gvsa']\n",
        "        }\n",
        "        self.seed = self.args['seed']\n",
        "        self.lr = self.args['lr']\n",
        "        self.gamma = self.args['gamma']\n",
        "        self.epoch = self.args['epoch']\n",
        "        self.gorder = self.args['gorder']\n",
        "        self.dim = self.args['dim']\n",
        "        self.resume = self.args['resume']\n",
        "        self.data_dir = self.args['data_dir']\n",
        "        self.dataset = self.args['dataset']\n",
        "        self.raw_data_dir = self.args['raw_data_dir']\n",
        "        self.model = self.args['model']\n",
        "\n",
        "\n",
        "    def get_args(self):\n",
        "        return self.args\n",
        "\n",
        "args = ArgumentParser()\n",
        "torch.manual_seed(args.seed)\n",
        "np.random.seed(args.seed)\n",
        "if 'hdc' in args.model:\n",
        "    args.gorder = 2\n",
        "    print(\"Use binary HDC with random fourier features, ignoring gorder, set to 2.\")\n",
        "args.data_dir = f'{args.data_dir}/{args.dataset}_{args.model}_order{args.gorder}_gamma{args.gamma}_dim{args.dim}'\n",
        "try:\n",
        "    os.makedirs(args.data_dir)\n",
        "except FileExistsError:\n",
        "    print('Encoded data folder already exists')\n",
        "if not args.resume:\n",
        "    print('Encode the dataset into hypervectors and save')\n",
        "    encode_and_save(args)\n",
        "    print('Finish encoding and saving')\n",
        "print(f'Optimizing class representatives for {args.epoch} epochs')\n",
        "train(args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M_4Gjmkpsb5h",
        "outputId": "dfcbf834-1567-44db-eba1-48c4632227df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoded data folder already exists\n",
            "Optimizing class representatives for 10 epochs\n",
            "Loading encoded training data...\n",
            "Loading encoded test data...\n",
            "Size of encoded training data torch.Size([60000, 10000]) and test data torch.Size([10000, 10000])\n",
            "Epoch: 1\n",
            "49 564582.4375 0.0\n",
            "99 267302.0312 12.5\n",
            "149 321624.1250 25.0\n",
            "199 229842.5938 0.0\n",
            "249 183192.1250 6.25\n",
            "299 373801.5000 25.0\n",
            "349 458999.3125 12.5\n",
            "399 479386.8125 18.75\n",
            "449 318339.2812 12.5\n",
            "499 471671.3125 6.25\n",
            "549 266498.3750 18.75\n",
            "599 261286.4844 0.0\n",
            "649 438965.8438 12.5\n",
            "699 262413.3750 18.75\n",
            "749 370561.4688 6.25\n",
            "799 346419.2812 12.5\n",
            "849 274350.8125 6.25\n",
            "899 700092.1875 12.5\n",
            "949 472051.5000 12.5\n",
            "999 202532.4219 25.0\n",
            "1049 455981.1875 12.5\n",
            "1099 394978.0938 12.5\n",
            "1149 228486.7500 12.5\n",
            "1199 178865.1406 6.25\n",
            "1249 228810.0625 25.0\n",
            "1299 462197.6250 12.5\n",
            "1349 560815.5625 0.0\n",
            "1399 698394.3125 0.0\n",
            "1449 502243.6875 6.25\n",
            "1499 436863.6875 6.25\n",
            "1549 283534.0000 0.0\n",
            "1599 447018.5000 6.25\n",
            "1649 358891.0000 6.25\n",
            "1699 384533.5000 12.5\n",
            "1749 379311.8125 12.5\n",
            "1799 709682.6250 0.0\n",
            "1849 258547.2031 12.5\n",
            "1899 333524.5000 18.75\n",
            "1949 326069.2812 0.0\n",
            "1999 392492.1562 6.25\n",
            "2049 417254.1250 6.25\n",
            "2099 498464.3750 12.5\n",
            "2149 264218.0625 12.5\n",
            "2199 318950.3750 12.5\n",
            "2249 321855.3750 12.5\n",
            "2299 409965.1875 12.5\n",
            "2349 446954.1875 0.0\n",
            "2399 398151.5625 0.0\n",
            "2449 233694.7031 6.25\n",
            "2499 424052.1562 0.0\n",
            "2549 384033.7500 18.75\n",
            "2599 485120.2500 12.5\n",
            "2649 310682.2500 6.25\n",
            "2699 332898.5000 25.0\n",
            "2749 292375.7812 18.75\n",
            "2799 497049.4375 6.25\n",
            "2849 281329.3750 12.5\n",
            "2899 284534.7500 6.25\n",
            "2949 534406.3125 0.0\n",
            "2999 459792.1875 0.0\n",
            "3049 330676.5312 12.5\n",
            "3099 167948.7812 0.0\n",
            "3149 371821.8125 6.25\n",
            "3199 652504.1250 6.25\n",
            "3249 350585.6875 18.75\n",
            "3299 122740.8281 6.25\n",
            "3349 351876.5000 18.75\n",
            "3399 346854.3125 12.5\n",
            "3449 386683.7500 31.25\n",
            "3499 307817.5625 6.25\n",
            "3549 287175.0625 6.25\n",
            "3599 578114.8125 6.25\n",
            "3649 690529.4375 0.0\n",
            "3699 601290.8125 6.25\n",
            "3749 188001.0312 6.25\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 17731.4316, Accuracy: 982/10000 (9.82%)\n",
            "\n",
            "--- 14.825300931930542 seconds ---\n",
            "Epoch: 2\n",
            "49 390616.8125 6.25\n",
            "99 296784.1562 6.25\n",
            "149 247708.1094 18.75\n",
            "199 589763.1875 12.5\n",
            "249 492635.5000 6.25\n",
            "299 499371.0625 0.0\n",
            "349 460573.6250 6.25\n",
            "399 456534.5625 12.5\n",
            "449 350383.8750 12.5\n",
            "499 273431.1875 18.75\n",
            "549 388500.5625 0.0\n",
            "599 309777.4688 12.5\n",
            "649 541263.6250 12.5\n",
            "699 313672.2500 6.25\n",
            "749 386565.8750 6.25\n",
            "799 277006.4375 12.5\n",
            "849 604278.1875 0.0\n",
            "899 349489.1875 12.5\n",
            "949 445438.1562 12.5\n",
            "999 655863.2500 12.5\n",
            "1049 297531.5000 6.25\n",
            "1099 489796.7500 12.5\n",
            "1149 343221.1250 6.25\n",
            "1199 216789.3281 12.5\n",
            "1249 557372.6250 0.0\n",
            "1299 510839.5000 12.5\n",
            "1349 361636.0000 18.75\n",
            "1399 461783.0000 12.5\n",
            "1449 548227.3125 6.25\n",
            "1499 649143.6875 12.5\n",
            "1549 779420.8750 12.5\n",
            "1599 558716.2500 6.25\n",
            "1649 234231.5938 12.5\n",
            "1699 523838.9375 25.0\n",
            "1749 282834.3125 12.5\n",
            "1799 469070.2500 6.25\n",
            "1849 175827.5312 6.25\n",
            "1899 521712.0938 0.0\n",
            "1949 313958.8750 18.75\n",
            "1999 218451.0625 12.5\n",
            "2049 461428.0625 6.25\n",
            "2099 269599.0000 6.25\n",
            "2149 560625.6250 0.0\n",
            "2199 417003.8125 18.75\n",
            "2249 329297.5312 0.0\n",
            "2299 477471.1875 12.5\n",
            "2349 342552.0000 18.75\n",
            "2399 449930.1562 6.25\n",
            "2449 889906.3125 6.25\n",
            "2499 221746.2500 6.25\n",
            "2549 587319.2500 6.25\n",
            "2599 352308.3125 25.0\n",
            "2649 569985.8750 12.5\n",
            "2699 292058.7500 6.25\n",
            "2749 261454.5625 12.5\n",
            "2799 694504.8750 0.0\n",
            "2849 264375.6250 12.5\n",
            "2899 558310.5625 6.25\n",
            "2949 368104.8750 12.5\n",
            "2999 258000.4531 0.0\n",
            "3049 590459.8125 0.0\n",
            "3099 240716.3125 12.5\n",
            "3149 405162.4062 6.25\n",
            "3199 455830.6562 12.5\n",
            "3249 154726.8750 18.75\n",
            "3299 409710.3125 6.25\n",
            "3349 347455.8750 0.0\n",
            "3399 261176.6094 12.5\n",
            "3449 560785.2500 12.5\n",
            "3499 311420.8750 6.25\n",
            "3549 215762.6562 25.0\n",
            "3599 217622.2188 18.75\n",
            "3649 281485.6250 12.5\n",
            "3699 249457.9531 6.25\n",
            "3749 191241.5625 25.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 21293.0137, Accuracy: 958/10000 (9.58%)\n",
            "\n",
            "--- 14.739592552185059 seconds ---\n",
            "Epoch: 3\n",
            "49 418919.0938 0.0\n",
            "99 429044.6875 12.5\n",
            "149 281714.3125 6.25\n",
            "199 310712.2500 12.5\n",
            "249 324277.5625 18.75\n",
            "299 439215.3750 18.75\n",
            "349 344082.0000 12.5\n",
            "399 145303.5781 31.25\n",
            "449 318087.8750 6.25\n",
            "499 529356.1875 18.75\n",
            "549 559658.3125 12.5\n",
            "599 405348.2812 12.5\n",
            "649 187392.7500 6.25\n",
            "699 341954.5625 12.5\n",
            "749 298674.5000 0.0\n",
            "799 331374.1875 0.0\n",
            "849 246682.1094 6.25\n",
            "899 552709.7500 12.5\n",
            "949 330594.1250 18.75\n",
            "999 195870.5156 25.0\n",
            "1049 575545.9375 6.25\n",
            "1099 381523.8750 12.5\n",
            "1149 426286.0000 6.25\n",
            "1199 381095.6250 18.75\n",
            "1249 267285.3438 12.5\n",
            "1299 758754.1250 0.0\n",
            "1349 467598.7812 18.75\n",
            "1399 492092.3438 6.25\n",
            "1449 344170.9375 0.0\n",
            "1499 415177.4375 18.75\n",
            "1549 430299.3750 6.25\n",
            "1599 380073.5000 6.25\n",
            "1649 241910.4375 12.5\n",
            "1699 319774.2812 12.5\n",
            "1749 203023.8750 18.75\n",
            "1799 236189.5469 18.75\n",
            "1849 504603.4375 18.75\n",
            "1899 301777.7500 18.75\n",
            "1949 243741.6719 31.25\n",
            "1999 343992.2812 0.0\n",
            "2049 587014.0000 0.0\n",
            "2099 262839.4688 12.5\n",
            "2149 362185.8750 12.5\n",
            "2199 411592.5000 6.25\n",
            "2249 284663.3750 12.5\n",
            "2299 208203.1719 0.0\n",
            "2349 553571.1250 12.5\n",
            "2399 464839.1250 6.25\n",
            "2449 350603.6875 6.25\n",
            "2499 345335.9062 6.25\n",
            "2549 372296.1250 6.25\n",
            "2599 562259.2500 6.25\n",
            "2649 399559.0625 6.25\n",
            "2699 561589.5625 0.0\n",
            "2749 524373.2500 6.25\n",
            "2799 431302.7188 12.5\n",
            "2849 560730.0625 0.0\n",
            "2899 495366.7500 6.25\n",
            "2949 234392.6719 6.25\n",
            "2999 536713.8750 0.0\n",
            "3049 364378.5312 6.25\n",
            "3099 150635.9531 12.5\n",
            "3149 340005.9688 6.25\n",
            "3199 485460.6562 0.0\n",
            "3249 440051.9688 6.25\n",
            "3299 307106.3438 6.25\n",
            "3349 335592.8750 12.5\n",
            "3399 301178.9062 12.5\n",
            "3449 195748.4219 6.25\n",
            "3499 297301.0312 6.25\n",
            "3549 108647.4375 31.25\n",
            "3599 500070.6250 12.5\n",
            "3649 153766.1094 25.0\n",
            "3699 222942.9844 25.0\n",
            "3749 220941.3125 18.75\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 19103.2754, Accuracy: 980/10000 (9.80%)\n",
            "\n",
            "--- 15.126137495040894 seconds ---\n",
            "Epoch: 4\n",
            "49 294955.2812 0.0\n",
            "99 458696.3438 6.25\n",
            "149 409210.7188 6.25\n",
            "199 270709.7188 18.75\n",
            "249 257553.8281 12.5\n",
            "299 390908.8750 6.25\n",
            "349 225669.4844 31.25\n",
            "399 290370.3750 12.5\n",
            "449 310056.5625 12.5\n",
            "499 191510.6875 6.25\n",
            "549 507355.6875 0.0\n",
            "599 401819.1250 0.0\n",
            "649 377035.0625 6.25\n",
            "699 575922.1250 0.0\n",
            "749 636349.8125 6.25\n",
            "799 568409.7500 12.5\n",
            "849 339151.3125 12.5\n",
            "899 269521.8125 37.5\n",
            "949 454556.7812 18.75\n",
            "999 132186.7500 18.75\n",
            "1049 423054.0625 6.25\n",
            "1099 308996.6250 18.75\n",
            "1149 699658.3750 0.0\n",
            "1199 329296.0000 6.25\n",
            "1249 382851.3438 6.25\n",
            "1299 267811.9062 6.25\n",
            "1349 370327.5000 0.0\n",
            "1399 335742.4688 12.5\n",
            "1449 350330.6562 18.75\n",
            "1499 448225.0000 6.25\n",
            "1549 157125.0312 12.5\n",
            "1599 331287.7812 0.0\n",
            "1649 530207.1250 6.25\n",
            "1699 443267.4375 6.25\n",
            "1749 467570.4375 12.5\n",
            "1799 483180.9688 6.25\n",
            "1849 283903.8438 25.0\n",
            "1899 171591.3438 12.5\n",
            "1949 196914.9688 18.75\n",
            "1999 536608.8125 6.25\n",
            "2049 302774.2188 12.5\n",
            "2099 433464.0312 25.0\n",
            "2149 365416.4062 6.25\n",
            "2199 329326.5000 0.0\n",
            "2249 174175.0938 6.25\n",
            "2299 296145.9375 18.75\n",
            "2349 471451.4688 6.25\n",
            "2399 388952.4688 6.25\n",
            "2449 665334.3750 6.25\n",
            "2499 177340.4062 18.75\n",
            "2549 463848.2812 0.0\n",
            "2599 240524.9375 18.75\n",
            "2649 264970.3750 0.0\n",
            "2699 309289.0625 12.5\n",
            "2749 374855.0625 12.5\n",
            "2799 613521.8125 0.0\n",
            "2849 718046.5000 6.25\n",
            "2899 393670.2812 12.5\n",
            "2949 253381.7500 6.25\n",
            "2999 299008.4688 6.25\n",
            "3049 451898.0625 6.25\n",
            "3099 228535.8125 12.5\n",
            "3149 213489.5000 6.25\n",
            "3199 319522.8125 12.5\n",
            "3249 211306.6250 6.25\n",
            "3299 289415.3125 6.25\n",
            "3349 243513.7188 6.25\n",
            "3399 484442.1250 6.25\n",
            "3449 412158.7188 0.0\n",
            "3499 229093.9688 0.0\n",
            "3549 155359.8438 25.0\n",
            "3599 313569.3125 12.5\n",
            "3649 493107.1562 6.25\n",
            "3699 503762.5938 12.5\n",
            "3749 295228.5938 18.75\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 23960.9609, Accuracy: 892/10000 (8.92%)\n",
            "\n",
            "--- 15.630601644515991 seconds ---\n",
            "Epoch: 5\n",
            "49 172635.8750 6.25\n",
            "99 316612.5000 18.75\n",
            "149 377902.8750 6.25\n",
            "199 329432.2500 6.25\n",
            "249 345587.5000 6.25\n",
            "299 257927.8750 0.0\n",
            "349 675903.8750 6.25\n",
            "399 323868.7500 6.25\n",
            "449 324729.7812 6.25\n",
            "499 271081.0000 18.75\n",
            "549 494453.9062 0.0\n",
            "599 776128.8750 6.25\n",
            "649 347775.0938 0.0\n",
            "699 295629.5312 6.25\n",
            "749 337869.7812 18.75\n",
            "799 257397.4688 12.5\n",
            "849 450053.3125 12.5\n",
            "899 201304.5312 12.5\n",
            "949 361255.5625 0.0\n",
            "999 317111.1562 12.5\n",
            "1049 438639.6875 0.0\n",
            "1099 443201.5000 12.5\n",
            "1149 273487.4688 12.5\n",
            "1199 243775.4375 18.75\n",
            "1249 474257.4688 6.25\n",
            "1299 379214.9375 12.5\n",
            "1349 325686.0938 6.25\n",
            "1399 532595.6250 0.0\n",
            "1449 288186.0938 12.5\n",
            "1499 326749.9375 18.75\n",
            "1549 617561.0625 0.0\n",
            "1599 386972.7188 6.25\n",
            "1649 239978.4062 12.5\n",
            "1699 331578.3750 0.0\n",
            "1749 409897.0625 0.0\n",
            "1799 463739.5938 0.0\n",
            "1849 523719.1250 6.25\n",
            "1899 343086.0000 6.25\n",
            "1949 339242.0938 0.0\n",
            "1999 280481.7812 18.75\n",
            "2049 521347.9062 0.0\n",
            "2099 439035.3750 12.5\n",
            "2149 488508.1875 12.5\n",
            "2199 363936.5625 6.25\n",
            "2249 406585.1875 18.75\n",
            "2299 333691.9062 18.75\n",
            "2349 523311.0938 0.0\n",
            "2399 361235.3750 6.25\n",
            "2449 504372.0938 12.5\n",
            "2499 294438.0312 6.25\n",
            "2549 328632.9375 6.25\n",
            "2599 274838.5938 6.25\n",
            "2649 370139.5938 18.75\n",
            "2699 364009.5625 6.25\n",
            "2749 270748.2500 12.5\n",
            "2799 715630.7500 12.5\n",
            "2849 237568.1562 12.5\n",
            "2899 396024.9688 6.25\n",
            "2949 273892.2188 12.5\n",
            "2999 337659.5312 12.5\n",
            "3049 315185.2812 18.75\n",
            "3099 422813.6875 12.5\n",
            "3149 306697.8750 6.25\n",
            "3199 279911.0938 12.5\n",
            "3249 630387.6250 6.25\n",
            "3299 340352.6250 6.25\n",
            "3349 251502.5938 18.75\n",
            "3399 233087.9375 6.25\n",
            "3449 517940.4062 6.25\n",
            "3499 406543.3750 12.5\n",
            "3549 329192.7188 18.75\n",
            "3599 518550.6250 0.0\n",
            "3649 407316.0938 25.0\n",
            "3699 526170.9375 6.25\n",
            "3749 282503.7188 6.25\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 15544.8809, Accuracy: 1028/10000 (10.28%)\n",
            "\n",
            "--- 15.023382663726807 seconds ---\n",
            "Epoch: 6\n",
            "49 559431.0000 6.25\n",
            "99 309461.1875 6.25\n",
            "149 349233.9688 0.0\n",
            "199 547019.9375 18.75\n",
            "249 578649.3125 25.0\n",
            "299 432220.2188 6.25\n",
            "349 349603.3438 12.5\n",
            "399 308188.8125 6.25\n",
            "449 400885.2188 6.25\n",
            "499 460391.8125 6.25\n",
            "549 236517.2500 18.75\n",
            "599 677174.2500 12.5\n",
            "649 403112.8438 6.25\n",
            "699 316293.4688 6.25\n",
            "749 409924.6875 18.75\n",
            "799 516525.4062 18.75\n",
            "849 525497.7500 0.0\n",
            "899 410608.0312 12.5\n",
            "949 322669.5000 6.25\n",
            "999 545714.2500 12.5\n",
            "1049 375259.5312 6.25\n",
            "1099 404419.4688 0.0\n",
            "1149 468541.1562 6.25\n",
            "1199 377540.5000 18.75\n",
            "1249 299037.5000 6.25\n",
            "1299 373231.9062 12.5\n",
            "1349 568244.2500 18.75\n",
            "1399 351680.8125 12.5\n",
            "1449 355553.0312 25.0\n",
            "1499 447023.7188 0.0\n",
            "1549 417495.1562 12.5\n",
            "1599 374525.8750 6.25\n",
            "1649 643389.7500 6.25\n",
            "1699 585648.0625 0.0\n",
            "1749 608263.0000 25.0\n",
            "1799 372340.6250 18.75\n",
            "1849 384076.0312 12.5\n",
            "1899 215647.5000 12.5\n",
            "1949 428377.4062 6.25\n",
            "1999 397175.6875 6.25\n",
            "2049 432923.7812 18.75\n",
            "2099 296986.9688 12.5\n",
            "2149 509949.4688 12.5\n",
            "2199 243162.0625 18.75\n",
            "2249 209845.4062 18.75\n",
            "2299 271355.5938 12.5\n",
            "2349 367297.7188 0.0\n",
            "2399 437827.5312 6.25\n",
            "2449 423359.2188 12.5\n",
            "2499 325742.6250 12.5\n",
            "2549 319098.0000 6.25\n",
            "2599 480611.6562 6.25\n",
            "2649 281593.0000 6.25\n",
            "2699 363301.1875 0.0\n",
            "2749 451633.7812 18.75\n",
            "2799 367928.0625 0.0\n",
            "2849 319138.5625 31.25\n",
            "2899 647055.2500 0.0\n",
            "2949 536580.7500 0.0\n",
            "2999 361951.5000 0.0\n",
            "3049 337096.7500 6.25\n",
            "3099 231723.0000 18.75\n",
            "3149 503028.9688 0.0\n",
            "3199 388743.3750 12.5\n",
            "3249 247746.5938 0.0\n",
            "3299 610033.8750 6.25\n",
            "3349 381342.3438 18.75\n",
            "3399 284250.3125 12.5\n",
            "3449 224975.1562 12.5\n",
            "3499 333009.2188 31.25\n",
            "3549 294983.1875 6.25\n",
            "3599 326064.7188 0.0\n",
            "3649 272829.4062 6.25\n",
            "3699 350531.9062 12.5\n",
            "3749 389665.9688 6.25\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 26551.3301, Accuracy: 892/10000 (8.92%)\n",
            "\n",
            "--- 14.902562141418457 seconds ---\n",
            "Epoch: 7\n",
            "49 448083.3438 25.0\n",
            "99 508343.6875 6.25\n",
            "149 381638.3438 18.75\n",
            "199 333791.7188 0.0\n",
            "249 599209.5000 12.5\n",
            "299 218273.1875 12.5\n",
            "349 413009.1875 0.0\n",
            "399 556429.0000 12.5\n",
            "449 645149.8125 12.5\n",
            "499 602456.7500 18.75\n",
            "549 358451.2812 6.25\n",
            "599 495497.9688 6.25\n",
            "649 369729.9688 0.0\n",
            "699 299260.2812 12.5\n",
            "749 286577.1562 12.5\n",
            "799 327026.7188 31.25\n",
            "849 505686.0938 12.5\n",
            "899 408410.0000 18.75\n",
            "949 217315.3125 18.75\n",
            "999 436044.8750 18.75\n",
            "1049 293324.1250 18.75\n",
            "1099 468969.7812 18.75\n",
            "1149 400634.0312 6.25\n",
            "1199 358850.0000 12.5\n",
            "1249 300821.7812 12.5\n",
            "1299 288085.1875 12.5\n",
            "1349 360499.0312 6.25\n",
            "1399 285754.6875 0.0\n",
            "1449 250466.8750 12.5\n",
            "1499 348792.3438 12.5\n",
            "1549 399925.6250 12.5\n",
            "1599 311218.7188 6.25\n",
            "1649 481439.6875 12.5\n",
            "1699 436250.4375 12.5\n",
            "1749 385423.2812 18.75\n",
            "1799 328041.4688 6.25\n",
            "1849 343251.1250 18.75\n",
            "1899 267358.9062 0.0\n",
            "1949 249509.7188 12.5\n",
            "1999 388131.9375 12.5\n",
            "2049 365830.6250 18.75\n",
            "2099 196760.2500 12.5\n",
            "2149 365613.6875 6.25\n",
            "2199 447377.6562 6.25\n",
            "2249 223931.8125 37.5\n",
            "2299 256901.9375 25.0\n",
            "2349 295544.1875 12.5\n",
            "2399 508989.1875 18.75\n",
            "2449 384957.1250 18.75\n",
            "2499 448741.2188 12.5\n",
            "2549 355643.8438 0.0\n",
            "2599 357233.8750 18.75\n",
            "2649 401862.6562 6.25\n",
            "2699 137699.4375 12.5\n",
            "2749 540202.6875 0.0\n",
            "2799 186593.8750 25.0\n",
            "2849 599360.3750 0.0\n",
            "2899 464373.7188 6.25\n",
            "2949 422141.9688 18.75\n",
            "2999 286317.4375 18.75\n",
            "3049 732198.0000 6.25\n",
            "3099 273902.3438 18.75\n",
            "3149 221685.1250 18.75\n",
            "3199 239470.6250 0.0\n",
            "3249 212046.0625 6.25\n",
            "3299 200874.1562 37.5\n",
            "3349 380326.0938 6.25\n",
            "3399 810681.5000 0.0\n",
            "3449 472047.1250 25.0\n",
            "3499 242778.5938 6.25\n",
            "3549 562043.6250 0.0\n",
            "3599 351649.6250 6.25\n",
            "3649 280967.9062 0.0\n",
            "3699 190405.1250 12.5\n",
            "3749 405652.9375 6.25\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 18904.4141, Accuracy: 1009/10000 (10.09%)\n",
            "\n",
            "--- 14.796595573425293 seconds ---\n",
            "Epoch: 8\n",
            "49 389124.4688 0.0\n",
            "99 481193.0625 12.5\n",
            "149 275947.0625 25.0\n",
            "199 360999.1562 6.25\n",
            "249 710579.1250 0.0\n",
            "299 513027.7812 6.25\n",
            "349 291578.3750 12.5\n",
            "399 301937.4375 6.25\n",
            "449 344654.1250 0.0\n",
            "499 377132.5312 6.25\n",
            "549 246530.2188 0.0\n",
            "599 556563.0000 6.25\n",
            "649 206212.5938 12.5\n",
            "699 562958.6875 6.25\n",
            "749 308445.2812 12.5\n",
            "799 559986.3750 0.0\n",
            "849 374556.0312 12.5\n",
            "899 287876.7812 18.75\n",
            "949 578769.3750 0.0\n",
            "999 317962.1250 0.0\n",
            "1049 403624.0000 6.25\n",
            "1099 246892.3438 12.5\n",
            "1149 334917.3750 6.25\n",
            "1199 196336.9375 18.75\n",
            "1249 369611.3750 0.0\n",
            "1299 604071.8750 12.5\n",
            "1349 423537.9375 18.75\n",
            "1399 291276.7188 18.75\n",
            "1449 445606.9375 6.25\n",
            "1499 210668.5312 18.75\n",
            "1549 294719.3750 12.5\n",
            "1599 533842.6875 25.0\n",
            "1649 331399.6562 6.25\n",
            "1699 177795.2812 12.5\n",
            "1749 518648.3438 0.0\n",
            "1799 266592.0625 12.5\n",
            "1849 502420.9688 12.5\n",
            "1899 472912.9062 12.5\n",
            "1949 404835.1250 12.5\n",
            "1999 448740.8125 0.0\n",
            "2049 352635.0312 0.0\n",
            "2099 246057.2812 0.0\n",
            "2149 666709.8125 12.5\n",
            "2199 337471.7188 0.0\n",
            "2249 508588.7500 6.25\n",
            "2299 244396.9375 25.0\n",
            "2349 207780.2188 18.75\n",
            "2399 303143.2500 25.0\n",
            "2449 297233.9375 18.75\n",
            "2499 449273.7812 6.25\n",
            "2549 392529.4062 6.25\n",
            "2599 547483.7500 12.5\n",
            "2649 382178.7500 18.75\n",
            "2699 395748.6875 6.25\n",
            "2749 459514.3125 12.5\n",
            "2799 284538.9062 18.75\n",
            "2849 287528.4688 0.0\n",
            "2899 435971.8125 6.25\n",
            "2949 312998.9375 18.75\n",
            "2999 422219.7812 6.25\n",
            "3049 399654.1250 18.75\n",
            "3099 439700.0938 12.5\n",
            "3149 256951.1562 0.0\n",
            "3199 268479.1562 18.75\n",
            "3249 350646.7188 6.25\n",
            "3299 416703.3750 0.0\n",
            "3349 380799.8750 0.0\n",
            "3399 417699.4375 12.5\n",
            "3449 207335.0000 0.0\n",
            "3499 460370.5312 6.25\n",
            "3549 280264.9688 12.5\n",
            "3599 649589.5000 0.0\n",
            "3649 292867.9688 12.5\n",
            "3699 517358.3750 6.25\n",
            "3749 456620.2188 6.25\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 19864.3145, Accuracy: 1009/10000 (10.09%)\n",
            "\n",
            "--- 14.649455070495605 seconds ---\n",
            "Epoch: 9\n",
            "49 409284.8750 18.75\n",
            "99 569400.6250 0.0\n",
            "149 399007.2188 12.5\n",
            "199 257709.4375 12.5\n",
            "249 449371.5625 12.5\n",
            "299 582777.1875 6.25\n",
            "349 326122.4375 12.5\n",
            "399 577283.0000 0.0\n",
            "449 258108.3438 6.25\n",
            "499 328649.3750 0.0\n",
            "549 431405.6562 18.75\n",
            "599 169730.4375 18.75\n",
            "649 247210.8438 12.5\n",
            "699 526278.1250 0.0\n",
            "749 329753.4062 18.75\n",
            "799 382322.1250 12.5\n",
            "849 442611.3125 18.75\n",
            "899 240103.8438 25.0\n",
            "949 268594.6562 0.0\n",
            "999 371237.1562 12.5\n",
            "1049 386916.6875 0.0\n",
            "1099 290055.4375 6.25\n",
            "1149 464784.3750 12.5\n",
            "1199 474466.7188 6.25\n",
            "1249 139098.9062 0.0\n",
            "1299 464233.7500 12.5\n",
            "1349 326464.2188 12.5\n",
            "1399 341853.9062 12.5\n",
            "1449 379903.5000 6.25\n",
            "1499 520465.8750 0.0\n",
            "1549 303693.2812 18.75\n",
            "1599 537091.6250 12.5\n",
            "1649 279810.3125 18.75\n",
            "1699 278802.4375 18.75\n",
            "1749 427163.0000 6.25\n",
            "1799 355863.2500 12.5\n",
            "1849 413238.4062 6.25\n",
            "1899 186261.3438 12.5\n",
            "1949 460298.7188 6.25\n",
            "1999 429913.8125 0.0\n",
            "2049 346172.1875 6.25\n",
            "2099 608181.8750 0.0\n",
            "2149 260865.6875 0.0\n",
            "2199 416950.9062 12.5\n",
            "2249 277607.6250 12.5\n",
            "2299 441216.6875 6.25\n",
            "2349 335901.0938 0.0\n",
            "2399 540013.6250 6.25\n",
            "2449 375943.4062 12.5\n",
            "2499 262046.1250 12.5\n",
            "2549 283027.4375 18.75\n",
            "2599 519423.7188 12.5\n",
            "2649 497624.3438 12.5\n",
            "2699 487492.1875 0.0\n",
            "2749 635587.6875 6.25\n",
            "2799 534519.1250 6.25\n",
            "2849 330660.0625 12.5\n",
            "2899 290534.2500 12.5\n",
            "2949 593959.2500 18.75\n",
            "2999 318628.6250 12.5\n",
            "3049 429659.7188 6.25\n",
            "3099 396952.2188 6.25\n",
            "3149 258681.9375 18.75\n",
            "3199 322871.7188 6.25\n",
            "3249 160035.5000 25.0\n",
            "3299 353315.4375 18.75\n",
            "3349 367526.1250 0.0\n",
            "3399 278980.5000 18.75\n",
            "3449 355713.5625 6.25\n",
            "3499 574386.0625 12.5\n",
            "3549 620098.1250 6.25\n",
            "3599 351694.7188 6.25\n",
            "3649 468284.1250 0.0\n",
            "3699 470459.0625 6.25\n",
            "3749 434981.7188 0.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 22799.1074, Accuracy: 982/10000 (9.82%)\n",
            "\n",
            "--- 14.639983892440796 seconds ---\n",
            "Epoch: 10\n",
            "49 746174.6875 0.0\n",
            "99 712418.8125 0.0\n",
            "149 630929.6250 12.5\n",
            "199 249458.3438 18.75\n",
            "249 186827.1875 18.75\n",
            "299 297010.9688 0.0\n",
            "349 236162.4688 18.75\n",
            "399 357190.4688 18.75\n",
            "449 384963.3125 6.25\n",
            "499 316609.5000 6.25\n",
            "549 324947.8750 12.5\n",
            "599 345517.9375 6.25\n",
            "649 204718.2188 12.5\n",
            "699 296110.0938 12.5\n",
            "749 401739.8750 0.0\n",
            "799 369576.1250 0.0\n",
            "849 710575.3750 0.0\n",
            "899 560154.3750 18.75\n",
            "949 758629.1250 0.0\n",
            "999 242800.5312 6.25\n",
            "1049 649290.5000 0.0\n",
            "1099 474013.0000 12.5\n",
            "1149 454730.6875 0.0\n",
            "1199 491603.1562 12.5\n",
            "1249 366083.3125 6.25\n",
            "1299 305398.0625 18.75\n",
            "1349 411332.3750 6.25\n",
            "1399 956962.2500 6.25\n",
            "1449 280381.7188 18.75\n",
            "1499 266265.0000 12.5\n",
            "1549 468552.4062 0.0\n",
            "1599 319543.0938 12.5\n",
            "1649 564075.3125 6.25\n",
            "1699 461521.0000 12.5\n",
            "1749 557078.1875 12.5\n",
            "1799 538258.2500 12.5\n",
            "1849 268414.2812 12.5\n",
            "1899 355553.4375 0.0\n",
            "1949 356168.2188 18.75\n",
            "1999 450971.0625 0.0\n",
            "2049 511389.3125 6.25\n",
            "2099 350721.3125 18.75\n",
            "2149 479833.6562 6.25\n",
            "2199 230703.2500 6.25\n",
            "2249 465312.4062 0.0\n",
            "2299 402165.2188 6.25\n",
            "2349 397225.9062 12.5\n",
            "2399 453217.5625 6.25\n",
            "2449 252629.7812 12.5\n",
            "2499 302916.4688 25.0\n",
            "2549 428230.9688 6.25\n",
            "2599 438802.8125 12.5\n",
            "2649 486880.3750 12.5\n",
            "2699 417320.3438 0.0\n",
            "2749 202875.6875 12.5\n",
            "2799 414082.5312 25.0\n",
            "2849 350685.0000 25.0\n",
            "2899 334850.1875 12.5\n",
            "2949 282395.0938 25.0\n",
            "2999 349787.6562 6.25\n",
            "3049 504679.4062 25.0\n",
            "3099 398129.0938 6.25\n",
            "3149 442688.6250 0.0\n",
            "3199 480962.7812 6.25\n",
            "3249 318029.5938 6.25\n",
            "3299 238887.7188 12.5\n",
            "3349 546462.0000 12.5\n",
            "3399 443324.8438 6.25\n",
            "3449 306735.9062 12.5\n",
            "3499 221185.3438 6.25\n",
            "3549 445662.2188 6.25\n",
            "3599 641060.6250 6.25\n",
            "3649 308914.6250 6.25\n",
            "3699 516989.2500 6.25\n",
            "3749 385065.2500 12.5\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 27191.0332, Accuracy: 1135/10000 (11.35%)\n",
            "\n",
            "--- 15.272642850875854 seconds ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FMNIST"
      ],
      "metadata": {
        "id": "yPG2pmb0uqnl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#fmnist Percep epoch 1\n",
        "class ArgumentParser:\n",
        "    def __init__(self):\n",
        "        self.args = {\n",
        "            'lr': 0.01,            # Learning rate for optimizing class representative\n",
        "            'gamma': 0.3,          # Kernel parameter for computing covariance\n",
        "            'epoch': 1,            # Epochs of training\n",
        "            'gorder': 16,           # Order of the cyclic group required for G-VSA (8 ou 16)\n",
        "            'dim': 10000,          # Dimension of hypervectors c le grand D\n",
        "            'seed': 43,            # Random seed for reproducing results\n",
        "            'resume': False,       # Resume from existing encoded hypervectors\n",
        "            'data_dir': './encoded_data', # Directory to save encoded data (hypervectors)\n",
        "            'dataset': 'fmnist',    # Dataset choice choices=['mnist', 'fmnist', 'cifar', 'isolet', 'ucihar']\n",
        "            'raw_data_dir': './dataset',   # Raw data directory\n",
        "            'model': 'percep-rff'    # Model type choices=['rff-hdc', 'linear-hdc', 'rff-gvsa']\n",
        "        }\n",
        "        self.seed = self.args['seed']\n",
        "        self.lr = self.args['lr']\n",
        "        self.gamma = self.args['gamma']\n",
        "        self.epoch = self.args['epoch']\n",
        "        self.gorder = self.args['gorder']\n",
        "        self.dim = self.args['dim']\n",
        "        self.resume = self.args['resume']\n",
        "        self.data_dir = self.args['data_dir']\n",
        "        self.dataset = self.args['dataset']\n",
        "        self.raw_data_dir = self.args['raw_data_dir']\n",
        "        self.model = self.args['model']\n",
        "\n",
        "\n",
        "    def get_args(self):\n",
        "        return self.args\n",
        "\n",
        "args = ArgumentParser()\n",
        "torch.manual_seed(args.seed)\n",
        "np.random.seed(args.seed)\n",
        "if 'hdc' in args.model:\n",
        "    args.gorder = 2\n",
        "    print(\"Use binary HDC with random fourier features, ignoring gorder, set to 2.\")\n",
        "args.data_dir = f'{args.data_dir}/{args.dataset}_{args.model}_order{args.gorder}_gamma{args.gamma}_dim{args.dim}'\n",
        "try:\n",
        "    os.makedirs(args.data_dir)\n",
        "except FileExistsError:\n",
        "    print('Encoded data folder already exists')\n",
        "if not args.resume:\n",
        "    print('Encode the dataset into hypervectors and save')\n",
        "    encode_and_save(args)\n",
        "    print('Finish encoding and saving')\n",
        "print(f'Optimizing class representatives for {args.epoch} epochs')\n",
        "train(args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3zG7836uutEJ",
        "outputId": "bb784826-87e2-4581-93ca-a5d9e207393a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encode the dataset into hypervectors and save\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./dataset/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26.4M/26.4M [00:01<00:00, 13.6MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./dataset/FashionMNIST/raw/train-images-idx3-ubyte.gz to ./dataset/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./dataset/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 29.5k/29.5k [00:00<00:00, 200kB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./dataset/FashionMNIST/raw/train-labels-idx1-ubyte.gz to ./dataset/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./dataset/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.42M/4.42M [00:01<00:00, 3.75MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./dataset/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ./dataset/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./dataset/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5.15k/5.15k [00:00<00:00, 6.42MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./dataset/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ./dataset/FashionMNIST/raw\n",
            "\n",
            "# of channels of data 1\n",
            "# of training samples and test samples 60000 10000\n",
            "Encoding with random fourier features encoder.\n",
            "the threshold to discretize fourier features to group elements tensor([-1.5343, -1.1505, -0.8872, -0.6746, -0.4888, -0.3187, -0.1573,  0.0000,\n",
            "         0.1573,  0.3187,  0.4888,  0.6746,  0.8872,  1.1505,  1.5343])\n",
            "Encoded pixels to hypervectors with size:  torch.Size([256, 10000])\n",
            "Encoding training data...\n",
            "Start encoding data\n",
            "12800 images encoded. Total time elapse = 27.41977572441101\n",
            "25600 images encoded. Total time elapse = 54.7140851020813\n",
            "38400 images encoded. Total time elapse = 81.93553471565247\n",
            "51200 images encoded. Total time elapse = 109.11835789680481\n",
            "Finish encoding data\n",
            "Encoding test data...\n",
            "Start encoding data\n",
            "Finish encoding data\n",
            "Finish encoding and saving\n",
            "Optimizing class representatives for 1 epochs\n",
            "Loading encoded training data...\n",
            "Loading encoded test data...\n",
            "Size of encoded training data torch.Size([60000, 10000]) and test data torch.Size([10000, 10000])\n",
            "Epoch: 1\n",
            "49 438946.3125 18.75\n",
            "99 335279.9375 25.0\n",
            "149 214293.1562 12.5\n",
            "199 306343.2500 6.25\n",
            "249 381308.9688 6.25\n",
            "299 402542.2500 6.25\n",
            "349 463236.8750 0.0\n",
            "399 429306.2500 12.5\n",
            "449 480766.5625 6.25\n",
            "499 224452.4219 6.25\n",
            "549 290787.7500 6.25\n",
            "599 331126.6250 6.25\n",
            "649 341917.3125 6.25\n",
            "699 420690.1250 18.75\n",
            "749 456473.9688 0.0\n",
            "799 623435.8750 0.0\n",
            "849 370848.3750 6.25\n",
            "899 372168.5625 6.25\n",
            "949 730141.6250 18.75\n",
            "999 793459.2500 12.5\n",
            "1049 365300.4688 6.25\n",
            "1099 340395.1562 0.0\n",
            "1149 312546.9375 12.5\n",
            "1199 262412.8125 6.25\n",
            "1249 197387.8594 6.25\n",
            "1299 235117.6719 12.5\n",
            "1349 318332.0312 0.0\n",
            "1399 268150.7188 6.25\n",
            "1449 161660.6719 6.25\n",
            "1499 418783.5625 0.0\n",
            "1549 308828.2500 12.5\n",
            "1599 203919.5469 18.75\n",
            "1649 552371.4375 0.0\n",
            "1699 194554.1250 6.25\n",
            "1749 274021.1875 18.75\n",
            "1799 379372.6250 6.25\n",
            "1849 474028.7500 18.75\n",
            "1899 428832.6875 18.75\n",
            "1949 309522.1250 6.25\n",
            "1999 341985.6250 0.0\n",
            "2049 247993.5156 6.25\n",
            "2099 643119.3125 6.25\n",
            "2149 523414.5000 6.25\n",
            "2199 208424.3906 18.75\n",
            "2249 453467.3750 12.5\n",
            "2299 266506.4375 25.0\n",
            "2349 408927.5000 12.5\n",
            "2399 435726.2500 6.25\n",
            "2449 322061.5625 6.25\n",
            "2499 363441.5312 0.0\n",
            "2549 361538.7500 12.5\n",
            "2599 340507.9062 6.25\n",
            "2649 597934.9375 6.25\n",
            "2699 518204.7500 12.5\n",
            "2749 517812.4375 12.5\n",
            "2799 220268.6094 6.25\n",
            "2849 284379.1875 12.5\n",
            "2899 748554.6250 0.0\n",
            "2949 400964.8125 12.5\n",
            "2999 432046.0625 6.25\n",
            "3049 547949.5000 12.5\n",
            "3099 448089.3750 12.5\n",
            "3149 227850.8281 6.25\n",
            "3199 279739.7500 12.5\n",
            "3249 439655.6250 6.25\n",
            "3299 225344.1406 6.25\n",
            "3349 393541.9375 12.5\n",
            "3399 182562.1562 25.0\n",
            "3449 394810.4688 12.5\n",
            "3499 356700.5000 12.5\n",
            "3549 247944.6719 6.25\n",
            "3599 231920.6562 0.0\n",
            "3649 218461.6719 18.75\n",
            "3699 433564.0625 6.25\n",
            "3749 663888.3750 0.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 26971.3301, Accuracy: 1000/10000 (10.00%)\n",
            "\n",
            "--- 14.99121618270874 seconds ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#fmnist Percep epoch 10\n",
        "class ArgumentParser:\n",
        "    def __init__(self):\n",
        "        self.args = {\n",
        "            'lr': 0.01,            # Learning rate for optimizing class representative\n",
        "            'gamma': 0.3,          # Kernel parameter for computing covariance\n",
        "            'epoch': 10,            # Epochs of training\n",
        "            'gorder': 16,           # Order of the cyclic group required for G-VSA (8 ou 16)\n",
        "            'dim': 10000,          # Dimension of hypervectors c le grand D\n",
        "            'seed': 43,            # Random seed for reproducing results\n",
        "            'resume': True,       # Resume from existing encoded hypervectors\n",
        "            'data_dir': './encoded_data', # Directory to save encoded data (hypervectors)\n",
        "            'dataset': 'fmnist',    # Dataset choice choices=['mnist', 'fmnist', 'cifar', 'isolet', 'ucihar']\n",
        "            'raw_data_dir': './dataset',   # Raw data directory\n",
        "            'model': 'percep-rff'    # Model type choices=['rff-hdc', 'linear-hdc', 'rff-gvsa']\n",
        "        }\n",
        "        self.seed = self.args['seed']\n",
        "        self.lr = self.args['lr']\n",
        "        self.gamma = self.args['gamma']\n",
        "        self.epoch = self.args['epoch']\n",
        "        self.gorder = self.args['gorder']\n",
        "        self.dim = self.args['dim']\n",
        "        self.resume = self.args['resume']\n",
        "        self.data_dir = self.args['data_dir']\n",
        "        self.dataset = self.args['dataset']\n",
        "        self.raw_data_dir = self.args['raw_data_dir']\n",
        "        self.model = self.args['model']\n",
        "\n",
        "\n",
        "    def get_args(self):\n",
        "        return self.args\n",
        "\n",
        "args = ArgumentParser()\n",
        "torch.manual_seed(args.seed)\n",
        "np.random.seed(args.seed)\n",
        "if 'hdc' in args.model:\n",
        "    args.gorder = 2\n",
        "    print(\"Use binary HDC with random fourier features, ignoring gorder, set to 2.\")\n",
        "args.data_dir = f'{args.data_dir}/{args.dataset}_{args.model}_order{args.gorder}_gamma{args.gamma}_dim{args.dim}'\n",
        "try:\n",
        "    os.makedirs(args.data_dir)\n",
        "except FileExistsError:\n",
        "    print('Encoded data folder already exists')\n",
        "if not args.resume:\n",
        "    print('Encode the dataset into hypervectors and save')\n",
        "    encode_and_save(args)\n",
        "    print('Finish encoding and saving')\n",
        "print(f'Optimizing class representatives for {args.epoch} epochs')\n",
        "train(args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rmw7jpSeuxFS",
        "outputId": "70512578-5d21-4bea-b505-e13cbd481b07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoded data folder already exists\n",
            "Optimizing class representatives for 10 epochs\n",
            "Loading encoded training data...\n",
            "Loading encoded test data...\n",
            "Size of encoded training data torch.Size([60000, 10000]) and test data torch.Size([10000, 10000])\n",
            "Epoch: 1\n",
            "49 341323.2188 6.25\n",
            "99 613184.3125 0.0\n",
            "149 491808.6250 12.5\n",
            "199 396343.5000 0.0\n",
            "249 143109.0938 12.5\n",
            "299 241291.2344 12.5\n",
            "349 550162.6875 0.0\n",
            "399 410321.6875 6.25\n",
            "449 178241.2031 18.75\n",
            "499 419919.7500 6.25\n",
            "549 292132.3750 18.75\n",
            "599 501967.1250 6.25\n",
            "649 439897.4375 6.25\n",
            "699 357114.0000 6.25\n",
            "749 403396.8125 6.25\n",
            "799 205117.6719 18.75\n",
            "849 623652.8125 6.25\n",
            "899 283737.7500 6.25\n",
            "949 384775.9375 6.25\n",
            "999 288441.0625 0.0\n",
            "1049 232387.1406 6.25\n",
            "1099 208408.4375 0.0\n",
            "1149 192921.5312 6.25\n",
            "1199 477143.2188 12.5\n",
            "1249 288184.8750 6.25\n",
            "1299 264831.6875 6.25\n",
            "1349 559181.0625 6.25\n",
            "1399 232921.2656 25.0\n",
            "1449 211989.5156 25.0\n",
            "1499 474718.0312 0.0\n",
            "1549 460942.6875 12.5\n",
            "1599 402281.2812 12.5\n",
            "1649 689392.7500 0.0\n",
            "1699 221355.3906 6.25\n",
            "1749 409987.2500 12.5\n",
            "1799 628051.7500 6.25\n",
            "1849 401748.2500 18.75\n",
            "1899 391020.1250 0.0\n",
            "1949 422571.0000 0.0\n",
            "1999 675817.1875 25.0\n",
            "2049 459561.9375 6.25\n",
            "2099 299802.0625 18.75\n",
            "2149 618982.0000 12.5\n",
            "2199 154842.4375 6.25\n",
            "2249 496071.2188 31.25\n",
            "2299 721974.2500 0.0\n",
            "2349 303840.4375 6.25\n",
            "2399 390538.6250 6.25\n",
            "2449 288969.0312 12.5\n",
            "2499 561642.1250 0.0\n",
            "2549 604499.1875 6.25\n",
            "2599 497911.5938 6.25\n",
            "2649 375504.5625 12.5\n",
            "2699 204873.4062 12.5\n",
            "2749 374294.5000 12.5\n",
            "2799 677806.8750 0.0\n",
            "2849 387584.2500 18.75\n",
            "2899 399799.3125 25.0\n",
            "2949 243180.8594 18.75\n",
            "2999 476310.9375 0.0\n",
            "3049 447724.3125 18.75\n",
            "3099 446739.7812 12.5\n",
            "3149 636999.2500 0.0\n",
            "3199 186116.0312 6.25\n",
            "3249 303189.3125 25.0\n",
            "3299 430793.6875 12.5\n",
            "3349 591248.1250 6.25\n",
            "3399 312334.3750 0.0\n",
            "3449 243849.6094 6.25\n",
            "3499 366303.2812 0.0\n",
            "3549 566456.0000 6.25\n",
            "3599 371269.5000 18.75\n",
            "3649 526271.6250 6.25\n",
            "3699 312947.3125 18.75\n",
            "3749 226587.0312 6.25\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 20867.3203, Accuracy: 1000/10000 (10.00%)\n",
            "\n",
            "--- 15.889714002609253 seconds ---\n",
            "Epoch: 2\n",
            "49 302218.8438 18.75\n",
            "99 320811.3125 6.25\n",
            "149 375844.1875 12.5\n",
            "199 435384.2188 12.5\n",
            "249 824456.3750 0.0\n",
            "299 292096.3438 25.0\n",
            "349 461659.3125 0.0\n",
            "399 504221.0625 0.0\n",
            "449 643573.5625 12.5\n",
            "499 389809.7500 12.5\n",
            "549 300808.5938 12.5\n",
            "599 748300.0625 0.0\n",
            "649 678164.7500 0.0\n",
            "699 440179.2500 0.0\n",
            "749 475556.4375 6.25\n",
            "799 188216.2188 0.0\n",
            "849 512188.0625 12.5\n",
            "899 474967.5938 12.5\n",
            "949 375565.1875 12.5\n",
            "999 313193.8125 6.25\n",
            "1049 448789.2812 6.25\n",
            "1099 568158.1250 0.0\n",
            "1149 288524.7812 12.5\n",
            "1199 248579.3125 0.0\n",
            "1249 692668.6875 0.0\n",
            "1299 483186.3125 0.0\n",
            "1349 441297.8750 6.25\n",
            "1399 539982.6875 6.25\n",
            "1449 372293.2188 18.75\n",
            "1499 418844.2500 12.5\n",
            "1549 297273.4375 6.25\n",
            "1599 491721.3438 0.0\n",
            "1649 379501.8750 0.0\n",
            "1699 260625.0938 0.0\n",
            "1749 379450.5625 12.5\n",
            "1799 317892.2500 0.0\n",
            "1849 338609.4688 12.5\n",
            "1899 302432.8125 0.0\n",
            "1949 306906.0625 6.25\n",
            "1999 415195.3750 0.0\n",
            "2049 199755.2969 6.25\n",
            "2099 612285.7500 0.0\n",
            "2149 527001.2500 6.25\n",
            "2199 213011.4531 6.25\n",
            "2249 297236.7500 25.0\n",
            "2299 283791.6250 12.5\n",
            "2349 234288.6250 12.5\n",
            "2399 547499.0000 18.75\n",
            "2449 270267.9375 18.75\n",
            "2499 268621.4375 18.75\n",
            "2549 746221.1250 6.25\n",
            "2599 520548.5000 6.25\n",
            "2649 351159.1250 25.0\n",
            "2699 278082.1250 25.0\n",
            "2749 437807.9688 18.75\n",
            "2799 362630.4375 6.25\n",
            "2849 388646.1250 0.0\n",
            "2899 392544.1562 18.75\n",
            "2949 337048.5000 12.5\n",
            "2999 218235.8750 6.25\n",
            "3049 380591.7812 0.0\n",
            "3099 491784.1562 0.0\n",
            "3149 291757.6875 25.0\n",
            "3199 557480.8125 6.25\n",
            "3249 477454.0938 12.5\n",
            "3299 400724.1250 6.25\n",
            "3349 419129.2500 6.25\n",
            "3399 208800.5156 25.0\n",
            "3449 234551.0156 12.5\n",
            "3499 382639.0938 12.5\n",
            "3549 331455.6250 6.25\n",
            "3599 487424.1250 18.75\n",
            "3649 460574.0625 12.5\n",
            "3699 367809.3125 0.0\n",
            "3749 362706.5312 6.25\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 27511.0938, Accuracy: 1000/10000 (10.00%)\n",
            "\n",
            "--- 14.829179286956787 seconds ---\n",
            "Epoch: 3\n",
            "49 785418.0625 0.0\n",
            "99 156325.0469 18.75\n",
            "149 520279.1250 12.5\n",
            "199 554845.1250 12.5\n",
            "249 272313.5625 0.0\n",
            "299 466392.6250 6.25\n",
            "349 429368.1875 6.25\n",
            "399 452773.9688 6.25\n",
            "449 494317.9375 6.25\n",
            "499 258934.8438 12.5\n",
            "549 286152.3438 12.5\n",
            "599 417580.0312 18.75\n",
            "649 418376.5000 12.5\n",
            "699 713188.3750 12.5\n",
            "749 133776.4219 6.25\n",
            "799 260431.1406 31.25\n",
            "849 410798.4375 0.0\n",
            "899 426092.8125 6.25\n",
            "949 371992.8750 6.25\n",
            "999 409767.7188 25.0\n",
            "1049 430203.1250 12.5\n",
            "1099 440877.9688 12.5\n",
            "1149 432313.2188 0.0\n",
            "1199 248393.2500 18.75\n",
            "1249 521390.1875 0.0\n",
            "1299 340667.8125 18.75\n",
            "1349 479650.1875 6.25\n",
            "1399 407355.2188 0.0\n",
            "1449 372052.5000 31.25\n",
            "1499 408520.7812 12.5\n",
            "1549 228008.2500 25.0\n",
            "1599 387715.0312 6.25\n",
            "1649 492504.4375 0.0\n",
            "1699 301381.5000 12.5\n",
            "1749 483843.3125 0.0\n",
            "1799 270364.3125 6.25\n",
            "1849 395393.6562 12.5\n",
            "1899 586739.6875 18.75\n",
            "1949 395794.1562 18.75\n",
            "1999 251970.6094 12.5\n",
            "2049 328133.6875 6.25\n",
            "2099 347633.4375 6.25\n",
            "2149 358334.4375 0.0\n",
            "2199 246075.1094 0.0\n",
            "2249 396560.1250 18.75\n",
            "2299 704941.5000 6.25\n",
            "2349 337460.2812 6.25\n",
            "2399 259231.2031 0.0\n",
            "2449 243936.3281 0.0\n",
            "2499 433649.6250 6.25\n",
            "2549 449021.4375 0.0\n",
            "2599 246058.7344 12.5\n",
            "2649 275653.5000 18.75\n",
            "2699 195332.0312 6.25\n",
            "2749 779637.2500 0.0\n",
            "2799 401092.0312 18.75\n",
            "2849 264796.0000 18.75\n",
            "2899 138676.3906 12.5\n",
            "2949 143763.0312 12.5\n",
            "2999 335447.9375 12.5\n",
            "3049 106654.2500 18.75\n",
            "3099 418161.9375 6.25\n",
            "3149 655504.3125 6.25\n",
            "3199 355231.7500 12.5\n",
            "3249 274138.0000 6.25\n",
            "3299 322359.5625 6.25\n",
            "3349 569734.3125 6.25\n",
            "3399 612877.3750 6.25\n",
            "3449 288751.3750 12.5\n",
            "3499 608424.6250 6.25\n",
            "3549 413156.5625 0.0\n",
            "3599 478145.0625 12.5\n",
            "3649 519819.5000 6.25\n",
            "3699 483674.5312 0.0\n",
            "3749 452904.0312 6.25\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 30545.5742, Accuracy: 1000/10000 (10.00%)\n",
            "\n",
            "--- 14.934064388275146 seconds ---\n",
            "Epoch: 4\n",
            "49 543991.1250 0.0\n",
            "99 364411.7812 12.5\n",
            "149 273867.6250 6.25\n",
            "199 497021.1250 6.25\n",
            "249 385369.8750 6.25\n",
            "299 451903.3438 12.5\n",
            "349 302152.2188 6.25\n",
            "399 381342.5625 12.5\n",
            "449 645154.2500 0.0\n",
            "499 563057.3750 12.5\n",
            "549 689735.5000 0.0\n",
            "599 761527.1250 0.0\n",
            "649 168422.6250 18.75\n",
            "699 438351.2500 0.0\n",
            "749 323849.0625 18.75\n",
            "799 246384.7812 6.25\n",
            "849 120828.3438 37.5\n",
            "899 278454.0938 25.0\n",
            "949 376613.9062 0.0\n",
            "999 218241.1875 0.0\n",
            "1049 222123.5312 0.0\n",
            "1099 384303.7500 12.5\n",
            "1149 219494.5312 6.25\n",
            "1199 246155.9062 18.75\n",
            "1249 262940.7188 12.5\n",
            "1299 400408.2812 6.25\n",
            "1349 470751.0000 6.25\n",
            "1399 388711.0312 6.25\n",
            "1449 215375.5938 31.25\n",
            "1499 499868.0000 12.5\n",
            "1549 255901.6562 12.5\n",
            "1599 302033.6250 18.75\n",
            "1649 215972.9062 18.75\n",
            "1699 420067.2188 6.25\n",
            "1749 355996.0938 12.5\n",
            "1799 472450.6250 6.25\n",
            "1849 286074.5312 6.25\n",
            "1899 498703.9375 6.25\n",
            "1949 408030.0000 12.5\n",
            "1999 392075.7812 0.0\n",
            "2049 379989.3125 12.5\n",
            "2099 370634.9375 18.75\n",
            "2149 412408.5938 18.75\n",
            "2199 289578.4375 0.0\n",
            "2249 395732.2812 6.25\n",
            "2299 563070.6250 6.25\n",
            "2349 515143.2188 0.0\n",
            "2399 288216.5000 6.25\n",
            "2449 321165.9062 37.5\n",
            "2499 294422.0000 31.25\n",
            "2549 287063.0312 12.5\n",
            "2599 314199.6250 12.5\n",
            "2649 156670.3750 6.25\n",
            "2699 270957.2812 12.5\n",
            "2749 244909.4688 6.25\n",
            "2799 551637.6250 12.5\n",
            "2849 294539.4688 12.5\n",
            "2899 391210.7812 12.5\n",
            "2949 466306.6875 6.25\n",
            "2999 427782.1250 0.0\n",
            "3049 294188.9688 18.75\n",
            "3099 346399.2500 18.75\n",
            "3149 431598.5000 12.5\n",
            "3199 216255.5000 6.25\n",
            "3249 370803.0312 25.0\n",
            "3299 342976.9688 18.75\n",
            "3349 302372.2188 0.0\n",
            "3399 355535.6562 12.5\n",
            "3449 251584.6875 6.25\n",
            "3499 221109.6875 12.5\n",
            "3549 771943.2500 0.0\n",
            "3599 304590.7812 0.0\n",
            "3649 297163.7188 6.25\n",
            "3699 188214.5312 18.75\n",
            "3749 477394.4062 18.75\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 36276.7656, Accuracy: 1000/10000 (10.00%)\n",
            "\n",
            "--- 14.740790367126465 seconds ---\n",
            "Epoch: 5\n",
            "49 138560.5000 12.5\n",
            "99 198634.4062 6.25\n",
            "149 513701.5312 6.25\n",
            "199 206965.0938 6.25\n",
            "249 283584.5938 31.25\n",
            "299 430430.9375 18.75\n",
            "349 608770.6875 6.25\n",
            "399 207118.3125 18.75\n",
            "449 521046.4375 0.0\n",
            "499 245980.8125 6.25\n",
            "549 554887.5000 0.0\n",
            "599 366205.2188 6.25\n",
            "649 288677.0312 12.5\n",
            "699 395529.2500 0.0\n",
            "749 599088.0625 6.25\n",
            "799 436783.5000 18.75\n",
            "849 305195.8750 12.5\n",
            "899 283658.6562 25.0\n",
            "949 555111.3750 0.0\n",
            "999 280675.5938 12.5\n",
            "1049 374051.5625 0.0\n",
            "1099 598461.1875 6.25\n",
            "1149 274873.4062 6.25\n",
            "1199 490324.9688 18.75\n",
            "1249 324480.9688 18.75\n",
            "1299 606794.2500 0.0\n",
            "1349 363299.3438 6.25\n",
            "1399 324134.8438 12.5\n",
            "1449 143285.4688 18.75\n",
            "1499 246998.1562 12.5\n",
            "1549 772830.7500 0.0\n",
            "1599 233752.9062 18.75\n",
            "1649 329112.9062 18.75\n",
            "1699 448006.5938 25.0\n",
            "1749 330150.0938 6.25\n",
            "1799 391132.9688 6.25\n",
            "1849 307840.0938 6.25\n",
            "1899 511180.1875 6.25\n",
            "1949 367258.0625 0.0\n",
            "1999 280285.2812 6.25\n",
            "2049 265151.7500 18.75\n",
            "2099 254124.7812 0.0\n",
            "2149 407540.7188 6.25\n",
            "2199 186216.8438 12.5\n",
            "2249 373669.0000 6.25\n",
            "2299 409257.0625 18.75\n",
            "2349 475725.2812 6.25\n",
            "2399 520027.2188 0.0\n",
            "2449 169435.8750 31.25\n",
            "2499 315957.4062 18.75\n",
            "2549 372380.5625 6.25\n",
            "2599 443361.7500 6.25\n",
            "2649 449102.3125 12.5\n",
            "2699 538540.0000 0.0\n",
            "2749 456015.0000 6.25\n",
            "2799 273860.5625 6.25\n",
            "2849 514565.8438 18.75\n",
            "2899 427615.8438 6.25\n",
            "2949 377754.6875 6.25\n",
            "2999 340035.2500 12.5\n",
            "3049 737548.5000 6.25\n",
            "3099 308053.4062 12.5\n",
            "3149 359033.5625 6.25\n",
            "3199 288746.3438 6.25\n",
            "3249 347556.6250 18.75\n",
            "3299 160707.3438 18.75\n",
            "3349 357933.2188 12.5\n",
            "3399 280181.8750 12.5\n",
            "3449 281609.9062 12.5\n",
            "3499 335630.9062 12.5\n",
            "3549 376996.7812 12.5\n",
            "3599 436454.7500 12.5\n",
            "3649 552108.1875 0.0\n",
            "3699 413470.4062 0.0\n",
            "3749 481984.7812 6.25\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 30997.4863, Accuracy: 1000/10000 (10.00%)\n",
            "\n",
            "--- 14.832969903945923 seconds ---\n",
            "Epoch: 6\n",
            "49 575174.7500 12.5\n",
            "99 378251.8750 6.25\n",
            "149 343669.8438 12.5\n",
            "199 497911.8750 12.5\n",
            "249 189601.8750 12.5\n",
            "299 297801.7812 12.5\n",
            "349 232794.8125 18.75\n",
            "399 424536.7500 0.0\n",
            "449 712696.1250 6.25\n",
            "499 313749.9375 0.0\n",
            "549 202775.4062 6.25\n",
            "599 247045.9688 6.25\n",
            "649 426366.7812 0.0\n",
            "699 387729.1875 6.25\n",
            "749 397477.0625 6.25\n",
            "799 456776.2500 18.75\n",
            "849 410418.7812 6.25\n",
            "899 385553.2812 0.0\n",
            "949 368019.3750 6.25\n",
            "999 335284.9688 0.0\n",
            "1049 179141.6562 6.25\n",
            "1099 440662.9688 6.25\n",
            "1149 468009.0000 0.0\n",
            "1199 366665.8125 6.25\n",
            "1249 602416.3750 6.25\n",
            "1299 348043.4375 6.25\n",
            "1349 192531.1562 6.25\n",
            "1399 399871.1875 12.5\n",
            "1449 360053.4062 0.0\n",
            "1499 210885.2812 12.5\n",
            "1549 281222.9688 12.5\n",
            "1599 293644.1562 6.25\n",
            "1649 358802.0938 6.25\n",
            "1699 505728.8438 0.0\n",
            "1749 362761.5625 25.0\n",
            "1799 404763.9062 12.5\n",
            "1849 338050.4375 0.0\n",
            "1899 463290.3125 0.0\n",
            "1949 402444.3125 6.25\n",
            "1999 150472.3750 18.75\n",
            "2049 439080.5625 6.25\n",
            "2099 292653.1875 12.5\n",
            "2149 279545.9375 25.0\n",
            "2199 286934.7188 0.0\n",
            "2249 366140.2188 0.0\n",
            "2299 368694.1562 18.75\n",
            "2349 996046.2500 6.25\n",
            "2399 325540.9375 12.5\n",
            "2449 526284.3125 6.25\n",
            "2499 392314.7812 12.5\n",
            "2549 642743.6250 0.0\n",
            "2599 272180.3125 12.5\n",
            "2649 230639.1562 25.0\n",
            "2699 263434.2812 12.5\n",
            "2749 505040.0312 6.25\n",
            "2799 598162.7500 6.25\n",
            "2849 500162.8750 18.75\n",
            "2899 609987.3125 6.25\n",
            "2949 332553.4375 0.0\n",
            "2999 311596.8125 12.5\n",
            "3049 259594.0625 12.5\n",
            "3099 547860.8750 6.25\n",
            "3149 505309.2188 6.25\n",
            "3199 564156.5000 6.25\n",
            "3249 431718.3750 6.25\n",
            "3299 365727.4375 25.0\n",
            "3349 493566.8438 18.75\n",
            "3399 307782.4062 0.0\n",
            "3449 429927.3438 18.75\n",
            "3499 309528.1562 6.25\n",
            "3549 157725.1250 12.5\n",
            "3599 384273.8125 6.25\n",
            "3649 351973.6250 6.25\n",
            "3699 423616.6875 0.0\n",
            "3749 163381.4375 25.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 23067.6328, Accuracy: 1000/10000 (10.00%)\n",
            "\n",
            "--- 15.294534683227539 seconds ---\n",
            "Epoch: 7\n",
            "49 440641.1250 12.5\n",
            "99 465606.1250 6.25\n",
            "149 425691.5000 6.25\n",
            "199 383897.7812 0.0\n",
            "249 227853.5000 31.25\n",
            "299 324668.2500 18.75\n",
            "349 327282.3750 6.25\n",
            "399 413774.3125 12.5\n",
            "449 463684.2188 12.5\n",
            "499 696491.3750 25.0\n",
            "549 249175.0312 0.0\n",
            "599 360374.1875 6.25\n",
            "649 247910.1250 0.0\n",
            "699 162573.5625 18.75\n",
            "749 263045.1562 6.25\n",
            "799 170259.2188 25.0\n",
            "849 498532.9062 6.25\n",
            "899 587509.3125 6.25\n",
            "949 274214.5000 18.75\n",
            "999 253456.2500 12.5\n",
            "1049 727095.6250 6.25\n",
            "1099 372615.4062 6.25\n",
            "1149 396772.5312 12.5\n",
            "1199 357059.9688 18.75\n",
            "1249 305693.5625 25.0\n",
            "1299 482156.7812 0.0\n",
            "1349 338229.4375 12.5\n",
            "1399 449842.1562 18.75\n",
            "1449 335814.0000 0.0\n",
            "1499 517968.7188 12.5\n",
            "1549 423117.2500 6.25\n",
            "1599 230251.0625 12.5\n",
            "1649 436403.6250 12.5\n",
            "1699 466027.5938 12.5\n",
            "1749 356144.0625 0.0\n",
            "1799 170488.2500 6.25\n",
            "1849 335809.7812 18.75\n",
            "1899 317602.7812 6.25\n",
            "1949 255421.1875 12.5\n",
            "1999 556995.5000 12.5\n",
            "2049 324387.1250 12.5\n",
            "2099 492262.9375 0.0\n",
            "2149 335491.4375 6.25\n",
            "2199 233613.7188 18.75\n",
            "2249 350218.8125 12.5\n",
            "2299 457026.8750 12.5\n",
            "2349 330434.4062 18.75\n",
            "2399 407938.5312 12.5\n",
            "2449 224041.6875 12.5\n",
            "2499 471537.3125 12.5\n",
            "2549 421695.3125 18.75\n",
            "2599 288046.5312 0.0\n",
            "2649 365794.1250 6.25\n",
            "2699 313082.7812 12.5\n",
            "2749 484299.7188 18.75\n",
            "2799 275920.1562 12.5\n",
            "2849 233096.3750 6.25\n",
            "2899 656931.2500 0.0\n",
            "2949 282459.4688 12.5\n",
            "2999 379368.0312 18.75\n",
            "3049 402742.0312 18.75\n",
            "3099 593105.9375 6.25\n",
            "3149 597990.0000 0.0\n",
            "3199 396408.0625 12.5\n",
            "3249 433671.5000 12.5\n",
            "3299 250342.8438 0.0\n",
            "3349 184964.3125 6.25\n",
            "3399 381937.9688 18.75\n",
            "3449 495276.7812 18.75\n",
            "3499 508094.3438 6.25\n",
            "3549 400897.1250 25.0\n",
            "3599 153131.0938 12.5\n",
            "3649 316850.5312 6.25\n",
            "3699 465877.8125 25.0\n",
            "3749 470782.7812 6.25\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 20226.3691, Accuracy: 1000/10000 (10.00%)\n",
            "\n",
            "--- 16.015461921691895 seconds ---\n",
            "Epoch: 8\n",
            "49 543612.6250 6.25\n",
            "99 558999.5000 0.0\n",
            "149 419420.4688 6.25\n",
            "199 301613.1562 6.25\n",
            "249 361704.9375 6.25\n",
            "299 189635.8438 25.0\n",
            "349 307984.3750 0.0\n",
            "399 245591.1562 18.75\n",
            "449 441219.3125 12.5\n",
            "499 295140.3438 6.25\n",
            "549 386800.2500 18.75\n",
            "599 308834.0938 6.25\n",
            "649 595250.6250 12.5\n",
            "699 244340.1562 12.5\n",
            "749 586030.1875 6.25\n",
            "799 414334.6562 0.0\n",
            "849 498128.8438 18.75\n",
            "899 452856.9688 18.75\n",
            "949 360805.1250 12.5\n",
            "999 198257.9062 18.75\n",
            "1049 339758.9688 25.0\n",
            "1099 409589.6562 18.75\n",
            "1149 430350.9062 6.25\n",
            "1199 445387.8438 6.25\n",
            "1249 338160.9688 6.25\n",
            "1299 451753.1250 6.25\n",
            "1349 463233.3438 18.75\n",
            "1399 483923.0000 18.75\n",
            "1449 272886.5938 12.5\n",
            "1499 335385.3438 12.5\n",
            "1549 583968.1250 6.25\n",
            "1599 234453.1875 12.5\n",
            "1649 502190.7188 0.0\n",
            "1699 279022.3125 12.5\n",
            "1749 445349.0312 6.25\n",
            "1799 284498.0000 0.0\n",
            "1849 197742.4375 18.75\n",
            "1899 456508.8750 25.0\n",
            "1949 213808.0312 18.75\n",
            "1999 281879.9375 18.75\n",
            "2049 378228.0938 6.25\n",
            "2099 324231.5938 0.0\n",
            "2149 554066.7500 12.5\n",
            "2199 508023.9062 12.5\n",
            "2249 497727.7812 12.5\n",
            "2299 288952.2812 6.25\n",
            "2349 461326.4688 0.0\n",
            "2399 415004.6875 18.75\n",
            "2449 654195.6250 6.25\n",
            "2499 251255.2812 6.25\n",
            "2549 426096.1250 0.0\n",
            "2599 262939.0625 18.75\n",
            "2649 349575.4688 12.5\n",
            "2699 336033.8750 6.25\n",
            "2749 584636.3125 6.25\n",
            "2799 157506.3125 12.5\n",
            "2849 448975.0000 12.5\n",
            "2899 331020.5938 6.25\n",
            "2949 365971.6562 12.5\n",
            "2999 380036.5938 6.25\n",
            "3049 328212.5312 6.25\n",
            "3099 540293.0000 12.5\n",
            "3149 581412.6250 6.25\n",
            "3199 249041.1562 6.25\n",
            "3249 536604.6250 0.0\n",
            "3299 429763.8750 6.25\n",
            "3349 277924.2812 18.75\n",
            "3399 397967.6562 18.75\n",
            "3449 403758.8750 0.0\n",
            "3499 444311.4375 6.25\n",
            "3549 405422.4375 0.0\n",
            "3599 249139.7188 0.0\n",
            "3649 515087.0312 12.5\n",
            "3699 367749.7500 18.75\n",
            "3749 460748.0625 6.25\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 26370.5195, Accuracy: 1000/10000 (10.00%)\n",
            "\n",
            "--- 15.026214122772217 seconds ---\n",
            "Epoch: 9\n",
            "49 249869.9688 6.25\n",
            "99 229092.7500 25.0\n",
            "149 411267.6562 18.75\n",
            "199 509112.8438 6.25\n",
            "249 369000.7188 0.0\n",
            "299 256344.6562 12.5\n",
            "349 401726.6875 6.25\n",
            "399 370582.9375 0.0\n",
            "449 332181.7188 18.75\n",
            "499 473929.3750 12.5\n",
            "549 414284.8750 12.5\n",
            "599 198917.8438 12.5\n",
            "649 468121.4062 12.5\n",
            "699 452203.6250 12.5\n",
            "749 266503.1250 12.5\n",
            "799 317136.1875 12.5\n",
            "849 251212.9375 6.25\n",
            "899 195249.5000 6.25\n",
            "949 421711.8750 12.5\n",
            "999 513988.1250 0.0\n",
            "1049 390980.4375 12.5\n",
            "1099 274713.5000 12.5\n",
            "1149 355823.1875 12.5\n",
            "1199 308804.7188 6.25\n",
            "1249 221436.9375 6.25\n",
            "1299 355162.0312 6.25\n",
            "1349 556744.2500 12.5\n",
            "1399 317976.4062 6.25\n",
            "1449 384157.6250 6.25\n",
            "1499 357125.9688 25.0\n",
            "1549 238188.7500 18.75\n",
            "1599 380356.0938 6.25\n",
            "1649 363000.7188 6.25\n",
            "1699 285028.9375 12.5\n",
            "1749 404534.7812 6.25\n",
            "1799 443939.8750 6.25\n",
            "1849 186364.5938 31.25\n",
            "1899 499815.9375 12.5\n",
            "1949 249306.0625 25.0\n",
            "1999 230106.2812 12.5\n",
            "2049 180574.1562 12.5\n",
            "2099 639011.5000 12.5\n",
            "2149 124583.1875 18.75\n",
            "2199 470289.9062 0.0\n",
            "2249 375516.7812 6.25\n",
            "2299 210839.4375 25.0\n",
            "2349 303459.6875 12.5\n",
            "2399 440021.7188 6.25\n",
            "2449 321436.9375 25.0\n",
            "2499 131259.3438 12.5\n",
            "2549 378817.9375 12.5\n",
            "2599 269835.6875 12.5\n",
            "2649 443735.8750 6.25\n",
            "2699 479068.2188 18.75\n",
            "2749 529248.0000 6.25\n",
            "2799 523756.7500 6.25\n",
            "2849 397106.9062 6.25\n",
            "2899 483189.1875 6.25\n",
            "2949 147228.2812 6.25\n",
            "2999 420303.5312 6.25\n",
            "3049 488027.5312 25.0\n",
            "3099 335298.7500 12.5\n",
            "3149 380134.3438 6.25\n",
            "3199 125732.6250 6.25\n",
            "3249 319104.4688 0.0\n",
            "3299 476235.6250 12.5\n",
            "3349 695905.4375 6.25\n",
            "3399 201561.1250 6.25\n",
            "3449 236941.6250 18.75\n",
            "3499 378173.0000 0.0\n",
            "3549 484897.6250 6.25\n",
            "3599 547251.0000 12.5\n",
            "3649 384463.3438 31.25\n",
            "3699 345563.8438 6.25\n",
            "3749 122050.2812 18.75\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 15955.9912, Accuracy: 1000/10000 (10.00%)\n",
            "\n",
            "--- 14.917766094207764 seconds ---\n",
            "Epoch: 10\n",
            "49 331481.3750 0.0\n",
            "99 175555.6562 6.25\n",
            "149 278084.4375 18.75\n",
            "199 808971.9375 6.25\n",
            "249 401141.5625 6.25\n",
            "299 589042.0000 0.0\n",
            "349 299018.8438 12.5\n",
            "399 181661.1562 18.75\n",
            "449 289630.9688 6.25\n",
            "499 271486.6875 25.0\n",
            "549 379004.1875 12.5\n",
            "599 128415.9375 12.5\n",
            "649 447709.3438 18.75\n",
            "699 340968.2500 0.0\n",
            "749 332817.3750 12.5\n",
            "799 696301.1250 0.0\n",
            "849 573692.6875 12.5\n",
            "899 287971.3438 6.25\n",
            "949 232416.9375 6.25\n",
            "999 343127.0000 6.25\n",
            "1049 387551.5938 0.0\n",
            "1099 427887.5312 12.5\n",
            "1149 498959.4062 18.75\n",
            "1199 291317.0312 25.0\n",
            "1249 266001.7188 12.5\n",
            "1299 518772.2188 6.25\n",
            "1349 144601.6875 6.25\n",
            "1399 540785.6250 0.0\n",
            "1449 370961.5000 6.25\n",
            "1499 486003.7812 6.25\n",
            "1549 377998.3750 0.0\n",
            "1599 515851.3750 12.5\n",
            "1649 597682.1875 6.25\n",
            "1699 569192.6250 6.25\n",
            "1749 558213.5000 0.0\n",
            "1799 388431.0312 6.25\n",
            "1849 447067.8750 0.0\n",
            "1899 454534.0938 6.25\n",
            "1949 429919.8438 12.5\n",
            "1999 193919.6250 6.25\n",
            "2049 449045.0625 0.0\n",
            "2099 465458.9062 6.25\n",
            "2149 495758.9688 18.75\n",
            "2199 601577.6875 6.25\n",
            "2249 245915.9375 25.0\n",
            "2299 367962.6562 12.5\n",
            "2349 517960.5312 12.5\n",
            "2399 102346.6875 6.25\n",
            "2449 262836.4375 12.5\n",
            "2499 180298.8750 6.25\n",
            "2549 214771.1562 25.0\n",
            "2599 271167.4375 18.75\n",
            "2649 484903.5625 0.0\n",
            "2699 230143.2188 25.0\n",
            "2749 380452.0000 6.25\n",
            "2799 470853.4375 12.5\n",
            "2849 429725.6250 12.5\n",
            "2899 388263.3438 6.25\n",
            "2949 570944.3125 25.0\n",
            "2999 186922.1875 12.5\n",
            "3049 445451.6250 18.75\n",
            "3099 237126.5938 12.5\n",
            "3149 241460.1875 31.25\n",
            "3199 351646.5625 12.5\n",
            "3249 528408.1875 6.25\n",
            "3299 324273.5000 6.25\n",
            "3349 414853.3438 6.25\n",
            "3399 266860.8125 12.5\n",
            "3449 268227.6562 25.0\n",
            "3499 390991.8125 0.0\n",
            "3549 444801.2812 0.0\n",
            "3599 323497.0000 6.25\n",
            "3649 341162.9062 12.5\n",
            "3699 276117.0000 12.5\n",
            "3749 432066.6875 6.25\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 22147.7363, Accuracy: 1000/10000 (10.00%)\n",
            "\n",
            "--- 14.87857961654663 seconds ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Linear HDC"
      ],
      "metadata": {
        "id": "isr23p9auaXi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ISOLET"
      ],
      "metadata": {
        "id": "SQl7kSeivrEY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YfIKYHqgo6ME",
        "outputId": "d5b31a8e-aa24-4dbb-80e0-c87165b5944f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Use binary HDC with random fourier features, ignoring gorder, set to 2.\n",
            "Encode the dataset into hypervectors and save\n",
            "# of channels of data 1\n",
            "# of training samples and test samples 6238 1559\n",
            "Encoding to binary HDC with linear hamming distance.\n",
            "generating linear item memory...\n",
            "Encoded pixels to hypervectors with size:  torch.Size([256, 10000])\n",
            "Encoding training data...\n",
            "start encoding data here\n",
            "1000 images encoded\n",
            "2000 images encoded\n",
            "3000 images encoded\n",
            "4000 images encoded\n",
            "5000 images encoded\n",
            "6000 images encoded\n",
            "finish encoding data here\n",
            "Encoding test data...\n",
            "start encoding data here\n",
            "1000 images encoded\n",
            "finish encoding data here\n",
            "Finish encoding and saving\n",
            "Optimizing class representatives for 1 epochs\n",
            "Loading encoded training data...\n",
            "Loading encoded test data...\n",
            "Size of encoded training data torch.Size([6238, 10000]) and test data torch.Size([1559, 10000])\n",
            "Epoch: 1\n",
            "49 3.6680 6.25\n",
            "99 3.6751 12.5\n",
            "149 3.4996 0.0\n",
            "199 3.6306 18.75\n",
            "249 3.6036 6.25\n",
            "299 3.7174 6.25\n",
            "349 3.4921 6.25\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.2350, Accuracy: 56/1559 (3.59%)\n",
            "\n",
            "--- 3.3082711696624756 seconds ---\n"
          ]
        }
      ],
      "source": [
        "#isolet epoch 1 linear-hdc\n",
        "class ArgumentParser:\n",
        "    def __init__(self):\n",
        "        self.args = {\n",
        "            'lr': 0.01,            # Learning rate for optimizing class representative\n",
        "            'gamma': 0.3,          # Kernel parameter for computing covariance\n",
        "            'epoch': 1,            # Epochs of training\n",
        "            'gorder': 16,           # Order of the cyclic group required for G-VSA (8 ou 16)\n",
        "            'dim': 10000,          # Dimension of hypervectors c le grand D\n",
        "            'seed': 43,            # Random seed for reproducing results\n",
        "            'resume': False,       # Resume from existing encoded hypervectors\n",
        "            'data_dir': './encoded_data', # Directory to save encoded data (hypervectors)\n",
        "            'dataset': 'isolet',    # Dataset choice choices=['mnist', 'fmnist', 'cifar', 'isolet', 'ucihar']\n",
        "            'raw_data_dir': './dataset',   # Raw data directory\n",
        "            'model': 'linear-hdc'    # Model type choices=['rff-hdc', 'linear-hdc', 'rff-gvsa']\n",
        "        }\n",
        "        self.seed = self.args['seed']\n",
        "        self.lr = self.args['lr']\n",
        "        self.gamma = self.args['gamma']\n",
        "        self.epoch = self.args['epoch']\n",
        "        self.gorder = self.args['gorder']\n",
        "        self.dim = self.args['dim']\n",
        "        self.resume = self.args['resume']\n",
        "        self.data_dir = self.args['data_dir']\n",
        "        self.dataset = self.args['dataset']\n",
        "        self.raw_data_dir = self.args['raw_data_dir']\n",
        "        self.model = self.args['model']\n",
        "\n",
        "\n",
        "    def get_args(self):\n",
        "        return self.args\n",
        "args = ArgumentParser()\n",
        "torch.manual_seed(args.seed)\n",
        "np.random.seed(args.seed)\n",
        "if 'hdc' in args.model:\n",
        "    args.gorder = 2\n",
        "    print(\"Use binary HDC with random fourier features, ignoring gorder, set to 2.\")\n",
        "args.data_dir = f'{args.data_dir}/{args.dataset}_{args.model}_order{args.gorder}_gamma{args.gamma}_dim{args.dim}'\n",
        "try:\n",
        "    os.makedirs(args.data_dir)\n",
        "except FileExistsError:\n",
        "    print('Encoded data folder already exists')\n",
        "if not args.resume:\n",
        "    print('Encode the dataset into hypervectors and save')\n",
        "    encode_and_save(args)\n",
        "    print('Finish encoding and saving')\n",
        "print(f'Optimizing class representatives for {args.epoch} epochs')\n",
        "train(args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-LBFiyX0pC40",
        "outputId": "c8a54676-4c19-4ca7-df28-639beb86b32a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Use binary HDC with random fourier features, ignoring gorder, set to 2.\n",
            "Encoded data folder already exists\n",
            "Optimizing class representatives for 10 epochs\n",
            "Loading encoded training data...\n",
            "Loading encoded test data...\n",
            "Size of encoded training data torch.Size([6238, 10000]) and test data torch.Size([1559, 10000])\n",
            "Epoch: 1\n",
            "49 3.6680 6.25\n",
            "99 3.6751 12.5\n",
            "149 3.4996 0.0\n",
            "199 3.6306 18.75\n",
            "249 3.6036 6.25\n",
            "299 3.7174 6.25\n",
            "349 3.4921 6.25\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.2350, Accuracy: 56/1559 (3.59%)\n",
            "\n",
            "--- 3.440319299697876 seconds ---\n",
            "Epoch: 2\n",
            "49 0.3289 100.0\n",
            "99 0.3025 100.0\n",
            "149 0.5690 100.0\n",
            "199 0.6325 100.0\n",
            "249 0.7024 100.0\n",
            "299 0.9241 100.0\n",
            "349 0.8942 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.2346, Accuracy: 54/1559 (3.46%)\n",
            "\n",
            "--- 2.488813877105713 seconds ---\n",
            "Epoch: 3\n",
            "49 0.3354 100.0\n",
            "99 0.3432 100.0\n",
            "149 0.5210 100.0\n",
            "199 0.6594 100.0\n",
            "249 0.6662 100.0\n",
            "299 0.6389 100.0\n",
            "349 0.9188 87.5\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.2359, Accuracy: 50/1559 (3.21%)\n",
            "\n",
            "--- 1.8293018341064453 seconds ---\n",
            "Epoch: 4\n",
            "49 0.3069 100.0\n",
            "99 0.2695 100.0\n",
            "149 0.2757 100.0\n",
            "199 0.6591 100.0\n",
            "249 0.4603 100.0\n",
            "299 0.5965 100.0\n",
            "349 0.8145 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.2350, Accuracy: 56/1559 (3.59%)\n",
            "\n",
            "--- 2.0024073123931885 seconds ---\n",
            "Epoch: 5\n",
            "49 0.3195 100.0\n",
            "99 0.2833 100.0\n",
            "149 0.3807 100.0\n",
            "199 0.3793 100.0\n",
            "249 0.4494 100.0\n",
            "299 0.5625 100.0\n",
            "349 0.5876 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.2345, Accuracy: 55/1559 (3.53%)\n",
            "\n",
            "--- 2.665886402130127 seconds ---\n",
            "Epoch: 6\n",
            "49 0.2846 100.0\n",
            "99 0.3563 100.0\n",
            "149 0.4020 100.0\n",
            "199 0.3763 100.0\n",
            "249 0.4366 100.0\n",
            "299 0.4020 100.0\n",
            "349 0.6830 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.2343, Accuracy: 51/1559 (3.27%)\n",
            "\n",
            "--- 3.1785643100738525 seconds ---\n",
            "Epoch: 7\n",
            "49 0.2319 100.0\n",
            "99 0.2398 100.0\n",
            "149 0.3399 100.0\n",
            "199 0.3323 100.0\n",
            "249 0.4306 100.0\n",
            "299 0.5147 100.0\n",
            "349 0.4924 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.2352, Accuracy: 52/1559 (3.34%)\n",
            "\n",
            "--- 2.5724315643310547 seconds ---\n",
            "Epoch: 8\n",
            "49 0.2374 100.0\n",
            "99 0.4060 100.0\n",
            "149 0.3524 100.0\n",
            "199 0.3505 100.0\n",
            "249 0.3979 100.0\n",
            "299 0.3657 100.0\n",
            "349 0.6212 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.2345, Accuracy: 55/1559 (3.53%)\n",
            "\n",
            "--- 2.719413995742798 seconds ---\n",
            "Epoch: 9\n",
            "49 0.2901 100.0\n",
            "99 0.3101 100.0\n",
            "149 0.3045 100.0\n",
            "199 0.3989 100.0\n",
            "249 0.4575 100.0\n",
            "299 0.5593 100.0\n",
            "349 0.5716 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.2353, Accuracy: 55/1559 (3.53%)\n",
            "\n",
            "--- 2.6599502563476562 seconds ---\n",
            "Epoch: 10\n",
            "49 0.2620 100.0\n",
            "99 0.2642 100.0\n",
            "149 0.2355 100.0\n",
            "199 0.3748 100.0\n",
            "249 0.3134 100.0\n",
            "299 0.6029 100.0\n",
            "349 0.4655 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.2350, Accuracy: 51/1559 (3.27%)\n",
            "\n",
            "--- 2.1411325931549072 seconds ---\n"
          ]
        }
      ],
      "source": [
        "#isolet epoch 10 linear-hdc\n",
        "class ArgumentParser:\n",
        "    def __init__(self):\n",
        "        self.args = {\n",
        "            'lr': 0.01,            # Learning rate for optimizing class representative\n",
        "            'gamma': 0.3,          # Kernel parameter for computing covariance\n",
        "            'epoch': 10,            # Epochs of training\n",
        "            'gorder': 16,           # Order of the cyclic group required for G-VSA (8 ou 16)\n",
        "            'dim': 10000,          # Dimension of hypervectors c le grand D\n",
        "            'seed': 43,            # Random seed for reproducing results\n",
        "            'resume': True,       # Resume from existing encoded hypervectors\n",
        "            'data_dir': './encoded_data', # Directory to save encoded data (hypervectors)\n",
        "            'dataset': 'isolet',    # Dataset choice choices=['mnist', 'fmnist', 'cifar', 'isolet', 'ucihar']\n",
        "            'raw_data_dir': './dataset',   # Raw data directory\n",
        "            'model': 'linear-hdc'    # Model type choices=['rff-hdc', 'linear-hdc', 'rff-gvsa']\n",
        "        }\n",
        "        self.seed = self.args['seed']\n",
        "        self.lr = self.args['lr']\n",
        "        self.gamma = self.args['gamma']\n",
        "        self.epoch = self.args['epoch']\n",
        "        self.gorder = self.args['gorder']\n",
        "        self.dim = self.args['dim']\n",
        "        self.resume = self.args['resume']\n",
        "        self.data_dir = self.args['data_dir']\n",
        "        self.dataset = self.args['dataset']\n",
        "        self.raw_data_dir = self.args['raw_data_dir']\n",
        "        self.model = self.args['model']\n",
        "\n",
        "\n",
        "    def get_args(self):\n",
        "        return self.args\n",
        "args = ArgumentParser()\n",
        "torch.manual_seed(args.seed)\n",
        "np.random.seed(args.seed)\n",
        "if 'hdc' in args.model:\n",
        "    args.gorder = 2\n",
        "    print(\"Use binary HDC with random fourier features, ignoring gorder, set to 2.\")\n",
        "args.data_dir = f'{args.data_dir}/{args.dataset}_{args.model}_order{args.gorder}_gamma{args.gamma}_dim{args.dim}'\n",
        "try:\n",
        "    os.makedirs(args.data_dir)\n",
        "except FileExistsError:\n",
        "    print('Encoded data folder already exists')\n",
        "if not args.resume:\n",
        "    print('Encode the dataset into hypervectors and save')\n",
        "    encode_and_save(args)\n",
        "    print('Finish encoding and saving')\n",
        "print(f'Optimizing class representatives for {args.epoch} epochs')\n",
        "train(args)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## UCIHAR"
      ],
      "metadata": {
        "id": "0p9rmYTZvtoq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wA97oYc0oe0U",
        "outputId": "c220c962-2d11-41fc-c6d1-3d7135e45e74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Use binary HDC with random fourier features, ignoring gorder, set to 2.\n",
            "Encode the dataset into hypervectors and save\n",
            "# of channels of data 1\n",
            "# of training samples and test samples 7352 2947\n",
            "Encoding to binary HDC with linear hamming distance.\n",
            "generating linear item memory...\n",
            "Encoded pixels to hypervectors with size:  torch.Size([256, 10000])\n",
            "Encoding training data...\n",
            "start encoding data here\n",
            "1000 images encoded\n",
            "2000 images encoded\n",
            "3000 images encoded\n",
            "4000 images encoded\n",
            "5000 images encoded\n",
            "6000 images encoded\n",
            "7000 images encoded\n",
            "finish encoding data here\n",
            "Encoding test data...\n",
            "start encoding data here\n",
            "1000 images encoded\n",
            "2000 images encoded\n",
            "finish encoding data here\n",
            "Finish encoding and saving\n",
            "Optimizing class representatives for 1 epochs\n",
            "Loading encoded training data...\n",
            "Loading encoded test data...\n",
            "Size of encoded training data torch.Size([7352, 10000]) and test data torch.Size([2947, 10000])\n",
            "Epoch: 1\n",
            "49 2.1905 18.75\n",
            "99 2.3607 18.75\n",
            "149 2.3786 6.25\n",
            "199 2.0715 25.0\n",
            "249 1.7966 25.0\n",
            "299 2.1223 12.5\n",
            "349 2.1425 18.75\n",
            "399 2.0151 25.0\n",
            "449 2.4769 18.75\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.1382, Accuracy: 469/2947 (15.91%)\n",
            "\n",
            "--- 2.287726402282715 seconds ---\n"
          ]
        }
      ],
      "source": [
        "#ucihar epoch 1 linear-hdc\n",
        "class ArgumentParser:\n",
        "    def __init__(self):\n",
        "        self.args = {\n",
        "            'lr': 0.01,            # Learning rate for optimizing class representative\n",
        "            'gamma': 0.3,          # Kernel parameter for computing covariance\n",
        "            'epoch': 1,            # Epochs of training\n",
        "            'gorder': 16,           # Order of the cyclic group required for G-VSA (8 ou 16)\n",
        "            'dim': 10000,          # Dimension of hypervectors c le grand D\n",
        "            'seed': 43,            # Random seed for reproducing results\n",
        "            'resume': False,       # Resume from existing encoded hypervectors\n",
        "            'data_dir': './encoded_data', # Directory to save encoded data (hypervectors)\n",
        "            'dataset': 'ucihar',    # Dataset choice choices=['mnist', 'fmnist', 'cifar', 'isolet', 'ucihar']\n",
        "            'raw_data_dir': './dataset',   # Raw data directory\n",
        "            'model': 'linear-hdc'    # Model type choices=['rff-hdc', 'linear-hdc', 'rff-gvsa']\n",
        "        }\n",
        "        self.seed = self.args['seed']\n",
        "        self.lr = self.args['lr']\n",
        "        self.gamma = self.args['gamma']\n",
        "        self.epoch = self.args['epoch']\n",
        "        self.gorder = self.args['gorder']\n",
        "        self.dim = self.args['dim']\n",
        "        self.resume = self.args['resume']\n",
        "        self.data_dir = self.args['data_dir']\n",
        "        self.dataset = self.args['dataset']\n",
        "        self.raw_data_dir = self.args['raw_data_dir']\n",
        "        self.model = self.args['model']\n",
        "\n",
        "\n",
        "    def get_args(self):\n",
        "        return self.args\n",
        "args = ArgumentParser()\n",
        "torch.manual_seed(args.seed)\n",
        "np.random.seed(args.seed)\n",
        "if 'hdc' in args.model:\n",
        "    args.gorder = 2\n",
        "    print(\"Use binary HDC with random fourier features, ignoring gorder, set to 2.\")\n",
        "args.data_dir = f'{args.data_dir}/{args.dataset}_{args.model}_order{args.gorder}_gamma{args.gamma}_dim{args.dim}'\n",
        "try:\n",
        "    os.makedirs(args.data_dir)\n",
        "except FileExistsError:\n",
        "    print('Encoded data folder already exists')\n",
        "if not args.resume:\n",
        "    print('Encode the dataset into hypervectors and save')\n",
        "    encode_and_save(args)\n",
        "    print('Finish encoding and saving')\n",
        "print(f'Optimizing class representatives for {args.epoch} epochs')\n",
        "train(args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YcZ4aooSo3KC",
        "outputId": "f8c0e85d-58e3-4718-ddce-d2cbd0cfa446"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Use binary HDC with random fourier features, ignoring gorder, set to 2.\n",
            "Encoded data folder already exists\n",
            "Optimizing class representatives for 10 epochs\n",
            "Loading encoded training data...\n",
            "Loading encoded test data...\n",
            "Size of encoded training data torch.Size([7352, 10000]) and test data torch.Size([2947, 10000])\n",
            "Epoch: 1\n",
            "49 2.1905 18.75\n",
            "99 2.3607 18.75\n",
            "149 2.3786 6.25\n",
            "199 2.0715 25.0\n",
            "249 1.7966 25.0\n",
            "299 2.1223 12.5\n",
            "349 2.1425 18.75\n",
            "399 2.0151 25.0\n",
            "449 2.4769 18.75\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.1382, Accuracy: 469/2947 (15.91%)\n",
            "\n",
            "--- 3.045701026916504 seconds ---\n",
            "Epoch: 2\n",
            "49 0.6541 87.5\n",
            "99 0.7335 75.0\n",
            "149 1.2067 50.0\n",
            "199 0.9138 62.5\n",
            "249 1.1908 62.5\n",
            "299 1.1308 50.0\n",
            "349 1.2650 37.5\n",
            "399 1.0606 50.0\n",
            "449 1.0830 56.25\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.1383, Accuracy: 487/2947 (16.53%)\n",
            "\n",
            "--- 2.320056915283203 seconds ---\n",
            "Epoch: 3\n",
            "49 0.7935 75.0\n",
            "99 0.5335 93.75\n",
            "149 0.8218 68.75\n",
            "199 0.7779 75.0\n",
            "249 0.9077 62.5\n",
            "299 0.9645 62.5\n",
            "349 1.1450 68.75\n",
            "399 1.0969 68.75\n",
            "449 1.2082 43.75\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.1376, Accuracy: 506/2947 (17.17%)\n",
            "\n",
            "--- 2.2978737354278564 seconds ---\n",
            "Epoch: 4\n",
            "49 0.5367 93.75\n",
            "99 0.6005 93.75\n",
            "149 0.8240 81.25\n",
            "199 0.7539 81.25\n",
            "249 1.2234 50.0\n",
            "299 0.8375 93.75\n",
            "349 0.9242 75.0\n",
            "399 1.2645 62.5\n",
            "449 1.0138 56.25\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.1385, Accuracy: 478/2947 (16.22%)\n",
            "\n",
            "--- 2.2946088314056396 seconds ---\n",
            "Epoch: 5\n",
            "49 0.5761 93.75\n",
            "99 0.7071 81.25\n",
            "149 0.6543 93.75\n",
            "199 0.7084 93.75\n",
            "249 0.9052 81.25\n",
            "299 0.7798 81.25\n",
            "349 1.0799 50.0\n",
            "399 0.9290 75.0\n",
            "449 1.0194 62.5\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.1391, Accuracy: 468/2947 (15.88%)\n",
            "\n",
            "--- 2.3120179176330566 seconds ---\n",
            "Epoch: 6\n",
            "49 0.6256 81.25\n",
            "99 0.5780 93.75\n",
            "149 0.6466 87.5\n",
            "199 0.8234 93.75\n",
            "249 0.7269 87.5\n",
            "299 0.7446 81.25\n",
            "349 0.7045 87.5\n",
            "399 0.8566 93.75\n",
            "449 0.9190 75.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.1389, Accuracy: 481/2947 (16.32%)\n",
            "\n",
            "--- 2.910700559616089 seconds ---\n",
            "Epoch: 7\n",
            "49 0.6422 93.75\n",
            "99 0.6079 93.75\n",
            "149 0.7793 81.25\n",
            "199 0.6101 93.75\n",
            "249 0.5071 93.75\n",
            "299 0.9031 75.0\n",
            "349 0.9070 68.75\n",
            "399 0.9274 68.75\n",
            "449 0.9617 75.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.1388, Accuracy: 486/2947 (16.49%)\n",
            "\n",
            "--- 2.3938910961151123 seconds ---\n",
            "Epoch: 8\n",
            "49 0.5459 87.5\n",
            "99 0.7258 87.5\n",
            "149 0.6752 81.25\n",
            "199 0.9721 75.0\n",
            "249 0.7308 81.25\n",
            "299 0.7481 81.25\n",
            "349 0.9497 68.75\n",
            "399 1.0545 62.5\n",
            "449 1.2130 50.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.1393, Accuracy: 499/2947 (16.93%)\n",
            "\n",
            "--- 2.278048515319824 seconds ---\n",
            "Epoch: 9\n",
            "49 0.6732 93.75\n",
            "99 0.7362 81.25\n",
            "149 0.7024 81.25\n",
            "199 0.6450 93.75\n",
            "249 0.7469 93.75\n",
            "299 0.6966 100.0\n",
            "349 1.0509 62.5\n",
            "399 0.7606 87.5\n",
            "449 0.8430 93.75\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.1389, Accuracy: 500/2947 (16.97%)\n",
            "\n",
            "--- 2.315324544906616 seconds ---\n",
            "Epoch: 10\n",
            "49 0.5462 100.0\n",
            "99 0.5774 100.0\n",
            "149 0.8558 75.0\n",
            "199 0.8516 75.0\n",
            "249 0.7982 93.75\n",
            "299 0.9744 75.0\n",
            "349 0.9659 75.0\n",
            "399 0.7679 81.25\n",
            "449 0.9148 75.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.1381, Accuracy: 508/2947 (17.24%)\n",
            "\n",
            "--- 2.288782835006714 seconds ---\n"
          ]
        }
      ],
      "source": [
        "#ucihar epoch 10 linear-hdc\n",
        "class ArgumentParser:\n",
        "    def __init__(self):\n",
        "        self.args = {\n",
        "            'lr': 0.01,            # Learning rate for optimizing class representative\n",
        "            'gamma': 0.3,          # Kernel parameter for computing covariance\n",
        "            'epoch': 10,            # Epochs of training\n",
        "            'gorder': 16,           # Order of the cyclic group required for G-VSA (8 ou 16)\n",
        "            'dim': 10000,          # Dimension of hypervectors c le grand D\n",
        "            'seed': 43,            # Random seed for reproducing results\n",
        "            'resume': True,       # Resume from existing encoded hypervectors\n",
        "            'data_dir': './encoded_data', # Directory to save encoded data (hypervectors)\n",
        "            'dataset': 'ucihar',    # Dataset choice choices=['mnist', 'fmnist', 'cifar', 'isolet', 'ucihar']\n",
        "            'raw_data_dir': './dataset',   # Raw data directory\n",
        "            'model': 'linear-hdc'    # Model type choices=['rff-hdc', 'linear-hdc', 'rff-gvsa']\n",
        "        }\n",
        "        self.seed = self.args['seed']\n",
        "        self.lr = self.args['lr']\n",
        "        self.gamma = self.args['gamma']\n",
        "        self.epoch = self.args['epoch']\n",
        "        self.gorder = self.args['gorder']\n",
        "        self.dim = self.args['dim']\n",
        "        self.resume = self.args['resume']\n",
        "        self.data_dir = self.args['data_dir']\n",
        "        self.dataset = self.args['dataset']\n",
        "        self.raw_data_dir = self.args['raw_data_dir']\n",
        "        self.model = self.args['model']\n",
        "\n",
        "\n",
        "    def get_args(self):\n",
        "        return self.args\n",
        "args = ArgumentParser()\n",
        "torch.manual_seed(args.seed)\n",
        "np.random.seed(args.seed)\n",
        "if 'hdc' in args.model:\n",
        "    args.gorder = 2\n",
        "    print(\"Use binary HDC with random fourier features, ignoring gorder, set to 2.\")\n",
        "args.data_dir = f'{args.data_dir}/{args.dataset}_{args.model}_order{args.gorder}_gamma{args.gamma}_dim{args.dim}'\n",
        "try:\n",
        "    os.makedirs(args.data_dir)\n",
        "except FileExistsError:\n",
        "    print('Encoded data folder already exists')\n",
        "if not args.resume:\n",
        "    print('Encode the dataset into hypervectors and save')\n",
        "    encode_and_save(args)\n",
        "    print('Finish encoding and saving')\n",
        "print(f'Optimizing class representatives for {args.epoch} epochs')\n",
        "train(args)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MNIST"
      ],
      "metadata": {
        "id": "dFQrl8hDvwJt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9a7zUDhBoVu4",
        "outputId": "44e9935f-2c50-4834-861a-d44c30b175d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Use binary HDC with random fourier features, ignoring gorder, set to 2.\n",
            "Encode the dataset into hypervectors and save\n",
            "# of channels of data 1\n",
            "# of training samples and test samples 60000 10000\n",
            "Encoding to binary HDC with linear hamming distance.\n",
            "generating linear item memory...\n",
            "Encoded pixels to hypervectors with size:  torch.Size([256, 10000])\n",
            "Encoding training data...\n",
            "start encoding data here\n",
            "1000 images encoded\n",
            "2000 images encoded\n",
            "3000 images encoded\n",
            "4000 images encoded\n",
            "5000 images encoded\n",
            "6000 images encoded\n",
            "7000 images encoded\n",
            "8000 images encoded\n",
            "9000 images encoded\n",
            "10000 images encoded\n",
            "11000 images encoded\n",
            "12000 images encoded\n",
            "13000 images encoded\n",
            "14000 images encoded\n",
            "15000 images encoded\n",
            "16000 images encoded\n",
            "17000 images encoded\n",
            "18000 images encoded\n",
            "19000 images encoded\n",
            "20000 images encoded\n",
            "21000 images encoded\n",
            "22000 images encoded\n",
            "23000 images encoded\n",
            "24000 images encoded\n",
            "25000 images encoded\n",
            "26000 images encoded\n",
            "27000 images encoded\n",
            "28000 images encoded\n",
            "29000 images encoded\n",
            "30000 images encoded\n",
            "31000 images encoded\n",
            "32000 images encoded\n",
            "33000 images encoded\n",
            "34000 images encoded\n",
            "35000 images encoded\n",
            "36000 images encoded\n",
            "37000 images encoded\n",
            "38000 images encoded\n",
            "39000 images encoded\n",
            "40000 images encoded\n",
            "41000 images encoded\n",
            "42000 images encoded\n",
            "43000 images encoded\n",
            "44000 images encoded\n",
            "45000 images encoded\n",
            "46000 images encoded\n",
            "47000 images encoded\n",
            "48000 images encoded\n",
            "49000 images encoded\n",
            "50000 images encoded\n",
            "51000 images encoded\n",
            "52000 images encoded\n",
            "53000 images encoded\n",
            "54000 images encoded\n",
            "55000 images encoded\n",
            "56000 images encoded\n",
            "57000 images encoded\n",
            "58000 images encoded\n",
            "59000 images encoded\n",
            "60000 images encoded\n",
            "finish encoding data here\n",
            "Encoding test data...\n",
            "start encoding data here\n",
            "1000 images encoded\n",
            "2000 images encoded\n",
            "3000 images encoded\n",
            "4000 images encoded\n",
            "5000 images encoded\n",
            "6000 images encoded\n",
            "7000 images encoded\n",
            "8000 images encoded\n",
            "9000 images encoded\n",
            "10000 images encoded\n",
            "finish encoding data here\n",
            "Finish encoding and saving\n",
            "Optimizing class representatives for 1 epochs\n",
            "Loading encoded training data...\n",
            "Loading encoded test data...\n",
            "Size of encoded training data torch.Size([60000, 10000]) and test data torch.Size([10000, 10000])\n",
            "Epoch: 1\n",
            "49 3.0296 0.0\n",
            "99 2.6802 12.5\n",
            "149 2.9251 6.25\n",
            "199 2.5587 6.25\n",
            "249 2.5108 25.0\n",
            "299 3.0860 6.25\n",
            "349 2.6826 12.5\n",
            "399 2.6276 6.25\n",
            "449 2.6436 25.0\n",
            "499 2.7960 6.25\n",
            "549 2.9707 6.25\n",
            "599 2.4753 25.0\n",
            "649 2.5953 18.75\n",
            "699 2.7661 6.25\n",
            "749 3.0150 6.25\n",
            "799 2.6468 0.0\n",
            "849 2.5650 6.25\n",
            "899 3.0149 6.25\n",
            "949 2.7726 12.5\n",
            "999 2.6841 18.75\n",
            "1049 2.7173 6.25\n",
            "1099 2.6282 6.25\n",
            "1149 2.5664 12.5\n",
            "1199 2.7995 0.0\n",
            "1249 2.7478 18.75\n",
            "1299 2.6889 6.25\n",
            "1349 2.6242 12.5\n",
            "1399 2.4804 18.75\n",
            "1449 2.4388 6.25\n",
            "1499 2.8117 12.5\n",
            "1549 2.6075 12.5\n",
            "1599 2.7489 0.0\n",
            "1649 2.7472 12.5\n",
            "1699 2.7073 12.5\n",
            "1749 2.5681 6.25\n",
            "1799 2.7572 12.5\n",
            "1849 2.8757 12.5\n",
            "1899 2.9989 18.75\n",
            "1949 2.9197 0.0\n",
            "1999 2.6215 12.5\n",
            "2049 3.0145 6.25\n",
            "2099 2.3239 18.75\n",
            "2149 2.8342 18.75\n",
            "2199 2.9348 6.25\n",
            "2249 2.5619 12.5\n",
            "2299 2.3253 25.0\n",
            "2349 2.5338 18.75\n",
            "2399 3.0528 6.25\n",
            "2449 3.1191 6.25\n",
            "2499 2.7522 12.5\n",
            "2549 2.5901 6.25\n",
            "2599 2.7072 12.5\n",
            "2649 3.0125 6.25\n",
            "2699 2.5972 18.75\n",
            "2749 2.6556 0.0\n",
            "2799 2.9146 6.25\n",
            "2849 2.6381 12.5\n",
            "2899 2.6140 12.5\n",
            "2949 2.5601 18.75\n",
            "2999 2.5982 12.5\n",
            "3049 2.6778 25.0\n",
            "3099 2.6494 12.5\n",
            "3149 2.7145 0.0\n",
            "3199 2.6356 12.5\n",
            "3249 2.7515 0.0\n",
            "3299 2.6635 12.5\n",
            "3349 2.2521 12.5\n",
            "3399 3.2225 6.25\n",
            "3449 2.7345 18.75\n",
            "3499 2.7799 12.5\n",
            "3549 2.5571 25.0\n",
            "3599 2.8041 25.0\n",
            "3649 3.0582 12.5\n",
            "3699 2.4601 18.75\n",
            "3749 3.0654 6.25\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.1724, Accuracy: 1041/10000 (10.41%)\n",
            "\n",
            "--- 16.09254813194275 seconds ---\n"
          ]
        }
      ],
      "source": [
        "#mnist epoch 1 linear-hdc\n",
        "class ArgumentParser:\n",
        "    def __init__(self):\n",
        "        self.args = {\n",
        "            'lr': 0.01,            # Learning rate for optimizing class representative\n",
        "            'gamma': 0.3,          # Kernel parameter for computing covariance\n",
        "            'epoch': 1,            # Epochs of training\n",
        "            'gorder': 16,           # Order of the cyclic group required for G-VSA (8 ou 16)\n",
        "            'dim': 10000,          # Dimension of hypervectors c le grand D\n",
        "            'seed': 43,            # Random seed for reproducing results\n",
        "            'resume': False,       # Resume from existing encoded hypervectors\n",
        "            'data_dir': './encoded_data', # Directory to save encoded data (hypervectors)\n",
        "            'dataset': 'mnist',    # Dataset choice choices=['mnist', 'fmnist', 'cifar', 'isolet', 'ucihar']\n",
        "            'raw_data_dir': './dataset',   # Raw data directory\n",
        "            'model': 'linear-hdc'    # Model type choices=['rff-hdc', 'linear-hdc', 'rff-gvsa']\n",
        "        }\n",
        "        self.seed = self.args['seed']\n",
        "        self.lr = self.args['lr']\n",
        "        self.gamma = self.args['gamma']\n",
        "        self.epoch = self.args['epoch']\n",
        "        self.gorder = self.args['gorder']\n",
        "        self.dim = self.args['dim']\n",
        "        self.resume = self.args['resume']\n",
        "        self.data_dir = self.args['data_dir']\n",
        "        self.dataset = self.args['dataset']\n",
        "        self.raw_data_dir = self.args['raw_data_dir']\n",
        "        self.model = self.args['model']\n",
        "\n",
        "\n",
        "    def get_args(self):\n",
        "        return self.args\n",
        "args = ArgumentParser()\n",
        "torch.manual_seed(args.seed)\n",
        "np.random.seed(args.seed)\n",
        "if 'hdc' in args.model:\n",
        "    args.gorder = 2\n",
        "    print(\"Use binary HDC with random fourier features, ignoring gorder, set to 2.\")\n",
        "args.data_dir = f'{args.data_dir}/{args.dataset}_{args.model}_order{args.gorder}_gamma{args.gamma}_dim{args.dim}'\n",
        "try:\n",
        "    os.makedirs(args.data_dir)\n",
        "except FileExistsError:\n",
        "    print('Encoded data folder already exists')\n",
        "if not args.resume:\n",
        "    print('Encode the dataset into hypervectors and save')\n",
        "    encode_and_save(args)\n",
        "    print('Finish encoding and saving')\n",
        "print(f'Optimizing class representatives for {args.epoch} epochs')\n",
        "train(args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jc-R6hJyoZnI",
        "outputId": "0fb660a7-5fe6-4add-f3fd-fd64686194c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Use binary HDC with random fourier features, ignoring gorder, set to 2.\n",
            "Encoded data folder already exists\n",
            "Optimizing class representatives for 10 epochs\n",
            "Loading encoded training data...\n",
            "Loading encoded test data...\n",
            "Size of encoded training data torch.Size([60000, 10000]) and test data torch.Size([10000, 10000])\n",
            "Epoch: 1\n",
            "49 3.0296 0.0\n",
            "99 2.6802 12.5\n",
            "149 2.9251 6.25\n",
            "199 2.5587 6.25\n",
            "249 2.5108 25.0\n",
            "299 3.0860 6.25\n",
            "349 2.6826 12.5\n",
            "399 2.6276 6.25\n",
            "449 2.6436 25.0\n",
            "499 2.7960 6.25\n",
            "549 2.9707 6.25\n",
            "599 2.4753 25.0\n",
            "649 2.5953 18.75\n",
            "699 2.7661 6.25\n",
            "749 3.0150 6.25\n",
            "799 2.6468 0.0\n",
            "849 2.5650 6.25\n",
            "899 3.0149 6.25\n",
            "949 2.7726 12.5\n",
            "999 2.6841 18.75\n",
            "1049 2.7173 6.25\n",
            "1099 2.6282 6.25\n",
            "1149 2.5664 12.5\n",
            "1199 2.7995 0.0\n",
            "1249 2.7478 18.75\n",
            "1299 2.6889 6.25\n",
            "1349 2.6242 12.5\n",
            "1399 2.4804 18.75\n",
            "1449 2.4388 6.25\n",
            "1499 2.8117 12.5\n",
            "1549 2.6075 12.5\n",
            "1599 2.7489 0.0\n",
            "1649 2.7472 12.5\n",
            "1699 2.7073 12.5\n",
            "1749 2.5681 6.25\n",
            "1799 2.7572 12.5\n",
            "1849 2.8757 12.5\n",
            "1899 2.9989 18.75\n",
            "1949 2.9197 0.0\n",
            "1999 2.6215 12.5\n",
            "2049 3.0145 6.25\n",
            "2099 2.3239 18.75\n",
            "2149 2.8342 18.75\n",
            "2199 2.9348 6.25\n",
            "2249 2.5619 12.5\n",
            "2299 2.3253 25.0\n",
            "2349 2.5338 18.75\n",
            "2399 3.0528 6.25\n",
            "2449 3.1191 6.25\n",
            "2499 2.7522 12.5\n",
            "2549 2.5901 6.25\n",
            "2599 2.7072 12.5\n",
            "2649 3.0125 6.25\n",
            "2699 2.5972 18.75\n",
            "2749 2.6556 0.0\n",
            "2799 2.9146 6.25\n",
            "2849 2.6381 12.5\n",
            "2899 2.6140 12.5\n",
            "2949 2.5601 18.75\n",
            "2999 2.5982 12.5\n",
            "3049 2.6778 25.0\n",
            "3099 2.6494 12.5\n",
            "3149 2.7145 0.0\n",
            "3199 2.6356 12.5\n",
            "3249 2.7515 0.0\n",
            "3299 2.6635 12.5\n",
            "3349 2.2521 12.5\n",
            "3399 3.2225 6.25\n",
            "3449 2.7345 18.75\n",
            "3499 2.7799 12.5\n",
            "3549 2.5571 25.0\n",
            "3599 2.8041 25.0\n",
            "3649 3.0582 12.5\n",
            "3699 2.4601 18.75\n",
            "3749 3.0654 6.25\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.1724, Accuracy: 1041/10000 (10.41%)\n",
            "\n",
            "--- 16.31844973564148 seconds ---\n",
            "Epoch: 2\n",
            "49 2.1170 12.5\n",
            "99 2.1391 18.75\n",
            "149 1.9506 25.0\n",
            "199 2.2674 12.5\n",
            "249 2.2004 31.25\n",
            "299 1.8719 50.0\n",
            "349 1.8807 31.25\n",
            "399 2.0891 25.0\n",
            "449 2.1402 31.25\n",
            "499 2.0695 37.5\n",
            "549 2.0048 25.0\n",
            "599 1.7014 37.5\n",
            "649 2.1011 25.0\n",
            "699 2.2514 25.0\n",
            "749 1.8482 18.75\n",
            "799 2.1590 18.75\n",
            "849 1.8963 31.25\n",
            "899 1.9586 25.0\n",
            "949 2.1040 25.0\n",
            "999 2.2625 18.75\n",
            "1049 2.2584 18.75\n",
            "1099 2.1779 25.0\n",
            "1149 2.6678 6.25\n",
            "1199 1.6811 31.25\n",
            "1249 2.5804 25.0\n",
            "1299 1.7595 37.5\n",
            "1349 2.0424 37.5\n",
            "1399 2.7034 6.25\n",
            "1449 2.4693 12.5\n",
            "1499 2.5729 18.75\n",
            "1549 2.3832 12.5\n",
            "1599 2.3415 6.25\n",
            "1649 2.3503 25.0\n",
            "1699 1.9118 31.25\n",
            "1749 1.8140 37.5\n",
            "1799 2.4413 18.75\n",
            "1849 1.9610 43.75\n",
            "1899 2.2807 37.5\n",
            "1949 2.2327 12.5\n",
            "1999 2.5437 0.0\n",
            "2049 2.4001 18.75\n",
            "2099 2.3177 25.0\n",
            "2149 2.3436 18.75\n",
            "2199 2.4964 12.5\n",
            "2249 2.2811 12.5\n",
            "2299 2.2693 6.25\n",
            "2349 2.0900 31.25\n",
            "2399 2.5092 0.0\n",
            "2449 2.5323 18.75\n",
            "2499 2.5056 6.25\n",
            "2549 2.5463 12.5\n",
            "2599 2.4646 12.5\n",
            "2649 2.2343 25.0\n",
            "2699 2.3934 25.0\n",
            "2749 2.5826 12.5\n",
            "2799 2.3087 18.75\n",
            "2849 2.6500 12.5\n",
            "2899 2.7845 12.5\n",
            "2949 2.5389 12.5\n",
            "2999 2.2597 18.75\n",
            "3049 2.4168 12.5\n",
            "3099 2.7629 6.25\n",
            "3149 2.4053 31.25\n",
            "3199 2.2011 12.5\n",
            "3249 2.9050 6.25\n",
            "3299 2.4771 6.25\n",
            "3349 2.4495 12.5\n",
            "3399 2.6922 0.0\n",
            "3449 2.3042 12.5\n",
            "3499 2.3137 18.75\n",
            "3549 2.2903 18.75\n",
            "3599 2.9273 12.5\n",
            "3649 2.7729 6.25\n",
            "3699 2.7250 25.0\n",
            "3749 2.2904 0.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.1725, Accuracy: 1030/10000 (10.30%)\n",
            "\n",
            "--- 16.241703271865845 seconds ---\n",
            "Epoch: 3\n",
            "49 1.8986 37.5\n",
            "99 1.8883 31.25\n",
            "149 1.6723 37.5\n",
            "199 1.3742 56.25\n",
            "249 2.0444 31.25\n",
            "299 2.2936 12.5\n",
            "349 1.9760 43.75\n",
            "399 2.0784 18.75\n",
            "449 2.0684 18.75\n",
            "499 2.5872 18.75\n",
            "549 2.2615 31.25\n",
            "599 2.2449 31.25\n",
            "649 2.0664 43.75\n",
            "699 1.9897 37.5\n",
            "749 1.9050 25.0\n",
            "799 2.1264 18.75\n",
            "849 2.2347 25.0\n",
            "899 2.2219 25.0\n",
            "949 2.1799 25.0\n",
            "999 2.3675 31.25\n",
            "1049 2.3016 18.75\n",
            "1099 2.1098 18.75\n",
            "1149 2.1605 18.75\n",
            "1199 2.0861 18.75\n",
            "1249 2.2579 18.75\n",
            "1299 1.9709 18.75\n",
            "1349 2.3075 18.75\n",
            "1399 2.0064 25.0\n",
            "1449 2.2765 18.75\n",
            "1499 2.4007 25.0\n",
            "1549 2.3315 18.75\n",
            "1599 1.8896 25.0\n",
            "1649 2.6794 6.25\n",
            "1699 2.3937 18.75\n",
            "1749 2.3697 12.5\n",
            "1799 2.1987 25.0\n",
            "1849 2.2772 12.5\n",
            "1899 2.4283 12.5\n",
            "1949 1.8570 37.5\n",
            "1999 2.2631 6.25\n",
            "2049 2.4786 12.5\n",
            "2099 2.1102 12.5\n",
            "2149 2.3944 12.5\n",
            "2199 2.3382 18.75\n",
            "2249 1.9078 37.5\n",
            "2299 2.2276 12.5\n",
            "2349 2.2051 31.25\n",
            "2399 2.6420 18.75\n",
            "2449 2.0496 18.75\n",
            "2499 2.0683 18.75\n",
            "2549 2.2999 18.75\n",
            "2599 2.3808 12.5\n",
            "2649 2.3602 6.25\n",
            "2699 2.3545 18.75\n",
            "2749 2.7021 6.25\n",
            "2799 2.3095 12.5\n",
            "2849 2.5937 0.0\n",
            "2899 2.4719 6.25\n",
            "2949 2.5318 18.75\n",
            "2999 2.4330 0.0\n",
            "3049 2.1528 31.25\n",
            "3099 2.5744 6.25\n",
            "3149 2.6015 12.5\n",
            "3199 1.9129 37.5\n",
            "3249 2.5207 12.5\n",
            "3299 2.6165 12.5\n",
            "3349 2.7301 12.5\n",
            "3399 2.7376 25.0\n",
            "3449 2.2685 18.75\n",
            "3499 2.4648 12.5\n",
            "3549 2.7749 0.0\n",
            "3599 3.0255 6.25\n",
            "3649 2.5394 6.25\n",
            "3699 2.4479 12.5\n",
            "3749 2.8776 12.5\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.1719, Accuracy: 985/10000 (9.85%)\n",
            "\n",
            "--- 16.94250774383545 seconds ---\n",
            "Epoch: 4\n",
            "49 1.6523 31.25\n",
            "99 1.4987 56.25\n",
            "149 1.9310 31.25\n",
            "199 1.5630 56.25\n",
            "249 1.8783 18.75\n",
            "299 2.3148 25.0\n",
            "349 1.8961 31.25\n",
            "399 1.7450 37.5\n",
            "449 1.8342 31.25\n",
            "499 1.7823 25.0\n",
            "549 1.6746 56.25\n",
            "599 1.8898 25.0\n",
            "649 1.7919 43.75\n",
            "699 2.0787 25.0\n",
            "749 1.8880 25.0\n",
            "799 2.3915 25.0\n",
            "849 1.8228 31.25\n",
            "899 2.2143 25.0\n",
            "949 2.1461 18.75\n",
            "999 2.2068 12.5\n",
            "1049 1.9926 25.0\n",
            "1099 2.6271 12.5\n",
            "1149 1.9318 31.25\n",
            "1199 2.0952 18.75\n",
            "1249 2.4044 18.75\n",
            "1299 2.2967 12.5\n",
            "1349 2.0209 12.5\n",
            "1399 1.8950 31.25\n",
            "1449 2.0136 31.25\n",
            "1499 2.0514 25.0\n",
            "1549 1.7502 31.25\n",
            "1599 2.5549 25.0\n",
            "1649 2.2868 18.75\n",
            "1699 2.2126 12.5\n",
            "1749 2.3593 25.0\n",
            "1799 1.9633 37.5\n",
            "1849 2.1835 31.25\n",
            "1899 2.2962 12.5\n",
            "1949 2.1394 37.5\n",
            "1999 2.2258 12.5\n",
            "2049 2.0672 37.5\n",
            "2099 2.5259 12.5\n",
            "2149 2.5499 18.75\n",
            "2199 1.8448 37.5\n",
            "2249 2.2694 31.25\n",
            "2299 2.4042 12.5\n",
            "2349 2.5039 6.25\n",
            "2399 2.2623 18.75\n",
            "2449 2.1344 43.75\n",
            "2499 2.2988 12.5\n",
            "2549 2.6403 12.5\n",
            "2599 2.0120 25.0\n",
            "2649 2.4594 25.0\n",
            "2699 1.8179 31.25\n",
            "2749 2.2941 18.75\n",
            "2799 2.7982 12.5\n",
            "2849 2.7124 12.5\n",
            "2899 2.0055 31.25\n",
            "2949 2.6233 12.5\n",
            "2999 2.3760 18.75\n",
            "3049 2.1261 12.5\n",
            "3099 2.4169 12.5\n",
            "3149 2.6816 12.5\n",
            "3199 2.4313 31.25\n",
            "3249 2.1365 18.75\n",
            "3299 2.4376 12.5\n",
            "3349 2.4813 12.5\n",
            "3399 2.5005 18.75\n",
            "3449 2.5525 12.5\n",
            "3499 2.8227 12.5\n",
            "3549 2.4766 12.5\n",
            "3599 2.3215 12.5\n",
            "3649 2.5240 0.0\n",
            "3699 2.2651 25.0\n",
            "3749 2.3773 6.25\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.1720, Accuracy: 999/10000 (9.99%)\n",
            "\n",
            "--- 16.23823571205139 seconds ---\n",
            "Epoch: 5\n",
            "49 1.9003 37.5\n",
            "99 1.4471 43.75\n",
            "149 1.8105 50.0\n",
            "199 1.8827 37.5\n",
            "249 1.9543 37.5\n",
            "299 1.6282 43.75\n",
            "349 1.9568 43.75\n",
            "399 1.8376 37.5\n",
            "449 1.9208 25.0\n",
            "499 2.0202 25.0\n",
            "549 2.3918 12.5\n",
            "599 1.6444 62.5\n",
            "649 2.0299 31.25\n",
            "699 1.8442 50.0\n",
            "749 2.1341 25.0\n",
            "799 1.9089 43.75\n",
            "849 1.7085 37.5\n",
            "899 1.8023 43.75\n",
            "949 2.4125 12.5\n",
            "999 2.3898 31.25\n",
            "1049 1.9068 31.25\n",
            "1099 2.1811 12.5\n",
            "1149 2.4863 18.75\n",
            "1199 1.9496 37.5\n",
            "1249 1.8181 31.25\n",
            "1299 1.9420 25.0\n",
            "1349 2.0035 31.25\n",
            "1399 1.8311 31.25\n",
            "1449 1.9790 37.5\n",
            "1499 1.7971 31.25\n",
            "1549 2.2726 18.75\n",
            "1599 2.3818 12.5\n",
            "1649 2.4577 25.0\n",
            "1699 2.0611 25.0\n",
            "1749 2.2154 18.75\n",
            "1799 2.2990 25.0\n",
            "1849 2.1595 12.5\n",
            "1899 2.0664 18.75\n",
            "1949 2.0198 18.75\n",
            "1999 2.5237 12.5\n",
            "2049 2.1913 18.75\n",
            "2099 2.4147 18.75\n",
            "2149 2.2308 25.0\n",
            "2199 2.4322 31.25\n",
            "2249 2.3865 12.5\n",
            "2299 2.4833 18.75\n",
            "2349 1.9602 31.25\n",
            "2399 1.9405 31.25\n",
            "2449 2.2883 25.0\n",
            "2499 2.3419 25.0\n",
            "2549 2.4223 12.5\n",
            "2599 2.2063 18.75\n",
            "2649 2.1025 18.75\n",
            "2699 1.9616 37.5\n",
            "2749 2.7101 6.25\n",
            "2799 1.9337 31.25\n",
            "2849 1.8176 37.5\n",
            "2899 2.0419 18.75\n",
            "2949 2.0430 18.75\n",
            "2999 2.4698 18.75\n",
            "3049 2.3249 18.75\n",
            "3099 2.2873 18.75\n",
            "3149 2.6700 0.0\n",
            "3199 1.9717 31.25\n",
            "3249 2.3518 12.5\n",
            "3299 1.9603 43.75\n",
            "3349 2.0182 12.5\n",
            "3399 2.1136 37.5\n",
            "3449 2.6253 6.25\n",
            "3499 2.5030 12.5\n",
            "3549 2.2346 12.5\n",
            "3599 2.5042 12.5\n",
            "3649 2.2356 18.75\n",
            "3699 2.3642 31.25\n",
            "3749 2.3797 25.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.1725, Accuracy: 963/10000 (9.63%)\n",
            "\n",
            "--- 16.272551774978638 seconds ---\n",
            "Epoch: 6\n",
            "49 1.7081 43.75\n",
            "99 2.2124 18.75\n",
            "149 1.8786 50.0\n",
            "199 1.8586 31.25\n",
            "249 2.1265 31.25\n",
            "299 1.7651 25.0\n",
            "349 1.8826 50.0\n",
            "399 2.5855 12.5\n",
            "449 1.8618 31.25\n",
            "499 2.0641 18.75\n",
            "549 2.3224 12.5\n",
            "599 2.0606 31.25\n",
            "649 1.8049 43.75\n",
            "699 2.0259 25.0\n",
            "749 2.0136 25.0\n",
            "799 2.0794 31.25\n",
            "849 2.1137 25.0\n",
            "899 1.8273 18.75\n",
            "949 2.0873 6.25\n",
            "999 1.7820 37.5\n",
            "1049 2.1087 31.25\n",
            "1099 2.1872 18.75\n",
            "1149 2.0385 12.5\n",
            "1199 1.9585 18.75\n",
            "1249 2.2056 18.75\n",
            "1299 1.9473 43.75\n",
            "1349 2.1845 25.0\n",
            "1399 1.9964 31.25\n",
            "1449 2.3758 18.75\n",
            "1499 2.2180 18.75\n",
            "1549 2.1576 6.25\n",
            "1599 1.7736 25.0\n",
            "1649 2.3143 25.0\n",
            "1699 2.0998 18.75\n",
            "1749 2.0163 37.5\n",
            "1799 2.0290 25.0\n",
            "1849 1.8441 43.75\n",
            "1899 2.1381 12.5\n",
            "1949 2.5022 0.0\n",
            "1999 2.1953 25.0\n",
            "2049 2.1707 18.75\n",
            "2099 2.1040 12.5\n",
            "2149 2.3202 18.75\n",
            "2199 1.6614 56.25\n",
            "2249 2.1164 18.75\n",
            "2299 2.2235 18.75\n",
            "2349 2.5307 12.5\n",
            "2399 2.3522 12.5\n",
            "2449 2.3571 6.25\n",
            "2499 1.9449 18.75\n",
            "2549 2.3513 25.0\n",
            "2599 2.4711 6.25\n",
            "2649 2.2382 12.5\n",
            "2699 2.6985 12.5\n",
            "2749 2.3207 12.5\n",
            "2799 2.3583 18.75\n",
            "2849 2.2745 12.5\n",
            "2899 2.1000 18.75\n",
            "2949 1.7705 25.0\n",
            "2999 2.3503 18.75\n",
            "3049 2.4470 12.5\n",
            "3099 2.0314 31.25\n",
            "3149 2.3151 18.75\n",
            "3199 2.0131 43.75\n",
            "3249 2.4591 18.75\n",
            "3299 2.3124 12.5\n",
            "3349 2.5723 18.75\n",
            "3399 2.2697 12.5\n",
            "3449 2.2955 25.0\n",
            "3499 2.2382 12.5\n",
            "3549 2.7138 6.25\n",
            "3599 2.5229 18.75\n",
            "3649 2.6638 12.5\n",
            "3699 2.4410 12.5\n",
            "3749 2.5146 6.25\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.1724, Accuracy: 1006/10000 (10.06%)\n",
            "\n",
            "--- 17.082547426223755 seconds ---\n",
            "Epoch: 7\n",
            "49 1.5379 56.25\n",
            "99 1.7158 43.75\n",
            "149 1.3824 56.25\n",
            "199 1.9989 25.0\n",
            "249 1.8787 56.25\n",
            "299 1.9275 37.5\n",
            "349 1.8455 43.75\n",
            "399 2.1850 31.25\n",
            "449 1.5117 31.25\n",
            "499 1.9921 25.0\n",
            "549 1.9994 37.5\n",
            "599 2.1739 12.5\n",
            "649 1.9621 43.75\n",
            "699 1.9540 25.0\n",
            "749 1.9538 31.25\n",
            "799 2.4677 25.0\n",
            "849 2.2009 18.75\n",
            "899 2.0189 37.5\n",
            "949 1.8941 25.0\n",
            "999 1.9858 25.0\n",
            "1049 2.2818 18.75\n",
            "1099 1.9645 31.25\n",
            "1149 2.0577 18.75\n",
            "1199 1.7377 50.0\n",
            "1249 2.4599 18.75\n",
            "1299 2.2359 25.0\n",
            "1349 2.3135 12.5\n",
            "1399 2.3162 18.75\n",
            "1449 1.8970 31.25\n",
            "1499 2.2150 6.25\n",
            "1549 2.0029 18.75\n",
            "1599 2.3943 25.0\n",
            "1649 1.8940 25.0\n",
            "1699 1.9145 37.5\n",
            "1749 2.3389 12.5\n",
            "1799 2.4542 6.25\n",
            "1849 2.0584 31.25\n",
            "1899 2.4709 6.25\n",
            "1949 2.5116 18.75\n",
            "1999 1.9387 31.25\n",
            "2049 2.1867 18.75\n",
            "2099 2.1086 25.0\n",
            "2149 1.9326 37.5\n",
            "2199 2.4465 6.25\n",
            "2249 2.2608 12.5\n",
            "2299 1.9726 18.75\n",
            "2349 1.9462 37.5\n",
            "2399 2.3921 18.75\n",
            "2449 2.0852 25.0\n",
            "2499 2.4121 18.75\n",
            "2549 2.4772 18.75\n",
            "2599 2.3977 12.5\n",
            "2649 2.0250 43.75\n",
            "2699 1.8061 50.0\n",
            "2749 2.0033 25.0\n",
            "2799 2.5865 18.75\n",
            "2849 2.4889 12.5\n",
            "2899 2.4023 25.0\n",
            "2949 2.1817 25.0\n",
            "2999 2.0456 43.75\n",
            "3049 2.2248 18.75\n",
            "3099 2.3867 12.5\n",
            "3149 2.0835 31.25\n",
            "3199 2.3090 31.25\n",
            "3249 2.4189 25.0\n",
            "3299 1.9342 31.25\n",
            "3349 2.2609 31.25\n",
            "3399 2.5345 12.5\n",
            "3449 2.1555 31.25\n",
            "3499 2.3825 25.0\n",
            "3549 2.0987 12.5\n",
            "3599 2.2718 18.75\n",
            "3649 2.4073 25.0\n",
            "3699 2.3463 6.25\n",
            "3749 2.5014 18.75\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.1718, Accuracy: 1002/10000 (10.02%)\n",
            "\n",
            "--- 16.13621187210083 seconds ---\n",
            "Epoch: 8\n",
            "49 1.8078 43.75\n",
            "99 1.9501 31.25\n",
            "149 1.7640 25.0\n",
            "199 2.0667 50.0\n",
            "249 1.8455 25.0\n",
            "299 1.9282 37.5\n",
            "349 1.8316 37.5\n",
            "399 1.9179 31.25\n",
            "449 2.1409 37.5\n",
            "499 1.9928 43.75\n",
            "549 1.9261 31.25\n",
            "599 1.9965 25.0\n",
            "649 1.7195 37.5\n",
            "699 2.1520 25.0\n",
            "749 1.6649 43.75\n",
            "799 2.4581 18.75\n",
            "849 2.0073 18.75\n",
            "899 1.9967 25.0\n",
            "949 1.9503 25.0\n",
            "999 1.9321 43.75\n",
            "1049 1.9013 25.0\n",
            "1099 1.8984 25.0\n",
            "1149 2.1379 18.75\n",
            "1199 1.8622 37.5\n",
            "1249 2.0103 25.0\n",
            "1299 1.7131 50.0\n",
            "1349 2.2441 18.75\n",
            "1399 2.0869 25.0\n",
            "1449 2.0748 37.5\n",
            "1499 2.0451 37.5\n",
            "1549 2.3392 12.5\n",
            "1599 1.9106 37.5\n",
            "1649 2.3334 12.5\n",
            "1699 1.9214 43.75\n",
            "1749 1.6179 50.0\n",
            "1799 2.0290 25.0\n",
            "1849 2.5581 0.0\n",
            "1899 2.0965 6.25\n",
            "1949 2.6571 12.5\n",
            "1999 2.1407 31.25\n",
            "2049 2.2737 18.75\n",
            "2099 2.1584 25.0\n",
            "2149 2.1565 25.0\n",
            "2199 2.0025 31.25\n",
            "2249 2.1684 25.0\n",
            "2299 2.2902 25.0\n",
            "2349 2.2353 18.75\n",
            "2399 2.2921 25.0\n",
            "2449 2.2306 18.75\n",
            "2499 2.2627 25.0\n",
            "2549 2.2141 6.25\n",
            "2599 2.0238 31.25\n",
            "2649 2.3131 18.75\n",
            "2699 2.1677 18.75\n",
            "2749 2.5547 12.5\n",
            "2799 2.1282 31.25\n",
            "2849 2.0115 25.0\n",
            "2899 2.3735 6.25\n",
            "2949 2.0880 18.75\n",
            "2999 2.2380 31.25\n",
            "3049 2.4773 18.75\n",
            "3099 2.4869 18.75\n",
            "3149 2.2453 25.0\n",
            "3199 1.7466 37.5\n",
            "3249 2.5743 12.5\n",
            "3299 2.4603 25.0\n",
            "3349 2.6765 6.25\n",
            "3399 2.5901 0.0\n",
            "3449 2.2894 18.75\n",
            "3499 2.5032 25.0\n",
            "3549 2.4104 25.0\n",
            "3599 2.4821 6.25\n",
            "3649 2.5587 12.5\n",
            "3699 2.6388 6.25\n",
            "3749 2.2569 18.75\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.1718, Accuracy: 994/10000 (9.94%)\n",
            "\n",
            "--- 16.20952534675598 seconds ---\n",
            "Epoch: 9\n",
            "49 1.6425 56.25\n",
            "99 1.6980 56.25\n",
            "149 1.9119 50.0\n",
            "199 1.9150 12.5\n",
            "249 1.6792 62.5\n",
            "299 2.1404 25.0\n",
            "349 1.6748 31.25\n",
            "399 2.3314 6.25\n",
            "449 2.0327 18.75\n",
            "499 1.9410 18.75\n",
            "549 2.0399 25.0\n",
            "599 2.1745 31.25\n",
            "649 1.8359 43.75\n",
            "699 2.0004 18.75\n",
            "749 1.9230 43.75\n",
            "799 1.8340 25.0\n",
            "849 1.8371 31.25\n",
            "899 2.2947 18.75\n",
            "949 2.0069 37.5\n",
            "999 1.9868 18.75\n",
            "1049 2.0299 18.75\n",
            "1099 2.0623 6.25\n",
            "1149 1.9712 31.25\n",
            "1199 1.8586 18.75\n",
            "1249 1.9072 37.5\n",
            "1299 2.0060 31.25\n",
            "1349 2.3569 12.5\n",
            "1399 2.0862 31.25\n",
            "1449 2.0548 37.5\n",
            "1499 2.4483 31.25\n",
            "1549 2.1045 37.5\n",
            "1599 1.8969 31.25\n",
            "1649 2.0291 31.25\n",
            "1699 2.5822 0.0\n",
            "1749 2.2160 25.0\n",
            "1799 1.9978 37.5\n",
            "1849 1.8850 31.25\n",
            "1899 2.2182 18.75\n",
            "1949 1.8536 37.5\n",
            "1999 2.2476 6.25\n",
            "2049 2.2510 12.5\n",
            "2099 1.9384 31.25\n",
            "2149 2.4085 12.5\n",
            "2199 2.1622 18.75\n",
            "2249 2.2906 25.0\n",
            "2299 2.1148 25.0\n",
            "2349 1.8613 37.5\n",
            "2399 2.2848 12.5\n",
            "2449 2.3195 12.5\n",
            "2499 2.5628 18.75\n",
            "2549 2.3870 12.5\n",
            "2599 1.9913 37.5\n",
            "2649 2.1961 18.75\n",
            "2699 2.0000 31.25\n",
            "2749 2.0850 25.0\n",
            "2799 2.3543 12.5\n",
            "2849 2.4378 6.25\n",
            "2899 2.1158 18.75\n",
            "2949 2.1805 18.75\n",
            "2999 2.1404 31.25\n",
            "3049 2.5845 18.75\n",
            "3099 2.3717 25.0\n",
            "3149 2.2240 12.5\n",
            "3199 1.9101 18.75\n",
            "3249 2.2663 25.0\n",
            "3299 2.0577 31.25\n",
            "3349 2.3171 31.25\n",
            "3399 2.2008 12.5\n",
            "3449 2.4487 18.75\n",
            "3499 2.0805 31.25\n",
            "3549 2.7428 6.25\n",
            "3599 2.5993 12.5\n",
            "3649 2.3681 31.25\n",
            "3699 2.1436 31.25\n",
            "3749 2.0452 31.25\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.1723, Accuracy: 996/10000 (9.96%)\n",
            "\n",
            "--- 16.385948419570923 seconds ---\n",
            "Epoch: 10\n",
            "49 2.1516 31.25\n",
            "99 1.4928 68.75\n",
            "149 1.7956 43.75\n",
            "199 2.0814 18.75\n",
            "249 1.8028 37.5\n",
            "299 1.8943 31.25\n",
            "349 1.7784 56.25\n",
            "399 2.2179 12.5\n",
            "449 2.0537 12.5\n",
            "499 1.8878 43.75\n",
            "549 1.8157 50.0\n",
            "599 1.8931 31.25\n",
            "649 2.1079 25.0\n",
            "699 2.2062 18.75\n",
            "749 2.0921 0.0\n",
            "799 2.0131 25.0\n",
            "849 1.8302 31.25\n",
            "899 2.0206 18.75\n",
            "949 2.1909 31.25\n",
            "999 2.0807 25.0\n",
            "1049 1.9801 37.5\n",
            "1099 2.1243 18.75\n",
            "1149 2.0778 25.0\n",
            "1199 2.0410 31.25\n",
            "1249 2.3479 25.0\n",
            "1299 2.0459 37.5\n",
            "1349 2.2625 12.5\n",
            "1399 2.4035 18.75\n",
            "1449 2.2777 18.75\n",
            "1499 2.0133 25.0\n",
            "1549 2.0659 18.75\n",
            "1599 2.4236 25.0\n",
            "1649 2.2000 25.0\n",
            "1699 1.9320 37.5\n",
            "1749 2.8432 6.25\n",
            "1799 2.2315 18.75\n",
            "1849 2.2623 18.75\n",
            "1899 1.9086 43.75\n",
            "1949 1.8570 37.5\n",
            "1999 2.0117 25.0\n",
            "2049 2.3507 25.0\n",
            "2099 2.3233 12.5\n",
            "2149 2.1690 43.75\n",
            "2199 1.9304 37.5\n",
            "2249 2.1598 37.5\n",
            "2299 1.8544 31.25\n",
            "2349 2.0929 25.0\n",
            "2399 2.1049 25.0\n",
            "2449 2.0674 12.5\n",
            "2499 2.3998 12.5\n",
            "2549 2.1005 25.0\n",
            "2599 2.1207 25.0\n",
            "2649 2.3301 18.75\n",
            "2699 1.8654 25.0\n",
            "2749 2.0843 25.0\n",
            "2799 2.5192 6.25\n",
            "2849 2.4008 18.75\n",
            "2899 2.3425 12.5\n",
            "2949 2.3085 6.25\n",
            "2999 2.2322 18.75\n",
            "3049 2.2766 12.5\n",
            "3099 2.1755 18.75\n",
            "3149 2.0905 25.0\n",
            "3199 2.7455 0.0\n",
            "3249 1.9800 31.25\n",
            "3299 2.3621 18.75\n",
            "3349 1.9643 18.75\n",
            "3399 2.7793 12.5\n",
            "3449 2.0763 6.25\n",
            "3499 2.4637 12.5\n",
            "3549 2.5765 12.5\n",
            "3599 2.2257 25.0\n",
            "3649 2.1919 25.0\n",
            "3699 2.3149 18.75\n",
            "3749 2.5259 12.5\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.1719, Accuracy: 1000/10000 (10.00%)\n",
            "\n",
            "--- 16.66964817047119 seconds ---\n"
          ]
        }
      ],
      "source": [
        "#mnist epoch 10 linear-hdc\n",
        "class ArgumentParser:\n",
        "    def __init__(self):\n",
        "        self.args = {\n",
        "            'lr': 0.01,            # Learning rate for optimizing class representative\n",
        "            'gamma': 0.3,          # Kernel parameter for computing covariance\n",
        "            'epoch': 10,            # Epochs of training\n",
        "            'gorder': 16,           # Order of the cyclic group required for G-VSA (8 ou 16)\n",
        "            'dim': 10000,          # Dimension of hypervectors c le grand D\n",
        "            'seed': 43,            # Random seed for reproducing results\n",
        "            'resume': True,       # Resume from existing encoded hypervectors\n",
        "            'data_dir': './encoded_data', # Directory to save encoded data (hypervectors)\n",
        "            'dataset': 'mnist',    # Dataset choice choices=['mnist', 'fmnist', 'cifar', 'isolet', 'ucihar']\n",
        "            'raw_data_dir': './dataset',   # Raw data directory\n",
        "            'model': 'linear-hdc'    # Model type choices=['rff-hdc', 'linear-hdc', 'rff-gvsa']\n",
        "        }\n",
        "        self.seed = self.args['seed']\n",
        "        self.lr = self.args['lr']\n",
        "        self.gamma = self.args['gamma']\n",
        "        self.epoch = self.args['epoch']\n",
        "        self.gorder = self.args['gorder']\n",
        "        self.dim = self.args['dim']\n",
        "        self.resume = self.args['resume']\n",
        "        self.data_dir = self.args['data_dir']\n",
        "        self.dataset = self.args['dataset']\n",
        "        self.raw_data_dir = self.args['raw_data_dir']\n",
        "        self.model = self.args['model']\n",
        "\n",
        "\n",
        "    def get_args(self):\n",
        "        return self.args\n",
        "args = ArgumentParser()\n",
        "torch.manual_seed(args.seed)\n",
        "np.random.seed(args.seed)\n",
        "if 'hdc' in args.model:\n",
        "    args.gorder = 2\n",
        "    print(\"Use binary HDC with random fourier features, ignoring gorder, set to 2.\")\n",
        "args.data_dir = f'{args.data_dir}/{args.dataset}_{args.model}_order{args.gorder}_gamma{args.gamma}_dim{args.dim}'\n",
        "try:\n",
        "    os.makedirs(args.data_dir)\n",
        "except FileExistsError:\n",
        "    print('Encoded data folder already exists')\n",
        "if not args.resume:\n",
        "    print('Encode the dataset into hypervectors and save')\n",
        "    encode_and_save(args)\n",
        "    print('Finish encoding and saving')\n",
        "print(f'Optimizing class representatives for {args.epoch} epochs')\n",
        "train(args)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FMNIST"
      ],
      "metadata": {
        "id": "25DngCzNvx3k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#fminst epoch 1 linear-hdc\n",
        "class ArgumentParser:\n",
        "    def __init__(self):\n",
        "        self.args = {\n",
        "            'lr': 0.01,            # Learning rate for optimizing class representative\n",
        "            'gamma': 0.3,          # Kernel parameter for computing covariance\n",
        "            'epoch': 1,            # Epochs of training\n",
        "            'gorder': 16,           # Order of the cyclic group required for G-VSA (8 ou 16)\n",
        "            'dim': 10000,          # Dimension of hypervectors c le grand D\n",
        "            'seed': 43,            # Random seed for reproducing results\n",
        "            'resume': False,       # Resume from existing encoded hypervectors\n",
        "            'data_dir': './encoded_data', # Directory to save encoded data (hypervectors)\n",
        "            'dataset': 'fmnist',    # Dataset choice choices=['mnist', 'fmnist', 'cifar', 'isolet', 'ucihar']\n",
        "            'raw_data_dir': './dataset',   # Raw data directory\n",
        "            'model': 'linear-hdc'    # Model type choices=['rff-hdc', 'linear-hdc', 'rff-gvsa']\n",
        "        }\n",
        "        self.seed = self.args['seed']\n",
        "        self.lr = self.args['lr']\n",
        "        self.gamma = self.args['gamma']\n",
        "        self.epoch = self.args['epoch']\n",
        "        self.gorder = self.args['gorder']\n",
        "        self.dim = self.args['dim']\n",
        "        self.resume = self.args['resume']\n",
        "        self.data_dir = self.args['data_dir']\n",
        "        self.dataset = self.args['dataset']\n",
        "        self.raw_data_dir = self.args['raw_data_dir']\n",
        "        self.model = self.args['model']\n",
        "\n",
        "\n",
        "    def get_args(self):\n",
        "        return self.args\n",
        "\n",
        "args = ArgumentParser()\n",
        "torch.manual_seed(args.seed)\n",
        "np.random.seed(args.seed)\n",
        "if 'hdc' in args.model:\n",
        "    args.gorder = 2\n",
        "    print(\"Use binary HDC with random fourier features, ignoring gorder, set to 2.\")\n",
        "args.data_dir = f'{args.data_dir}/{args.dataset}_{args.model}_order{args.gorder}_gamma{args.gamma}_dim{args.dim}'\n",
        "try:\n",
        "    os.makedirs(args.data_dir)\n",
        "except FileExistsError:\n",
        "    print('Encoded data folder already exists')\n",
        "if not args.resume:\n",
        "    print('Encode the dataset into hypervectors and save')\n",
        "    encode_and_save(args)\n",
        "    print('Finish encoding and saving')\n",
        "print(f'Optimizing class representatives for {args.epoch} epochs')\n",
        "train(args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2IakHkDzpI7u",
        "outputId": "31d1bf79-0a11-4b75-e1b8-6e461bf36a66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Use binary HDC with random fourier features, ignoring gorder, set to 2.\n",
            "Encode the dataset into hypervectors and save\n",
            "# of channels of data 1\n",
            "# of training samples and test samples 60000 10000\n",
            "Encoding to binary HDC with linear hamming distance.\n",
            "generating linear item memory...\n",
            "Encoded pixels to hypervectors with size:  torch.Size([256, 10000])\n",
            "Encoding training data...\n",
            "start encoding data here\n",
            "1000 images encoded\n",
            "2000 images encoded\n",
            "3000 images encoded\n",
            "4000 images encoded\n",
            "5000 images encoded\n",
            "6000 images encoded\n",
            "7000 images encoded\n",
            "8000 images encoded\n",
            "9000 images encoded\n",
            "10000 images encoded\n",
            "11000 images encoded\n",
            "12000 images encoded\n",
            "13000 images encoded\n",
            "14000 images encoded\n",
            "15000 images encoded\n",
            "16000 images encoded\n",
            "17000 images encoded\n",
            "18000 images encoded\n",
            "19000 images encoded\n",
            "20000 images encoded\n",
            "21000 images encoded\n",
            "22000 images encoded\n",
            "23000 images encoded\n",
            "24000 images encoded\n",
            "25000 images encoded\n",
            "26000 images encoded\n",
            "27000 images encoded\n",
            "28000 images encoded\n",
            "29000 images encoded\n",
            "30000 images encoded\n",
            "31000 images encoded\n",
            "32000 images encoded\n",
            "33000 images encoded\n",
            "34000 images encoded\n",
            "35000 images encoded\n",
            "36000 images encoded\n",
            "37000 images encoded\n",
            "38000 images encoded\n",
            "39000 images encoded\n",
            "40000 images encoded\n",
            "41000 images encoded\n",
            "42000 images encoded\n",
            "43000 images encoded\n",
            "44000 images encoded\n",
            "45000 images encoded\n",
            "46000 images encoded\n",
            "47000 images encoded\n",
            "48000 images encoded\n",
            "49000 images encoded\n",
            "50000 images encoded\n",
            "51000 images encoded\n",
            "52000 images encoded\n",
            "53000 images encoded\n",
            "54000 images encoded\n",
            "55000 images encoded\n",
            "56000 images encoded\n",
            "57000 images encoded\n",
            "58000 images encoded\n",
            "59000 images encoded\n",
            "60000 images encoded\n",
            "finish encoding data here\n",
            "Encoding test data...\n",
            "start encoding data here\n",
            "1000 images encoded\n",
            "2000 images encoded\n",
            "3000 images encoded\n",
            "4000 images encoded\n",
            "5000 images encoded\n",
            "6000 images encoded\n",
            "7000 images encoded\n",
            "8000 images encoded\n",
            "9000 images encoded\n",
            "10000 images encoded\n",
            "finish encoding data here\n",
            "Finish encoding and saving\n",
            "Optimizing class representatives for 1 epochs\n",
            "Loading encoded training data...\n",
            "Loading encoded test data...\n",
            "Size of encoded training data torch.Size([60000, 10000]) and test data torch.Size([10000, 10000])\n",
            "Epoch: 1\n",
            "49 2.5683 18.75\n",
            "99 2.8018 18.75\n",
            "149 2.5834 18.75\n",
            "199 2.7154 0.0\n",
            "249 2.4274 12.5\n",
            "299 2.2016 25.0\n",
            "349 2.6011 12.5\n",
            "399 2.4826 6.25\n",
            "449 2.8634 0.0\n",
            "499 2.3867 18.75\n",
            "549 2.8498 6.25\n",
            "599 2.8376 0.0\n",
            "649 3.1383 0.0\n",
            "699 2.7834 12.5\n",
            "749 2.5576 6.25\n",
            "799 2.5611 12.5\n",
            "849 2.7648 18.75\n",
            "899 2.6754 12.5\n",
            "949 2.6073 31.25\n",
            "999 3.1782 0.0\n",
            "1049 2.5408 12.5\n",
            "1099 2.2742 31.25\n",
            "1149 3.2473 0.0\n",
            "1199 3.0077 12.5\n",
            "1249 2.7598 12.5\n",
            "1299 2.7092 0.0\n",
            "1349 2.6402 12.5\n",
            "1399 2.8426 0.0\n",
            "1449 2.1192 50.0\n",
            "1499 2.9140 12.5\n",
            "1549 2.9010 6.25\n",
            "1599 2.5573 18.75\n",
            "1649 2.9761 0.0\n",
            "1699 3.0149 6.25\n",
            "1749 3.1668 6.25\n",
            "1799 2.5949 6.25\n",
            "1849 2.7213 12.5\n",
            "1899 3.3172 0.0\n",
            "1949 2.5461 18.75\n",
            "1999 2.6413 12.5\n",
            "2049 2.9021 18.75\n",
            "2099 2.9972 18.75\n",
            "2149 2.7281 6.25\n",
            "2199 2.9928 0.0\n",
            "2249 2.3696 18.75\n",
            "2299 2.5249 12.5\n",
            "2349 2.8804 0.0\n",
            "2399 2.7384 0.0\n",
            "2449 2.8721 0.0\n",
            "2499 2.7827 0.0\n",
            "2549 2.3692 12.5\n",
            "2599 2.7630 6.25\n",
            "2649 2.9588 12.5\n",
            "2699 2.4065 25.0\n",
            "2749 2.1431 18.75\n",
            "2799 3.2772 6.25\n",
            "2849 2.7198 12.5\n",
            "2899 2.6801 18.75\n",
            "2949 2.5404 6.25\n",
            "2999 2.7526 0.0\n",
            "3049 3.1165 12.5\n",
            "3099 2.4268 6.25\n",
            "3149 2.5034 12.5\n",
            "3199 2.8156 6.25\n",
            "3249 2.6141 6.25\n",
            "3299 2.5532 6.25\n",
            "3349 2.9872 12.5\n",
            "3399 2.5595 12.5\n",
            "3449 2.7196 6.25\n",
            "3499 2.6622 25.0\n",
            "3549 2.5681 12.5\n",
            "3599 3.0803 0.0\n",
            "3649 2.9999 6.25\n",
            "3699 2.4094 6.25\n",
            "3749 2.9045 6.25\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.1715, Accuracy: 1019/10000 (10.19%)\n",
            "\n",
            "--- 15.846480131149292 seconds ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z5HConJNoEUV",
        "outputId": "5ed54b8e-d779-43c8-cd74-d8c1d8aa999a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Use binary HDC with random fourier features, ignoring gorder, set to 2.\n",
            "Encoded data folder already exists\n",
            "Optimizing class representatives for 10 epochs\n",
            "Loading encoded training data...\n",
            "Loading encoded test data...\n",
            "Size of encoded training data torch.Size([60000, 10000]) and test data torch.Size([10000, 10000])\n",
            "Epoch: 1\n",
            "49 2.5683 18.75\n",
            "99 2.8018 18.75\n",
            "149 2.5834 18.75\n",
            "199 2.7154 0.0\n",
            "249 2.4274 12.5\n",
            "299 2.2016 25.0\n",
            "349 2.6011 12.5\n",
            "399 2.4826 6.25\n",
            "449 2.8634 0.0\n",
            "499 2.3867 18.75\n",
            "549 2.8498 6.25\n",
            "599 2.8376 0.0\n",
            "649 3.1383 0.0\n",
            "699 2.7834 12.5\n",
            "749 2.5576 6.25\n",
            "799 2.5611 12.5\n",
            "849 2.7648 18.75\n",
            "899 2.6754 12.5\n",
            "949 2.6073 31.25\n",
            "999 3.1782 0.0\n",
            "1049 2.5408 12.5\n",
            "1099 2.2742 31.25\n",
            "1149 3.2473 0.0\n",
            "1199 3.0077 12.5\n",
            "1249 2.7598 12.5\n",
            "1299 2.7092 0.0\n",
            "1349 2.6402 12.5\n",
            "1399 2.8426 0.0\n",
            "1449 2.1192 50.0\n",
            "1499 2.9140 12.5\n",
            "1549 2.9010 6.25\n",
            "1599 2.5573 18.75\n",
            "1649 2.9761 0.0\n",
            "1699 3.0149 6.25\n",
            "1749 3.1668 6.25\n",
            "1799 2.5949 6.25\n",
            "1849 2.7213 12.5\n",
            "1899 3.3172 0.0\n",
            "1949 2.5461 18.75\n",
            "1999 2.6413 12.5\n",
            "2049 2.9021 18.75\n",
            "2099 2.9972 18.75\n",
            "2149 2.7281 6.25\n",
            "2199 2.9928 0.0\n",
            "2249 2.3696 18.75\n",
            "2299 2.5249 12.5\n",
            "2349 2.8804 0.0\n",
            "2399 2.7384 0.0\n",
            "2449 2.8721 0.0\n",
            "2499 2.7827 0.0\n",
            "2549 2.3692 12.5\n",
            "2599 2.7630 6.25\n",
            "2649 2.9588 12.5\n",
            "2699 2.4065 25.0\n",
            "2749 2.1431 18.75\n",
            "2799 3.2772 6.25\n",
            "2849 2.7198 12.5\n",
            "2899 2.6801 18.75\n",
            "2949 2.5404 6.25\n",
            "2999 2.7526 0.0\n",
            "3049 3.1165 12.5\n",
            "3099 2.4268 6.25\n",
            "3149 2.5034 12.5\n",
            "3199 2.8156 6.25\n",
            "3249 2.6141 6.25\n",
            "3299 2.5532 6.25\n",
            "3349 2.9872 12.5\n",
            "3399 2.5595 12.5\n",
            "3449 2.7196 6.25\n",
            "3499 2.6622 25.0\n",
            "3549 2.5681 12.5\n",
            "3599 3.0803 0.0\n",
            "3649 2.9999 6.25\n",
            "3699 2.4094 6.25\n",
            "3749 2.9045 6.25\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.1715, Accuracy: 1019/10000 (10.19%)\n",
            "\n",
            "--- 15.640072584152222 seconds ---\n",
            "Epoch: 2\n",
            "49 2.4355 25.0\n",
            "99 1.9481 43.75\n",
            "149 2.1745 37.5\n",
            "199 1.9104 37.5\n",
            "249 2.5004 12.5\n",
            "299 2.0671 31.25\n",
            "349 1.6728 43.75\n",
            "399 2.0357 31.25\n",
            "449 1.8770 43.75\n",
            "499 1.9603 31.25\n",
            "549 2.2196 25.0\n",
            "599 1.6531 31.25\n",
            "649 2.4542 0.0\n",
            "699 2.5312 18.75\n",
            "749 2.0476 31.25\n",
            "799 2.0187 25.0\n",
            "849 2.2347 18.75\n",
            "899 1.9512 18.75\n",
            "949 2.0982 25.0\n",
            "999 2.5809 12.5\n",
            "1049 1.8150 43.75\n",
            "1099 2.5096 12.5\n",
            "1149 2.3146 31.25\n",
            "1199 2.3375 25.0\n",
            "1249 1.9283 37.5\n",
            "1299 2.2665 18.75\n",
            "1349 2.1650 12.5\n",
            "1399 1.9725 25.0\n",
            "1449 2.0342 25.0\n",
            "1499 2.2843 6.25\n",
            "1549 2.5888 12.5\n",
            "1599 2.3569 50.0\n",
            "1649 2.0524 18.75\n",
            "1699 2.2654 18.75\n",
            "1749 2.2979 18.75\n",
            "1799 2.3313 12.5\n",
            "1849 2.1902 25.0\n",
            "1899 2.3140 12.5\n",
            "1949 2.5005 25.0\n",
            "1999 2.2546 6.25\n",
            "2049 2.2951 31.25\n",
            "2099 2.4926 25.0\n",
            "2149 2.0365 37.5\n",
            "2199 2.4052 6.25\n",
            "2249 2.3568 12.5\n",
            "2299 2.7189 0.0\n",
            "2349 2.4859 18.75\n",
            "2399 2.5553 12.5\n",
            "2449 2.7468 0.0\n",
            "2499 2.4254 18.75\n",
            "2549 2.7526 25.0\n",
            "2599 3.0123 6.25\n",
            "2649 2.3095 25.0\n",
            "2699 2.3574 12.5\n",
            "2749 2.3376 12.5\n",
            "2799 2.7465 6.25\n",
            "2849 2.6326 12.5\n",
            "2899 2.3796 12.5\n",
            "2949 2.4200 18.75\n",
            "2999 2.5340 12.5\n",
            "3049 2.4684 6.25\n",
            "3099 2.3793 6.25\n",
            "3149 2.6983 12.5\n",
            "3199 2.5234 18.75\n",
            "3249 2.1240 31.25\n",
            "3299 2.7565 18.75\n",
            "3349 2.9353 0.0\n",
            "3399 2.6788 18.75\n",
            "3449 2.3952 18.75\n",
            "3499 2.8455 12.5\n",
            "3549 2.4314 12.5\n",
            "3599 2.5552 12.5\n",
            "3649 2.5045 12.5\n",
            "3699 2.4229 12.5\n",
            "3749 2.1604 12.5\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.1720, Accuracy: 992/10000 (9.92%)\n",
            "\n",
            "--- 15.577473878860474 seconds ---\n",
            "Epoch: 3\n",
            "49 1.9062 37.5\n",
            "99 2.0256 31.25\n",
            "149 1.8680 31.25\n",
            "199 1.5944 43.75\n",
            "249 1.7362 31.25\n",
            "299 2.0855 37.5\n",
            "349 2.0009 31.25\n",
            "399 1.5930 56.25\n",
            "449 1.9861 25.0\n",
            "499 2.1546 31.25\n",
            "549 2.1785 37.5\n",
            "599 2.1070 25.0\n",
            "649 1.5954 43.75\n",
            "699 1.8928 37.5\n",
            "749 1.6629 43.75\n",
            "799 2.5148 18.75\n",
            "849 2.5366 12.5\n",
            "899 2.3264 12.5\n",
            "949 1.9701 31.25\n",
            "999 2.2445 25.0\n",
            "1049 1.9798 31.25\n",
            "1099 2.2177 18.75\n",
            "1149 2.4523 12.5\n",
            "1199 2.2183 18.75\n",
            "1249 2.0724 18.75\n",
            "1299 2.1684 12.5\n",
            "1349 2.2287 25.0\n",
            "1399 2.2967 6.25\n",
            "1449 2.1382 31.25\n",
            "1499 2.4174 18.75\n",
            "1549 2.0193 37.5\n",
            "1599 2.5040 18.75\n",
            "1649 2.4292 6.25\n",
            "1699 2.1663 12.5\n",
            "1749 2.3549 25.0\n",
            "1799 1.9476 43.75\n",
            "1849 1.9552 18.75\n",
            "1899 2.3289 12.5\n",
            "1949 2.8935 12.5\n",
            "1999 2.6623 12.5\n",
            "2049 2.2891 6.25\n",
            "2099 2.1830 18.75\n",
            "2149 2.6172 0.0\n",
            "2199 2.4251 12.5\n",
            "2249 2.1756 37.5\n",
            "2299 2.2381 25.0\n",
            "2349 2.4118 18.75\n",
            "2399 2.3219 25.0\n",
            "2449 2.6988 6.25\n",
            "2499 2.4095 0.0\n",
            "2549 2.6371 6.25\n",
            "2599 2.4564 6.25\n",
            "2649 1.5343 50.0\n",
            "2699 2.6275 6.25\n",
            "2749 2.7902 6.25\n",
            "2799 2.1252 12.5\n",
            "2849 2.7007 18.75\n",
            "2899 2.7742 6.25\n",
            "2949 2.5053 6.25\n",
            "2999 2.3816 25.0\n",
            "3049 2.3304 18.75\n",
            "3099 2.2351 25.0\n",
            "3149 2.7082 0.0\n",
            "3199 2.4564 37.5\n",
            "3249 2.5251 12.5\n",
            "3299 2.1821 12.5\n",
            "3349 2.4821 12.5\n",
            "3399 2.1922 12.5\n",
            "3449 2.4452 18.75\n",
            "3499 2.1627 18.75\n",
            "3549 2.4544 6.25\n",
            "3599 2.7177 25.0\n",
            "3649 2.1979 18.75\n",
            "3699 2.3372 18.75\n",
            "3749 2.1762 25.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.1718, Accuracy: 1009/10000 (10.09%)\n",
            "\n",
            "--- 16.173743963241577 seconds ---\n",
            "Epoch: 4\n",
            "49 2.0757 12.5\n",
            "99 2.0777 25.0\n",
            "149 1.9644 25.0\n",
            "199 2.1101 25.0\n",
            "249 1.6562 50.0\n",
            "299 1.7202 31.25\n",
            "349 1.6441 37.5\n",
            "399 1.6433 75.0\n",
            "449 1.7696 31.25\n",
            "499 1.8008 37.5\n",
            "549 1.7751 31.25\n",
            "599 2.1790 18.75\n",
            "649 1.6744 50.0\n",
            "699 2.3202 18.75\n",
            "749 2.0745 12.5\n",
            "799 2.6031 18.75\n",
            "849 1.8083 37.5\n",
            "899 1.8873 31.25\n",
            "949 1.8143 37.5\n",
            "999 2.4413 18.75\n",
            "1049 1.8208 37.5\n",
            "1099 2.2311 31.25\n",
            "1149 2.3664 12.5\n",
            "1199 1.9211 25.0\n",
            "1249 1.9931 37.5\n",
            "1299 2.2059 25.0\n",
            "1349 2.1606 12.5\n",
            "1399 2.3606 18.75\n",
            "1449 2.1224 18.75\n",
            "1499 2.3181 25.0\n",
            "1549 1.9909 25.0\n",
            "1599 2.0324 18.75\n",
            "1649 2.0285 18.75\n",
            "1699 2.2795 12.5\n",
            "1749 1.9176 25.0\n",
            "1799 2.1330 31.25\n",
            "1849 2.2385 12.5\n",
            "1899 2.1047 12.5\n",
            "1949 1.8788 37.5\n",
            "1999 2.2899 12.5\n",
            "2049 2.1389 18.75\n",
            "2099 2.0931 18.75\n",
            "2149 2.6285 12.5\n",
            "2199 2.3223 18.75\n",
            "2249 2.0657 31.25\n",
            "2299 2.3370 12.5\n",
            "2349 2.3231 18.75\n",
            "2399 2.2050 12.5\n",
            "2449 2.3889 12.5\n",
            "2499 2.3929 18.75\n",
            "2549 1.8862 37.5\n",
            "2599 2.1526 31.25\n",
            "2649 2.2662 6.25\n",
            "2699 2.6304 12.5\n",
            "2749 2.2307 18.75\n",
            "2799 2.3746 25.0\n",
            "2849 2.3228 25.0\n",
            "2899 2.5951 12.5\n",
            "2949 2.5835 0.0\n",
            "2999 2.4752 6.25\n",
            "3049 2.6049 6.25\n",
            "3099 2.1738 18.75\n",
            "3149 2.3271 18.75\n",
            "3199 2.6891 12.5\n",
            "3249 1.7912 25.0\n",
            "3299 2.3489 6.25\n",
            "3349 2.2557 37.5\n",
            "3399 2.2418 31.25\n",
            "3449 2.1347 18.75\n",
            "3499 2.0310 31.25\n",
            "3549 1.8871 37.5\n",
            "3599 2.2184 18.75\n",
            "3649 2.2389 18.75\n",
            "3699 2.6025 12.5\n",
            "3749 2.2616 12.5\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.1711, Accuracy: 1063/10000 (10.63%)\n",
            "\n",
            "--- 16.900943279266357 seconds ---\n",
            "Epoch: 5\n",
            "49 1.8841 12.5\n",
            "99 1.7544 43.75\n",
            "149 2.0013 12.5\n",
            "199 1.9839 31.25\n",
            "249 1.9473 43.75\n",
            "299 2.0808 25.0\n",
            "349 1.6808 50.0\n",
            "399 1.8094 43.75\n",
            "449 1.7620 50.0\n",
            "499 2.5942 12.5\n",
            "549 1.6951 43.75\n",
            "599 1.7061 56.25\n",
            "649 1.9498 37.5\n",
            "699 2.0032 25.0\n",
            "749 2.0958 25.0\n",
            "799 1.8499 31.25\n",
            "849 1.9507 43.75\n",
            "899 1.9311 25.0\n",
            "949 2.1234 25.0\n",
            "999 2.4629 6.25\n",
            "1049 2.0429 37.5\n",
            "1099 2.3912 18.75\n",
            "1149 2.0321 31.25\n",
            "1199 2.1503 12.5\n",
            "1249 2.1945 25.0\n",
            "1299 1.6224 50.0\n",
            "1349 2.0493 31.25\n",
            "1399 2.3272 6.25\n",
            "1449 1.7327 37.5\n",
            "1499 2.7401 6.25\n",
            "1549 2.0591 25.0\n",
            "1599 2.1608 12.5\n",
            "1649 2.2571 25.0\n",
            "1699 2.0896 18.75\n",
            "1749 2.2319 31.25\n",
            "1799 2.4625 18.75\n",
            "1849 2.2756 25.0\n",
            "1899 1.9665 6.25\n",
            "1949 2.5649 6.25\n",
            "1999 2.0034 31.25\n",
            "2049 1.9477 37.5\n",
            "2099 2.5066 18.75\n",
            "2149 2.4348 12.5\n",
            "2199 2.7382 6.25\n",
            "2249 2.3795 6.25\n",
            "2299 2.2154 6.25\n",
            "2349 2.0448 37.5\n",
            "2399 2.7710 6.25\n",
            "2449 2.3837 12.5\n",
            "2499 2.4092 6.25\n",
            "2549 2.0818 43.75\n",
            "2599 1.7253 31.25\n",
            "2649 2.0138 50.0\n",
            "2699 2.1039 31.25\n",
            "2749 2.1519 25.0\n",
            "2799 2.3283 12.5\n",
            "2849 2.1976 31.25\n",
            "2899 1.9459 43.75\n",
            "2949 2.2598 31.25\n",
            "2999 2.1985 31.25\n",
            "3049 2.6384 0.0\n",
            "3099 2.3812 18.75\n",
            "3149 2.0971 18.75\n",
            "3199 2.3412 18.75\n",
            "3249 2.5967 0.0\n",
            "3299 2.1666 37.5\n",
            "3349 2.1622 18.75\n",
            "3399 2.2063 25.0\n",
            "3449 2.6132 18.75\n",
            "3499 2.1313 25.0\n",
            "3549 2.1952 37.5\n",
            "3599 2.2837 25.0\n",
            "3649 2.5517 18.75\n",
            "3699 2.4104 6.25\n",
            "3749 2.5204 18.75\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.1710, Accuracy: 1066/10000 (10.66%)\n",
            "\n",
            "--- 16.13567852973938 seconds ---\n",
            "Epoch: 6\n",
            "49 1.7359 31.25\n",
            "99 1.9140 18.75\n",
            "149 1.9111 25.0\n",
            "199 1.8453 50.0\n",
            "249 1.9332 43.75\n",
            "299 2.3668 6.25\n",
            "349 1.9762 37.5\n",
            "399 2.0404 18.75\n",
            "449 1.7556 43.75\n",
            "499 1.8682 25.0\n",
            "549 1.9087 37.5\n",
            "599 1.9960 18.75\n",
            "649 2.0415 25.0\n",
            "699 1.8630 18.75\n",
            "749 2.1072 25.0\n",
            "799 1.4295 43.75\n",
            "849 2.2651 6.25\n",
            "899 1.9500 37.5\n",
            "949 2.1058 31.25\n",
            "999 2.1159 37.5\n",
            "1049 1.9054 31.25\n",
            "1099 2.1006 37.5\n",
            "1149 2.1414 12.5\n",
            "1199 1.9795 37.5\n",
            "1249 2.1454 25.0\n",
            "1299 2.1982 37.5\n",
            "1349 1.8059 31.25\n",
            "1399 2.2604 31.25\n",
            "1449 2.2434 18.75\n",
            "1499 2.0358 25.0\n",
            "1549 2.0234 37.5\n",
            "1599 1.9478 31.25\n",
            "1649 2.2919 18.75\n",
            "1699 2.2523 18.75\n",
            "1749 2.0416 31.25\n",
            "1799 2.1191 31.25\n",
            "1849 2.1603 18.75\n",
            "1899 1.9448 37.5\n",
            "1949 2.1887 25.0\n",
            "1999 1.9414 43.75\n",
            "2049 2.2355 25.0\n",
            "2099 2.2044 18.75\n",
            "2149 2.1781 31.25\n",
            "2199 2.0353 12.5\n",
            "2249 2.4237 6.25\n",
            "2299 2.5145 18.75\n",
            "2349 2.0374 25.0\n",
            "2399 2.2669 18.75\n",
            "2449 2.6106 6.25\n",
            "2499 1.9374 12.5\n",
            "2549 2.5017 6.25\n",
            "2599 1.9751 43.75\n",
            "2649 2.0706 37.5\n",
            "2699 2.2533 25.0\n",
            "2749 2.1612 25.0\n",
            "2799 2.3391 6.25\n",
            "2849 2.3664 18.75\n",
            "2899 1.9420 31.25\n",
            "2949 2.2931 18.75\n",
            "2999 2.0691 18.75\n",
            "3049 2.3612 12.5\n",
            "3099 2.8679 6.25\n",
            "3149 2.3149 12.5\n",
            "3199 2.4666 6.25\n",
            "3249 2.4593 25.0\n",
            "3299 2.2875 18.75\n",
            "3349 2.4571 18.75\n",
            "3399 2.6231 6.25\n",
            "3449 2.4012 12.5\n",
            "3499 2.6236 12.5\n",
            "3549 2.9086 6.25\n",
            "3599 2.4950 6.25\n",
            "3649 2.4809 12.5\n",
            "3699 2.3190 25.0\n",
            "3749 2.4773 18.75\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.1711, Accuracy: 1005/10000 (10.05%)\n",
            "\n",
            "--- 16.05625557899475 seconds ---\n",
            "Epoch: 7\n",
            "49 1.4984 62.5\n",
            "99 2.3170 25.0\n",
            "149 1.8540 25.0\n",
            "199 1.8387 37.5\n",
            "249 1.6377 43.75\n",
            "299 1.7725 43.75\n",
            "349 2.1840 31.25\n",
            "399 1.9347 31.25\n",
            "449 1.9073 25.0\n",
            "499 1.8563 43.75\n",
            "549 1.8247 31.25\n",
            "599 2.0055 37.5\n",
            "649 1.8467 31.25\n",
            "699 1.8952 25.0\n",
            "749 1.8501 43.75\n",
            "799 2.1089 25.0\n",
            "849 2.0776 18.75\n",
            "899 2.0561 25.0\n",
            "949 2.5567 18.75\n",
            "999 1.9121 25.0\n",
            "1049 2.0156 25.0\n",
            "1099 1.9654 31.25\n",
            "1149 1.8994 31.25\n",
            "1199 2.1220 12.5\n",
            "1249 1.8654 37.5\n",
            "1299 2.2266 12.5\n",
            "1349 2.3879 25.0\n",
            "1399 2.2471 25.0\n",
            "1449 2.1352 25.0\n",
            "1499 2.0960 18.75\n",
            "1549 2.4609 18.75\n",
            "1599 2.0047 25.0\n",
            "1649 1.9112 18.75\n",
            "1699 1.7873 37.5\n",
            "1749 2.2777 31.25\n",
            "1799 2.2380 6.25\n",
            "1849 1.7388 50.0\n",
            "1899 2.0726 12.5\n",
            "1949 2.2060 25.0\n",
            "1999 2.4818 18.75\n",
            "2049 1.7344 37.5\n",
            "2099 2.0216 25.0\n",
            "2149 2.1080 31.25\n",
            "2199 2.5214 12.5\n",
            "2249 2.1702 37.5\n",
            "2299 1.8846 25.0\n",
            "2349 2.2319 18.75\n",
            "2399 1.9933 37.5\n",
            "2449 2.5473 12.5\n",
            "2499 2.1471 18.75\n",
            "2549 1.9559 37.5\n",
            "2599 2.4030 12.5\n",
            "2649 2.1300 37.5\n",
            "2699 2.4392 12.5\n",
            "2749 2.0838 31.25\n",
            "2799 2.4387 25.0\n",
            "2849 2.5586 12.5\n",
            "2899 2.3639 25.0\n",
            "2949 2.2490 6.25\n",
            "2999 2.4611 12.5\n",
            "3049 2.2515 12.5\n",
            "3099 2.5014 6.25\n",
            "3149 2.4112 18.75\n",
            "3199 2.6316 12.5\n",
            "3249 2.6307 6.25\n",
            "3299 2.0067 31.25\n",
            "3349 2.3323 18.75\n",
            "3399 2.6700 25.0\n",
            "3449 2.1047 25.0\n",
            "3499 2.4311 18.75\n",
            "3549 2.3513 18.75\n",
            "3599 2.1193 25.0\n",
            "3649 2.4068 12.5\n",
            "3699 2.7122 18.75\n",
            "3749 2.3273 18.75\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.1717, Accuracy: 1020/10000 (10.20%)\n",
            "\n",
            "--- 16.866233825683594 seconds ---\n",
            "Epoch: 8\n",
            "49 1.8598 31.25\n",
            "99 1.9861 12.5\n",
            "149 1.5518 37.5\n",
            "199 1.8828 43.75\n",
            "249 1.8603 31.25\n",
            "299 2.1974 6.25\n",
            "349 2.0455 31.25\n",
            "399 1.9034 37.5\n",
            "449 2.0744 25.0\n",
            "499 1.6366 56.25\n",
            "549 1.9088 31.25\n",
            "599 2.1710 12.5\n",
            "649 2.2394 18.75\n",
            "699 2.0931 18.75\n",
            "749 2.1562 25.0\n",
            "799 1.9829 37.5\n",
            "849 2.1016 12.5\n",
            "899 1.9931 50.0\n",
            "949 2.2923 31.25\n",
            "999 2.1203 18.75\n",
            "1049 1.9605 12.5\n",
            "1099 2.0036 18.75\n",
            "1149 1.8335 37.5\n",
            "1199 1.8239 43.75\n",
            "1249 2.0173 18.75\n",
            "1299 2.2182 25.0\n",
            "1349 2.0870 18.75\n",
            "1399 2.0128 31.25\n",
            "1449 2.2511 31.25\n",
            "1499 2.0808 12.5\n",
            "1549 2.3586 0.0\n",
            "1599 2.0689 43.75\n",
            "1649 1.9992 31.25\n",
            "1699 1.7459 50.0\n",
            "1749 2.2584 18.75\n",
            "1799 2.0743 25.0\n",
            "1849 2.1889 25.0\n",
            "1899 2.5270 6.25\n",
            "1949 1.7829 31.25\n",
            "1999 2.1794 12.5\n",
            "2049 2.3535 25.0\n",
            "2099 2.2006 43.75\n",
            "2149 1.9804 37.5\n",
            "2199 1.9958 31.25\n",
            "2249 2.0613 31.25\n",
            "2299 2.1013 31.25\n",
            "2349 2.2348 12.5\n",
            "2399 2.3389 18.75\n",
            "2449 2.3437 6.25\n",
            "2499 2.4619 25.0\n",
            "2549 2.1730 25.0\n",
            "2599 2.2273 18.75\n",
            "2649 2.2313 18.75\n",
            "2699 1.9272 31.25\n",
            "2749 2.1332 18.75\n",
            "2799 2.3665 25.0\n",
            "2849 2.1825 31.25\n",
            "2899 2.2962 25.0\n",
            "2949 2.2305 25.0\n",
            "2999 2.4102 18.75\n",
            "3049 2.0898 31.25\n",
            "3099 2.2633 31.25\n",
            "3149 2.4081 6.25\n",
            "3199 2.3224 18.75\n",
            "3249 2.1635 12.5\n",
            "3299 1.9841 37.5\n",
            "3349 2.5584 18.75\n",
            "3399 2.2066 18.75\n",
            "3449 1.8710 25.0\n",
            "3499 2.0110 18.75\n",
            "3549 2.4647 12.5\n",
            "3599 2.4741 18.75\n",
            "3649 2.0405 18.75\n",
            "3699 1.7998 37.5\n",
            "3749 2.0847 37.5\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.1709, Accuracy: 1017/10000 (10.17%)\n",
            "\n",
            "--- 16.028798818588257 seconds ---\n",
            "Epoch: 9\n",
            "49 1.5894 50.0\n",
            "99 1.7663 37.5\n",
            "149 1.7078 50.0\n",
            "199 1.9542 37.5\n",
            "249 1.8970 37.5\n",
            "299 1.8942 56.25\n",
            "349 2.1036 37.5\n",
            "399 1.9855 25.0\n",
            "449 1.8989 31.25\n",
            "499 1.7025 37.5\n",
            "549 2.2756 25.0\n",
            "599 1.9793 37.5\n",
            "649 2.1016 25.0\n",
            "699 1.8820 18.75\n",
            "749 1.8285 43.75\n",
            "799 1.7963 25.0\n",
            "849 2.5508 12.5\n",
            "899 1.9329 31.25\n",
            "949 2.0996 31.25\n",
            "999 1.9883 25.0\n",
            "1049 1.6375 50.0\n",
            "1099 2.2151 25.0\n",
            "1149 2.0144 43.75\n",
            "1199 1.9634 18.75\n",
            "1249 2.0217 31.25\n",
            "1299 1.9806 43.75\n",
            "1349 2.1808 25.0\n",
            "1399 2.2130 25.0\n",
            "1449 1.9804 43.75\n",
            "1499 1.9921 43.75\n",
            "1549 1.9449 43.75\n",
            "1599 2.3832 25.0\n",
            "1649 1.8142 43.75\n",
            "1699 2.0952 25.0\n",
            "1749 2.0102 25.0\n",
            "1799 2.3681 12.5\n",
            "1849 2.5745 12.5\n",
            "1899 2.3089 6.25\n",
            "1949 1.9291 25.0\n",
            "1999 2.2206 18.75\n",
            "2049 2.3298 18.75\n",
            "2099 2.1295 12.5\n",
            "2149 2.5994 18.75\n",
            "2199 1.8433 18.75\n",
            "2249 2.2167 12.5\n",
            "2299 2.4799 25.0\n",
            "2349 2.3533 18.75\n",
            "2399 2.0079 25.0\n",
            "2449 2.4500 18.75\n",
            "2499 2.1730 18.75\n",
            "2549 2.5643 18.75\n",
            "2599 1.8870 31.25\n",
            "2649 2.3860 18.75\n",
            "2699 2.6307 6.25\n",
            "2749 2.1171 18.75\n",
            "2799 2.1303 31.25\n",
            "2849 2.6269 6.25\n",
            "2899 2.3887 18.75\n",
            "2949 2.5348 18.75\n",
            "2999 2.2118 12.5\n",
            "3049 2.0326 12.5\n",
            "3099 2.9656 0.0\n",
            "3149 2.0423 31.25\n",
            "3199 2.4849 6.25\n",
            "3249 2.1866 25.0\n",
            "3299 2.3356 12.5\n",
            "3349 2.4710 6.25\n",
            "3399 2.5323 18.75\n",
            "3449 2.2792 31.25\n",
            "3499 2.3356 12.5\n",
            "3549 2.2158 18.75\n",
            "3599 1.9382 31.25\n",
            "3649 2.4975 6.25\n",
            "3699 2.0997 18.75\n",
            "3749 2.5011 12.5\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.1707, Accuracy: 1031/10000 (10.31%)\n",
            "\n",
            "--- 16.073591709136963 seconds ---\n",
            "Epoch: 10\n",
            "49 1.9108 31.25\n",
            "99 1.9179 31.25\n",
            "149 1.9959 31.25\n",
            "199 1.9245 43.75\n",
            "249 1.7819 43.75\n",
            "299 1.7293 37.5\n",
            "349 1.9728 37.5\n",
            "399 1.7644 37.5\n",
            "449 1.9658 25.0\n",
            "499 1.8046 43.75\n",
            "549 1.8477 31.25\n",
            "599 1.8661 31.25\n",
            "649 1.5982 50.0\n",
            "699 1.9400 25.0\n",
            "749 1.8660 31.25\n",
            "799 1.6648 50.0\n",
            "849 1.8161 25.0\n",
            "899 1.9763 25.0\n",
            "949 2.1742 25.0\n",
            "999 2.2253 31.25\n",
            "1049 1.9054 31.25\n",
            "1099 2.7732 6.25\n",
            "1149 1.8660 31.25\n",
            "1199 2.0033 31.25\n",
            "1249 2.3715 12.5\n",
            "1299 1.6867 43.75\n",
            "1349 1.9496 37.5\n",
            "1399 1.7615 37.5\n",
            "1449 1.5792 43.75\n",
            "1499 2.1954 31.25\n",
            "1549 2.0440 43.75\n",
            "1599 1.9332 37.5\n",
            "1649 1.9194 37.5\n",
            "1699 1.7614 31.25\n",
            "1749 2.3950 18.75\n",
            "1799 2.0292 37.5\n",
            "1849 2.0869 6.25\n",
            "1899 2.1935 31.25\n",
            "1949 2.0749 31.25\n",
            "1999 2.5566 25.0\n",
            "2049 2.5161 12.5\n",
            "2099 2.4776 12.5\n",
            "2149 2.4141 25.0\n",
            "2199 2.0477 25.0\n",
            "2249 2.3881 6.25\n",
            "2299 2.6023 12.5\n",
            "2349 2.4678 18.75\n",
            "2399 1.8618 37.5\n",
            "2449 2.1788 31.25\n",
            "2499 1.9865 31.25\n",
            "2549 2.1187 18.75\n",
            "2599 2.2657 25.0\n",
            "2649 2.5454 12.5\n",
            "2699 2.2791 31.25\n",
            "2749 2.3673 18.75\n",
            "2799 2.2459 18.75\n",
            "2849 2.2311 31.25\n",
            "2899 2.1317 37.5\n",
            "2949 2.1916 18.75\n",
            "2999 1.8459 31.25\n",
            "3049 2.5017 18.75\n",
            "3099 2.1721 12.5\n",
            "3149 2.1750 18.75\n",
            "3199 2.3788 18.75\n",
            "3249 2.4986 25.0\n",
            "3299 2.0248 25.0\n",
            "3349 2.4664 12.5\n",
            "3399 2.3195 18.75\n",
            "3449 2.5517 12.5\n",
            "3499 2.2250 18.75\n",
            "3549 2.3257 6.25\n",
            "3599 2.3852 12.5\n",
            "3649 2.4469 6.25\n",
            "3699 2.3610 12.5\n",
            "3749 2.2699 12.5\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.1711, Accuracy: 1003/10000 (10.03%)\n",
            "\n",
            "--- 16.10630202293396 seconds ---\n"
          ]
        }
      ],
      "source": [
        "#fminst epoch 10 linear-hdc\n",
        "class ArgumentParser:\n",
        "    def __init__(self):\n",
        "        self.args = {\n",
        "            'lr': 0.01,            # Learning rate for optimizing class representative\n",
        "            'gamma': 0.3,          # Kernel parameter for computing covariance\n",
        "            'epoch': 10,            # Epochs of training\n",
        "            'gorder': 16,           # Order of the cyclic group required for G-VSA (8 ou 16)\n",
        "            'dim': 10000,          # Dimension of hypervectors c le grand D\n",
        "            'seed': 43,            # Random seed for reproducing results\n",
        "            'resume': True,       # Resume from existing encoded hypervectors\n",
        "            'data_dir': './encoded_data', # Directory to save encoded data (hypervectors)\n",
        "            'dataset': 'fmnist',    # Dataset choice choices=['mnist', 'fmnist', 'cifar', 'isolet', 'ucihar']\n",
        "            'raw_data_dir': './dataset',   # Raw data directory\n",
        "            'model': 'linear-hdc'    # Model type choices=['rff-hdc', 'linear-hdc', 'rff-gvsa']\n",
        "        }\n",
        "        self.seed = self.args['seed']\n",
        "        self.lr = self.args['lr']\n",
        "        self.gamma = self.args['gamma']\n",
        "        self.epoch = self.args['epoch']\n",
        "        self.gorder = self.args['gorder']\n",
        "        self.dim = self.args['dim']\n",
        "        self.resume = self.args['resume']\n",
        "        self.data_dir = self.args['data_dir']\n",
        "        self.dataset = self.args['dataset']\n",
        "        self.raw_data_dir = self.args['raw_data_dir']\n",
        "        self.model = self.args['model']\n",
        "\n",
        "\n",
        "    def get_args(self):\n",
        "        return self.args\n",
        "args = ArgumentParser()\n",
        "torch.manual_seed(args.seed)\n",
        "np.random.seed(args.seed)\n",
        "if 'hdc' in args.model:\n",
        "    args.gorder = 2\n",
        "    print(\"Use binary HDC with random fourier features, ignoring gorder, set to 2.\")\n",
        "args.data_dir = f'{args.data_dir}/{args.dataset}_{args.model}_order{args.gorder}_gamma{args.gamma}_dim{args.dim}'\n",
        "try:\n",
        "    os.makedirs(args.data_dir)\n",
        "except FileExistsError:\n",
        "    print('Encoded data folder already exists')\n",
        "if not args.resume:\n",
        "    print('Encode the dataset into hypervectors and save')\n",
        "    encode_and_save(args)\n",
        "    print('Finish encoding and saving')\n",
        "print(f'Optimizing class representatives for {args.epoch} epochs')\n",
        "train(args)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RFF HDC"
      ],
      "metadata": {
        "id": "gS2d9NI8wtlN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ISOLET"
      ],
      "metadata": {
        "id": "Sa0nfCcLwtlN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96b8b936-485c-43bd-971e-3f3c166626be",
        "id": "T3lRCYJQwtlN"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Use binary HDC with random fourier features, ignoring gorder, set to 2.\n",
            "Encode the dataset into hypervectors and save\n",
            "# of channels of data 1\n",
            "# of training samples and test samples 6238 1559\n",
            "Encoding with random fourier features encoder.\n",
            "the threshold to discretize fourier features to group elements tensor([0.])\n",
            "Encoded pixels to hypervectors with size:  torch.Size([256, 10000])\n",
            "Encoding training data...\n",
            "Start encoding data\n",
            "Finish encoding data\n",
            "Encoding test data...\n",
            "Start encoding data\n",
            "Finish encoding data\n",
            "Finish encoding and saving\n",
            "Optimizing class representatives for 1 epochs\n",
            "Loading encoded training data...\n",
            "Loading encoded test data...\n",
            "Size of encoded training data torch.Size([6238, 10000]) and test data torch.Size([1559, 10000])\n",
            "Epoch: 1\n",
            "49 0.8162 87.5\n",
            "99 0.5102 87.5\n",
            "149 0.4082 87.5\n",
            "199 0.1630 93.75\n",
            "249 0.8207 75.0\n",
            "299 0.4206 81.25\n",
            "349 0.1552 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0205, Accuracy: 1411/1559 (90.51%)\n",
            "\n",
            "--- 3.6395585536956787 seconds ---\n"
          ]
        }
      ],
      "source": [
        "#isolet epoch 1 rff-hdc\n",
        "class ArgumentParser:\n",
        "    def __init__(self):\n",
        "        self.args = {\n",
        "            'lr': 0.01,            # Learning rate for optimizing class representative\n",
        "            'gamma': 0.3,          # Kernel parameter for computing covariance\n",
        "            'epoch': 1,            # Epochs of training\n",
        "            'gorder': 16,           # Order of the cyclic group required for G-VSA (8 ou 16)\n",
        "            'dim': 10000,          # Dimension of hypervectors c le grand D\n",
        "            'seed': 43,            # Random seed for reproducing results\n",
        "            'resume': False,       # Resume from existing encoded hypervectors\n",
        "            'data_dir': './encoded_data', # Directory to save encoded data (hypervectors)\n",
        "            'dataset': 'isolet',    # Dataset choice choices=['mnist', 'fmnist', 'cifar', 'isolet', 'ucihar']\n",
        "            'raw_data_dir': './dataset',   # Raw data directory\n",
        "            'model': 'rff-hdc'    # Model type choices=['rff-hdc', 'linear-hdc', 'rff-gvsa']\n",
        "        }\n",
        "        self.seed = self.args['seed']\n",
        "        self.lr = self.args['lr']\n",
        "        self.gamma = self.args['gamma']\n",
        "        self.epoch = self.args['epoch']\n",
        "        self.gorder = self.args['gorder']\n",
        "        self.dim = self.args['dim']\n",
        "        self.resume = self.args['resume']\n",
        "        self.data_dir = self.args['data_dir']\n",
        "        self.dataset = self.args['dataset']\n",
        "        self.raw_data_dir = self.args['raw_data_dir']\n",
        "        self.model = self.args['model']\n",
        "\n",
        "\n",
        "    def get_args(self):\n",
        "        return self.args\n",
        "args = ArgumentParser()\n",
        "torch.manual_seed(args.seed)\n",
        "np.random.seed(args.seed)\n",
        "if 'hdc' in args.model:\n",
        "    args.gorder = 2\n",
        "    print(\"Use binary HDC with random fourier features, ignoring gorder, set to 2.\")\n",
        "args.data_dir = f'{args.data_dir}/{args.dataset}_{args.model}_order{args.gorder}_gamma{args.gamma}_dim{args.dim}'\n",
        "try:\n",
        "    os.makedirs(args.data_dir)\n",
        "except FileExistsError:\n",
        "    print('Encoded data folder already exists')\n",
        "if not args.resume:\n",
        "    print('Encode the dataset into hypervectors and save')\n",
        "    encode_and_save(args)\n",
        "    print('Finish encoding and saving')\n",
        "print(f'Optimizing class representatives for {args.epoch} epochs')\n",
        "train(args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3efd0e4c-0ea1-4ac3-d27e-f058eeb95d71",
        "id": "8QPdZVYMwtlO"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Use binary HDC with random fourier features, ignoring gorder, set to 2.\n",
            "Encoded data folder already exists\n",
            "Optimizing class representatives for 10 epochs\n",
            "Loading encoded training data...\n",
            "Loading encoded test data...\n",
            "Size of encoded training data torch.Size([6238, 10000]) and test data torch.Size([1559, 10000])\n",
            "Epoch: 1\n",
            "49 0.6727 75.0\n",
            "99 0.5162 75.0\n",
            "149 0.2010 93.75\n",
            "199 0.8416 81.25\n",
            "249 0.5621 87.5\n",
            "299 0.6517 75.0\n",
            "349 0.2199 93.75\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0235, Accuracy: 1382/1559 (88.65%)\n",
            "\n",
            "--- 1.8294272422790527 seconds ---\n",
            "Epoch: 2\n",
            "49 0.0897 100.0\n",
            "99 0.0332 100.0\n",
            "149 0.0098 100.0\n",
            "199 0.0389 100.0\n",
            "249 0.0323 100.0\n",
            "299 0.0946 93.75\n",
            "349 0.0373 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0184, Accuracy: 1414/1559 (90.70%)\n",
            "\n",
            "--- 1.846909523010254 seconds ---\n",
            "Epoch: 3\n",
            "49 0.0072 100.0\n",
            "99 0.0043 100.0\n",
            "149 0.0050 100.0\n",
            "199 0.0144 100.0\n",
            "249 0.0076 100.0\n",
            "299 0.0064 100.0\n",
            "349 0.0131 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0177, Accuracy: 1418/1559 (90.96%)\n",
            "\n",
            "--- 1.8153924942016602 seconds ---\n",
            "Epoch: 4\n",
            "49 0.0013 100.0\n",
            "99 0.0059 100.0\n",
            "149 0.0045 100.0\n",
            "199 0.0037 100.0\n",
            "249 0.0046 100.0\n",
            "299 0.0033 100.0\n",
            "349 0.0029 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0173, Accuracy: 1419/1559 (91.02%)\n",
            "\n",
            "--- 1.820068597793579 seconds ---\n",
            "Epoch: 5\n",
            "49 0.0043 100.0\n",
            "99 0.0051 100.0\n",
            "149 0.0032 100.0\n",
            "199 0.0039 100.0\n",
            "249 0.0058 100.0\n",
            "299 0.0047 100.0\n",
            "349 0.0069 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0170, Accuracy: 1421/1559 (91.15%)\n",
            "\n",
            "--- 1.825897216796875 seconds ---\n",
            "Epoch: 6\n",
            "49 0.0018 100.0\n",
            "99 0.0031 100.0\n",
            "149 0.0032 100.0\n",
            "199 0.0030 100.0\n",
            "249 0.0022 100.0\n",
            "299 0.0041 100.0\n",
            "349 0.0028 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0166, Accuracy: 1426/1559 (91.47%)\n",
            "\n",
            "--- 2.315241575241089 seconds ---\n",
            "Epoch: 7\n",
            "49 0.0022 100.0\n",
            "99 0.0025 100.0\n",
            "149 0.0027 100.0\n",
            "199 0.0015 100.0\n",
            "249 0.0032 100.0\n",
            "299 0.0033 100.0\n",
            "349 0.0021 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0166, Accuracy: 1427/1559 (91.53%)\n",
            "\n",
            "--- 2.2071573734283447 seconds ---\n",
            "Epoch: 8\n",
            "49 0.0026 100.0\n",
            "99 0.0024 100.0\n",
            "149 0.0020 100.0\n",
            "199 0.0020 100.0\n",
            "249 0.0028 100.0\n",
            "299 0.0025 100.0\n",
            "349 0.0025 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0164, Accuracy: 1424/1559 (91.34%)\n",
            "\n",
            "--- 1.8362016677856445 seconds ---\n",
            "Epoch: 9\n",
            "49 0.0029 100.0\n",
            "99 0.0021 100.0\n",
            "149 0.0021 100.0\n",
            "199 0.0018 100.0\n",
            "249 0.0033 100.0\n",
            "299 0.0027 100.0\n",
            "349 0.0024 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0162, Accuracy: 1431/1559 (91.79%)\n",
            "\n",
            "--- 1.8994555473327637 seconds ---\n",
            "Epoch: 10\n",
            "49 0.0019 100.0\n",
            "99 0.0022 100.0\n",
            "149 0.0023 100.0\n",
            "199 0.0018 100.0\n",
            "249 0.0014 100.0\n",
            "299 0.0038 100.0\n",
            "349 0.0029 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0160, Accuracy: 1431/1559 (91.79%)\n",
            "\n",
            "--- 1.8221466541290283 seconds ---\n"
          ]
        }
      ],
      "source": [
        "#isolet epoch 10 rff-hdc\n",
        "class ArgumentParser:\n",
        "    def __init__(self):\n",
        "        self.args = {\n",
        "            'lr': 0.01,            # Learning rate for optimizing class representative\n",
        "            'gamma': 0.3,          # Kernel parameter for computing covariance\n",
        "            'epoch': 10,            # Epochs of training\n",
        "            'gorder': 16,           # Order of the cyclic group required for G-VSA (8 ou 16)\n",
        "            'dim': 10000,          # Dimension of hypervectors c le grand D\n",
        "            'seed': 43,            # Random seed for reproducing results\n",
        "            'resume': True,       # Resume from existing encoded hypervectors\n",
        "            'data_dir': './encoded_data', # Directory to save encoded data (hypervectors)\n",
        "            'dataset': 'isolet',    # Dataset choice choices=['mnist', 'fmnist', 'cifar', 'isolet', 'ucihar']\n",
        "            'raw_data_dir': './dataset',   # Raw data directory\n",
        "            'model': 'rff-hdc'    # Model type choices=['rff-hdc', 'linear-hdc', 'rff-gvsa']\n",
        "        }\n",
        "        self.seed = self.args['seed']\n",
        "        self.lr = self.args['lr']\n",
        "        self.gamma = self.args['gamma']\n",
        "        self.epoch = self.args['epoch']\n",
        "        self.gorder = self.args['gorder']\n",
        "        self.dim = self.args['dim']\n",
        "        self.resume = self.args['resume']\n",
        "        self.data_dir = self.args['data_dir']\n",
        "        self.dataset = self.args['dataset']\n",
        "        self.raw_data_dir = self.args['raw_data_dir']\n",
        "        self.model = self.args['model']\n",
        "\n",
        "\n",
        "    def get_args(self):\n",
        "        return self.args\n",
        "args = ArgumentParser()\n",
        "torch.manual_seed(args.seed)\n",
        "np.random.seed(args.seed)\n",
        "if 'hdc' in args.model:\n",
        "    args.gorder = 2\n",
        "    print(\"Use binary HDC with random fourier features, ignoring gorder, set to 2.\")\n",
        "args.data_dir = f'{args.data_dir}/{args.dataset}_{args.model}_order{args.gorder}_gamma{args.gamma}_dim{args.dim}'\n",
        "try:\n",
        "    os.makedirs(args.data_dir)\n",
        "except FileExistsError:\n",
        "    print('Encoded data folder already exists')\n",
        "if not args.resume:\n",
        "    print('Encode the dataset into hypervectors and save')\n",
        "    encode_and_save(args)\n",
        "    print('Finish encoding and saving')\n",
        "print(f'Optimizing class representatives for {args.epoch} epochs')\n",
        "train(args)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## UCIHAR"
      ],
      "metadata": {
        "id": "JTWKiEZ1wtlP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "582e4f2c-5746-4bad-e50c-10e96cae5040",
        "id": "36qVl3QRwtlP"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Use binary HDC with random fourier features, ignoring gorder, set to 2.\n",
            "Encode the dataset into hypervectors and save\n",
            "# of channels of data 1\n",
            "# of training samples and test samples 7352 2947\n",
            "Encoding with random fourier features encoder.\n",
            "the threshold to discretize fourier features to group elements tensor([0.])\n",
            "Encoded pixels to hypervectors with size:  torch.Size([256, 10000])\n",
            "Encoding training data...\n",
            "Start encoding data\n",
            "Finish encoding data\n",
            "Encoding test data...\n",
            "Start encoding data\n",
            "Finish encoding data\n",
            "Finish encoding and saving\n",
            "Optimizing class representatives for 1 epochs\n",
            "Loading encoded training data...\n",
            "Loading encoded test data...\n",
            "Size of encoded training data torch.Size([7352, 10000]) and test data torch.Size([2947, 10000])\n",
            "Epoch: 1\n",
            "49 0.9588 75.0\n",
            "99 0.0221 100.0\n",
            "149 0.5313 87.5\n",
            "199 0.0612 100.0\n",
            "249 0.1879 93.75\n",
            "299 0.0794 100.0\n",
            "349 0.4489 81.25\n",
            "399 0.2374 87.5\n",
            "449 1.0105 87.5\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0577, Accuracy: 2316/2947 (78.59%)\n",
            "\n",
            "--- 2.2752835750579834 seconds ---\n"
          ]
        }
      ],
      "source": [
        "#ucihar epoch 1 rff-hdc\n",
        "class ArgumentParser:\n",
        "    def __init__(self):\n",
        "        self.args = {\n",
        "            'lr': 0.01,            # Learning rate for optimizing class representative\n",
        "            'gamma': 0.3,          # Kernel parameter for computing covariance\n",
        "            'epoch': 1,            # Epochs of training\n",
        "            'gorder': 16,           # Order of the cyclic group required for G-VSA (8 ou 16)\n",
        "            'dim': 10000,          # Dimension of hypervectors c le grand D\n",
        "            'seed': 43,            # Random seed for reproducing results\n",
        "            'resume': False,       # Resume from existing encoded hypervectors\n",
        "            'data_dir': './encoded_data', # Directory to save encoded data (hypervectors)\n",
        "            'dataset': 'ucihar',    # Dataset choice choices=['mnist', 'fmnist', 'cifar', 'isolet', 'ucihar']\n",
        "            'raw_data_dir': './dataset',   # Raw data directory\n",
        "            'model': 'rff-hdc'    # Model type choices=['rff-hdc', 'linear-hdc', 'rff-gvsa']\n",
        "        }\n",
        "        self.seed = self.args['seed']\n",
        "        self.lr = self.args['lr']\n",
        "        self.gamma = self.args['gamma']\n",
        "        self.epoch = self.args['epoch']\n",
        "        self.gorder = self.args['gorder']\n",
        "        self.dim = self.args['dim']\n",
        "        self.resume = self.args['resume']\n",
        "        self.data_dir = self.args['data_dir']\n",
        "        self.dataset = self.args['dataset']\n",
        "        self.raw_data_dir = self.args['raw_data_dir']\n",
        "        self.model = self.args['model']\n",
        "\n",
        "\n",
        "    def get_args(self):\n",
        "        return self.args\n",
        "args = ArgumentParser()\n",
        "torch.manual_seed(args.seed)\n",
        "np.random.seed(args.seed)\n",
        "if 'hdc' in args.model:\n",
        "    args.gorder = 2\n",
        "    print(\"Use binary HDC with random fourier features, ignoring gorder, set to 2.\")\n",
        "args.data_dir = f'{args.data_dir}/{args.dataset}_{args.model}_order{args.gorder}_gamma{args.gamma}_dim{args.dim}'\n",
        "try:\n",
        "    os.makedirs(args.data_dir)\n",
        "except FileExistsError:\n",
        "    print('Encoded data folder already exists')\n",
        "if not args.resume:\n",
        "    print('Encode the dataset into hypervectors and save')\n",
        "    encode_and_save(args)\n",
        "    print('Finish encoding and saving')\n",
        "print(f'Optimizing class representatives for {args.epoch} epochs')\n",
        "train(args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0ecda2a-593a-44a7-8a98-1bab94e89596",
        "id": "LSAYJCZjwtlP"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Use binary HDC with random fourier features, ignoring gorder, set to 2.\n",
            "Encoded data folder already exists\n",
            "Optimizing class representatives for 10 epochs\n",
            "Loading encoded training data...\n",
            "Loading encoded test data...\n",
            "Size of encoded training data torch.Size([7352, 10000]) and test data torch.Size([2947, 10000])\n",
            "Epoch: 1\n",
            "49 0.8315 75.0\n",
            "99 0.0903 100.0\n",
            "149 0.2130 93.75\n",
            "199 0.3415 87.5\n",
            "249 0.2836 93.75\n",
            "299 0.0112 100.0\n",
            "349 0.1453 87.5\n",
            "399 0.5739 81.25\n",
            "449 0.0114 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0112, Accuracy: 2753/2947 (93.42%)\n",
            "\n",
            "--- 3.0124118328094482 seconds ---\n",
            "Epoch: 2\n",
            "49 0.0604 93.75\n",
            "99 0.0328 100.0\n",
            "149 0.0081 100.0\n",
            "199 0.0037 100.0\n",
            "249 0.0503 100.0\n",
            "299 0.0153 100.0\n",
            "349 0.6410 81.25\n",
            "399 0.0426 100.0\n",
            "449 0.0162 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0156, Accuracy: 2715/2947 (92.13%)\n",
            "\n",
            "--- 2.402364730834961 seconds ---\n",
            "Epoch: 3\n",
            "49 0.0040 100.0\n",
            "99 0.0029 100.0\n",
            "149 0.0114 100.0\n",
            "199 0.0145 100.0\n",
            "249 0.0102 100.0\n",
            "299 0.0085 100.0\n",
            "349 0.1522 93.75\n",
            "399 0.1830 93.75\n",
            "449 0.0580 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0138, Accuracy: 2712/2947 (92.03%)\n",
            "\n",
            "--- 2.2695820331573486 seconds ---\n",
            "Epoch: 4\n",
            "49 0.0084 100.0\n",
            "99 0.0066 100.0\n",
            "149 0.0056 100.0\n",
            "199 0.0448 100.0\n",
            "249 0.0078 100.0\n",
            "299 0.9851 81.25\n",
            "349 0.0382 100.0\n",
            "399 0.0285 100.0\n",
            "449 0.0491 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0104, Accuracy: 2781/2947 (94.37%)\n",
            "\n",
            "--- 2.2607176303863525 seconds ---\n",
            "Epoch: 5\n",
            "49 0.0024 100.0\n",
            "99 0.0024 100.0\n",
            "149 0.0078 100.0\n",
            "199 0.0072 100.0\n",
            "249 0.5870 81.25\n",
            "299 0.0062 100.0\n",
            "349 0.0154 100.0\n",
            "399 0.0160 100.0\n",
            "449 0.0550 93.75\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0089, Accuracy: 2793/2947 (94.77%)\n",
            "\n",
            "--- 2.291341781616211 seconds ---\n",
            "Epoch: 6\n",
            "49 0.0854 93.75\n",
            "99 0.0041 100.0\n",
            "149 0.0049 100.0\n",
            "199 0.0067 100.0\n",
            "249 0.0242 100.0\n",
            "299 0.0505 100.0\n",
            "349 0.0036 100.0\n",
            "399 0.0083 100.0\n",
            "449 0.0163 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0099, Accuracy: 2781/2947 (94.37%)\n",
            "\n",
            "--- 2.8397510051727295 seconds ---\n",
            "Epoch: 7\n",
            "49 0.0131 100.0\n",
            "99 0.0104 100.0\n",
            "149 0.0409 100.0\n",
            "199 0.0241 100.0\n",
            "249 0.0087 100.0\n",
            "299 0.0272 100.0\n",
            "349 0.0008 100.0\n",
            "399 0.0065 100.0\n",
            "449 0.0005 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0120, Accuracy: 2733/2947 (92.74%)\n",
            "\n",
            "--- 2.6422624588012695 seconds ---\n",
            "Epoch: 8\n",
            "49 0.0098 100.0\n",
            "99 0.0123 100.0\n",
            "149 0.1434 93.75\n",
            "199 0.1404 93.75\n",
            "249 0.0625 93.75\n",
            "299 0.0012 100.0\n",
            "349 0.0002 100.0\n",
            "399 0.2083 87.5\n",
            "449 0.0155 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0151, Accuracy: 2706/2947 (91.82%)\n",
            "\n",
            "--- 2.3149664402008057 seconds ---\n",
            "Epoch: 9\n",
            "49 0.0014 100.0\n",
            "99 0.1664 93.75\n",
            "149 0.0137 100.0\n",
            "199 0.0641 93.75\n",
            "249 0.0273 100.0\n",
            "299 0.0132 100.0\n",
            "349 0.0172 100.0\n",
            "399 0.0224 100.0\n",
            "449 0.0085 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0083, Accuracy: 2794/2947 (94.81%)\n",
            "\n",
            "--- 2.306988000869751 seconds ---\n",
            "Epoch: 10\n",
            "49 0.0003 100.0\n",
            "99 0.0067 100.0\n",
            "149 0.0026 100.0\n",
            "199 0.0122 100.0\n",
            "249 0.0052 100.0\n",
            "299 0.0152 100.0\n",
            "349 0.2742 93.75\n",
            "399 0.1696 93.75\n",
            "449 0.0180 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0075, Accuracy: 2819/2947 (95.66%)\n",
            "\n",
            "--- 2.2991442680358887 seconds ---\n"
          ]
        }
      ],
      "source": [
        "#ucihar epoch 10 rff-hdc\n",
        "class ArgumentParser:\n",
        "    def __init__(self):\n",
        "        self.args = {\n",
        "            'lr': 0.01,            # Learning rate for optimizing class representative\n",
        "            'gamma': 0.3,          # Kernel parameter for computing covariance\n",
        "            'epoch': 10,            # Epochs of training\n",
        "            'gorder': 16,           # Order of the cyclic group required for G-VSA (8 ou 16)\n",
        "            'dim': 10000,          # Dimension of hypervectors c le grand D\n",
        "            'seed': 43,            # Random seed for reproducing results\n",
        "            'resume': True,       # Resume from existing encoded hypervectors\n",
        "            'data_dir': './encoded_data', # Directory to save encoded data (hypervectors)\n",
        "            'dataset': 'ucihar',    # Dataset choice choices=['mnist', 'fmnist', 'cifar', 'isolet', 'ucihar']\n",
        "            'raw_data_dir': './dataset',   # Raw data directory\n",
        "            'model': 'rff-hdc'    # Model type choices=['rff-hdc', 'linear-hdc', 'rff-gvsa']\n",
        "        }\n",
        "        self.seed = self.args['seed']\n",
        "        self.lr = self.args['lr']\n",
        "        self.gamma = self.args['gamma']\n",
        "        self.epoch = self.args['epoch']\n",
        "        self.gorder = self.args['gorder']\n",
        "        self.dim = self.args['dim']\n",
        "        self.resume = self.args['resume']\n",
        "        self.data_dir = self.args['data_dir']\n",
        "        self.dataset = self.args['dataset']\n",
        "        self.raw_data_dir = self.args['raw_data_dir']\n",
        "        self.model = self.args['model']\n",
        "\n",
        "\n",
        "    def get_args(self):\n",
        "        return self.args\n",
        "args = ArgumentParser()\n",
        "torch.manual_seed(args.seed)\n",
        "np.random.seed(args.seed)\n",
        "if 'hdc' in args.model:\n",
        "    args.gorder = 2\n",
        "    print(\"Use binary HDC with random fourier features, ignoring gorder, set to 2.\")\n",
        "args.data_dir = f'{args.data_dir}/{args.dataset}_{args.model}_order{args.gorder}_gamma{args.gamma}_dim{args.dim}'\n",
        "try:\n",
        "    os.makedirs(args.data_dir)\n",
        "except FileExistsError:\n",
        "    print('Encoded data folder already exists')\n",
        "if not args.resume:\n",
        "    print('Encode the dataset into hypervectors and save')\n",
        "    encode_and_save(args)\n",
        "    print('Finish encoding and saving')\n",
        "print(f'Optimizing class representatives for {args.epoch} epochs')\n",
        "train(args)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MNIST"
      ],
      "metadata": {
        "id": "1VdYuU6VwtlQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "933f742f-6e97-4c64-e35a-c1123ebbf508",
        "id": "iIvdc0VdwtlQ"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Use binary HDC with random fourier features, ignoring gorder, set to 2.\n",
            "Encode the dataset into hypervectors and save\n",
            "# of channels of data 1\n",
            "# of training samples and test samples 60000 10000\n",
            "Encoding with random fourier features encoder.\n",
            "the threshold to discretize fourier features to group elements tensor([0.])\n",
            "Encoded pixels to hypervectors with size:  torch.Size([256, 10000])\n",
            "Encoding training data...\n",
            "Start encoding data\n",
            "12800 images encoded. Total time elapse = 22.82528042793274\n",
            "25600 images encoded. Total time elapse = 45.816142320632935\n",
            "38400 images encoded. Total time elapse = 68.91876697540283\n",
            "51200 images encoded. Total time elapse = 91.96357250213623\n",
            "Finish encoding data\n",
            "Encoding test data...\n",
            "Start encoding data\n",
            "Finish encoding data\n",
            "Finish encoding and saving\n",
            "Optimizing class representatives for 1 epochs\n",
            "Loading encoded training data...\n",
            "Loading encoded test data...\n",
            "Size of encoded training data torch.Size([60000, 10000]) and test data torch.Size([10000, 10000])\n",
            "Epoch: 1\n",
            "49 0.3666 81.25\n",
            "99 0.5773 87.5\n",
            "149 0.0840 100.0\n",
            "199 0.3551 93.75\n",
            "249 0.3082 87.5\n",
            "299 0.3969 93.75\n",
            "349 0.7337 81.25\n",
            "399 0.3391 93.75\n",
            "449 0.5854 75.0\n",
            "499 0.2105 100.0\n",
            "549 0.1878 100.0\n",
            "599 0.2767 87.5\n",
            "649 0.7070 68.75\n",
            "699 0.1280 100.0\n",
            "749 0.2293 93.75\n",
            "799 0.3072 93.75\n",
            "849 0.1985 93.75\n",
            "899 0.0821 100.0\n",
            "949 0.2769 93.75\n",
            "999 0.1300 100.0\n",
            "1049 0.2978 93.75\n",
            "1099 0.1240 93.75\n",
            "1149 0.2185 93.75\n",
            "1199 0.4334 87.5\n",
            "1249 0.1021 100.0\n",
            "1299 0.0840 100.0\n",
            "1349 0.1078 100.0\n",
            "1399 0.0904 100.0\n",
            "1449 0.2950 87.5\n",
            "1499 0.0636 100.0\n",
            "1549 0.1479 100.0\n",
            "1599 0.1830 93.75\n",
            "1649 0.1446 93.75\n",
            "1699 0.3655 87.5\n",
            "1749 0.1758 100.0\n",
            "1799 0.2099 87.5\n",
            "1849 0.3513 81.25\n",
            "1899 0.2661 100.0\n",
            "1949 0.5592 75.0\n",
            "1999 0.3337 81.25\n",
            "2049 0.5948 87.5\n",
            "2099 0.3834 87.5\n",
            "2149 0.0749 100.0\n",
            "2199 0.0896 100.0\n",
            "2249 0.2390 100.0\n",
            "2299 0.3308 87.5\n",
            "2349 0.0976 100.0\n",
            "2399 0.2871 87.5\n",
            "2449 0.2248 93.75\n",
            "2499 0.0341 100.0\n",
            "2549 0.1644 93.75\n",
            "2599 0.0355 100.0\n",
            "2649 0.1163 100.0\n",
            "2699 0.0433 100.0\n",
            "2749 0.2192 93.75\n",
            "2799 0.1250 100.0\n",
            "2849 0.3223 93.75\n",
            "2899 0.2438 93.75\n",
            "2949 0.0863 100.0\n",
            "2999 0.3360 87.5\n",
            "3049 0.1648 93.75\n",
            "3099 0.0886 93.75\n",
            "3149 0.0903 100.0\n",
            "3199 0.2783 93.75\n",
            "3249 0.2776 93.75\n",
            "3299 0.2567 93.75\n",
            "3349 0.4127 87.5\n",
            "3399 0.2197 93.75\n",
            "3449 0.2262 93.75\n",
            "3499 0.0772 93.75\n",
            "3549 0.2041 93.75\n",
            "3599 0.0629 100.0\n",
            "3649 0.1464 100.0\n",
            "3699 0.1386 100.0\n",
            "3749 0.0317 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0122, Accuracy: 9448/10000 (94.48%)\n",
            "\n",
            "--- 16.819302797317505 seconds ---\n"
          ]
        }
      ],
      "source": [
        "#mnist epoch 1 rff-hdc\n",
        "class ArgumentParser:\n",
        "    def __init__(self):\n",
        "        self.args = {\n",
        "            'lr': 0.01,            # Learning rate for optimizing class representative\n",
        "            'gamma': 0.3,          # Kernel parameter for computing covariance\n",
        "            'epoch': 1,            # Epochs of training\n",
        "            'gorder': 16,           # Order of the cyclic group required for G-VSA (8 ou 16)\n",
        "            'dim': 10000,          # Dimension of hypervectors c le grand D\n",
        "            'seed': 43,            # Random seed for reproducing results\n",
        "            'resume': False,       # Resume from existing encoded hypervectors\n",
        "            'data_dir': './encoded_data', # Directory to save encoded data (hypervectors)\n",
        "            'dataset': 'mnist',    # Dataset choice choices=['mnist', 'fmnist', 'cifar', 'isolet', 'ucihar']\n",
        "            'raw_data_dir': './dataset',   # Raw data directory\n",
        "            'model': 'rff-hdc'    # Model type choices=['rff-hdc', 'linear-hdc', 'rff-gvsa']\n",
        "        }\n",
        "        self.seed = self.args['seed']\n",
        "        self.lr = self.args['lr']\n",
        "        self.gamma = self.args['gamma']\n",
        "        self.epoch = self.args['epoch']\n",
        "        self.gorder = self.args['gorder']\n",
        "        self.dim = self.args['dim']\n",
        "        self.resume = self.args['resume']\n",
        "        self.data_dir = self.args['data_dir']\n",
        "        self.dataset = self.args['dataset']\n",
        "        self.raw_data_dir = self.args['raw_data_dir']\n",
        "        self.model = self.args['model']\n",
        "\n",
        "\n",
        "    def get_args(self):\n",
        "        return self.args\n",
        "args = ArgumentParser()\n",
        "torch.manual_seed(args.seed)\n",
        "np.random.seed(args.seed)\n",
        "if 'hdc' in args.model:\n",
        "    args.gorder = 2\n",
        "    print(\"Use binary HDC with random fourier features, ignoring gorder, set to 2.\")\n",
        "args.data_dir = f'{args.data_dir}/{args.dataset}_{args.model}_order{args.gorder}_gamma{args.gamma}_dim{args.dim}'\n",
        "try:\n",
        "    os.makedirs(args.data_dir)\n",
        "except FileExistsError:\n",
        "    print('Encoded data folder already exists')\n",
        "if not args.resume:\n",
        "    print('Encode the dataset into hypervectors and save')\n",
        "    encode_and_save(args)\n",
        "    print('Finish encoding and saving')\n",
        "print(f'Optimizing class representatives for {args.epoch} epochs')\n",
        "train(args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a09d2e5b-0799-4e6f-cc62-129afd5b6089",
        "id": "xrQKQKLuwtlQ"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Use binary HDC with random fourier features, ignoring gorder, set to 2.\n",
            "Encoded data folder already exists\n",
            "Optimizing class representatives for 10 epochs\n",
            "Loading encoded training data...\n",
            "Loading encoded test data...\n",
            "Size of encoded training data torch.Size([60000, 10000]) and test data torch.Size([10000, 10000])\n",
            "Epoch: 1\n",
            "49 0.9820 81.25\n",
            "99 0.3640 81.25\n",
            "149 0.1924 93.75\n",
            "199 0.1704 93.75\n",
            "249 0.2110 93.75\n",
            "299 0.1443 100.0\n",
            "349 0.5804 75.0\n",
            "399 0.1857 93.75\n",
            "449 0.2395 87.5\n",
            "499 0.5235 81.25\n",
            "549 0.2044 93.75\n",
            "599 0.5589 75.0\n",
            "649 0.1620 93.75\n",
            "699 0.3172 93.75\n",
            "749 0.1744 93.75\n",
            "799 0.1649 93.75\n",
            "849 0.4774 87.5\n",
            "899 0.3549 81.25\n",
            "949 0.1349 93.75\n",
            "999 0.1843 93.75\n",
            "1049 0.1660 87.5\n",
            "1099 0.3242 87.5\n",
            "1149 0.2789 87.5\n",
            "1199 0.1363 93.75\n",
            "1249 0.2294 87.5\n",
            "1299 0.2810 87.5\n",
            "1349 0.0472 100.0\n",
            "1399 0.2559 93.75\n",
            "1449 0.0752 100.0\n",
            "1499 0.2567 93.75\n",
            "1549 0.0384 100.0\n",
            "1599 0.0919 100.0\n",
            "1649 0.0995 100.0\n",
            "1699 0.1512 100.0\n",
            "1749 0.0860 100.0\n",
            "1799 0.1760 87.5\n",
            "1849 0.1624 87.5\n",
            "1899 0.2381 93.75\n",
            "1949 0.1418 93.75\n",
            "1999 0.0341 100.0\n",
            "2049 0.1292 100.0\n",
            "2099 0.3192 87.5\n",
            "2149 0.1463 93.75\n",
            "2199 0.1038 93.75\n",
            "2249 0.6441 87.5\n",
            "2299 0.4183 93.75\n",
            "2349 0.0639 100.0\n",
            "2399 0.0674 100.0\n",
            "2449 0.0510 100.0\n",
            "2499 0.1311 93.75\n",
            "2549 0.2363 87.5\n",
            "2599 0.2157 87.5\n",
            "2649 0.1457 100.0\n",
            "2699 0.0625 100.0\n",
            "2749 0.2618 93.75\n",
            "2799 0.1147 93.75\n",
            "2849 0.0994 93.75\n",
            "2899 0.0231 100.0\n",
            "2949 0.4257 87.5\n",
            "2999 0.3551 81.25\n",
            "3049 0.0367 100.0\n",
            "3099 0.1926 93.75\n",
            "3149 0.1676 100.0\n",
            "3199 0.3934 87.5\n",
            "3249 0.2300 93.75\n",
            "3299 0.1757 100.0\n",
            "3349 0.0648 100.0\n",
            "3399 0.2292 93.75\n",
            "3449 0.3302 87.5\n",
            "3499 0.2272 93.75\n",
            "3549 0.0502 100.0\n",
            "3599 0.0938 100.0\n",
            "3649 0.4805 87.5\n",
            "3699 0.2260 93.75\n",
            "3749 0.1406 93.75\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0121, Accuracy: 9463/10000 (94.63%)\n",
            "\n",
            "--- 16.999792337417603 seconds ---\n",
            "Epoch: 2\n",
            "49 0.1216 100.0\n",
            "99 0.1386 93.75\n",
            "149 0.0887 100.0\n",
            "199 0.1012 100.0\n",
            "249 0.1853 93.75\n",
            "299 0.0608 100.0\n",
            "349 0.0562 100.0\n",
            "399 0.0540 100.0\n",
            "449 0.0700 100.0\n",
            "499 0.0705 100.0\n",
            "549 0.1203 100.0\n",
            "599 0.2395 93.75\n",
            "649 0.0287 100.0\n",
            "699 0.0598 100.0\n",
            "749 0.1835 93.75\n",
            "799 0.0666 100.0\n",
            "849 0.2163 93.75\n",
            "899 0.0504 100.0\n",
            "949 0.0793 100.0\n",
            "999 0.0917 93.75\n",
            "1049 0.0473 100.0\n",
            "1099 0.1245 100.0\n",
            "1149 0.3028 87.5\n",
            "1199 0.1195 100.0\n",
            "1249 0.1375 100.0\n",
            "1299 0.0516 100.0\n",
            "1349 0.0948 93.75\n",
            "1399 0.1420 100.0\n",
            "1449 0.3174 87.5\n",
            "1499 0.0573 100.0\n",
            "1549 0.0478 100.0\n",
            "1599 0.0571 100.0\n",
            "1649 0.0620 100.0\n",
            "1699 0.1292 100.0\n",
            "1749 0.0103 100.0\n",
            "1799 0.0787 100.0\n",
            "1849 0.1462 100.0\n",
            "1899 0.0489 100.0\n",
            "1949 0.0668 100.0\n",
            "1999 0.0623 100.0\n",
            "2049 0.1661 93.75\n",
            "2099 0.1104 100.0\n",
            "2149 0.1416 100.0\n",
            "2199 0.0391 100.0\n",
            "2249 0.0972 100.0\n",
            "2299 0.1107 100.0\n",
            "2349 0.0761 100.0\n",
            "2399 0.2095 93.75\n",
            "2449 0.1113 93.75\n",
            "2499 0.2301 93.75\n",
            "2549 0.0354 100.0\n",
            "2599 0.2746 93.75\n",
            "2649 0.1742 93.75\n",
            "2699 0.1606 93.75\n",
            "2749 0.1328 93.75\n",
            "2799 0.1427 93.75\n",
            "2849 0.2757 93.75\n",
            "2899 0.1366 93.75\n",
            "2949 0.1371 93.75\n",
            "2999 0.0953 100.0\n",
            "3049 0.1797 93.75\n",
            "3099 0.2236 93.75\n",
            "3149 0.0937 100.0\n",
            "3199 0.3596 93.75\n",
            "3249 0.2993 87.5\n",
            "3299 0.1763 93.75\n",
            "3349 0.0145 100.0\n",
            "3399 0.2442 93.75\n",
            "3449 0.1342 93.75\n",
            "3499 0.0982 93.75\n",
            "3549 0.1174 100.0\n",
            "3599 0.0393 100.0\n",
            "3649 0.0163 100.0\n",
            "3699 0.0324 100.0\n",
            "3749 0.1221 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0116, Accuracy: 9483/10000 (94.83%)\n",
            "\n",
            "--- 16.638183116912842 seconds ---\n",
            "Epoch: 3\n",
            "49 0.1363 93.75\n",
            "99 0.1287 100.0\n",
            "149 0.0879 100.0\n",
            "199 0.0733 100.0\n",
            "249 0.0943 100.0\n",
            "299 0.0369 100.0\n",
            "349 0.1085 100.0\n",
            "399 0.0817 100.0\n",
            "449 0.0367 100.0\n",
            "499 0.1555 100.0\n",
            "549 0.0910 100.0\n",
            "599 0.0899 100.0\n",
            "649 0.0829 100.0\n",
            "699 0.2279 93.75\n",
            "749 0.0518 100.0\n",
            "799 0.0533 100.0\n",
            "849 0.1299 100.0\n",
            "899 0.0318 100.0\n",
            "949 0.0113 100.0\n",
            "999 0.1031 100.0\n",
            "1049 0.0999 100.0\n",
            "1099 0.1313 93.75\n",
            "1149 0.1026 100.0\n",
            "1199 0.0426 100.0\n",
            "1249 0.3266 93.75\n",
            "1299 0.1315 100.0\n",
            "1349 0.2217 87.5\n",
            "1399 0.0944 100.0\n",
            "1449 0.1964 93.75\n",
            "1499 0.0897 100.0\n",
            "1549 0.1773 93.75\n",
            "1599 0.0962 100.0\n",
            "1649 0.0995 100.0\n",
            "1699 0.0379 100.0\n",
            "1749 0.0416 100.0\n",
            "1799 0.1666 93.75\n",
            "1849 0.2517 93.75\n",
            "1899 0.0792 100.0\n",
            "1949 0.0617 100.0\n",
            "1999 0.1324 93.75\n",
            "2049 0.0399 100.0\n",
            "2099 0.1613 93.75\n",
            "2149 0.2054 93.75\n",
            "2199 0.1297 100.0\n",
            "2249 0.0140 100.0\n",
            "2299 0.0782 100.0\n",
            "2349 0.0830 100.0\n",
            "2399 0.0610 100.0\n",
            "2449 0.0301 100.0\n",
            "2499 0.0532 100.0\n",
            "2549 0.0486 100.0\n",
            "2599 0.2559 87.5\n",
            "2649 0.1227 100.0\n",
            "2699 0.0556 100.0\n",
            "2749 0.1485 93.75\n",
            "2799 0.0538 100.0\n",
            "2849 0.0814 100.0\n",
            "2899 0.0336 100.0\n",
            "2949 0.0489 100.0\n",
            "2999 0.4353 87.5\n",
            "3049 0.1109 93.75\n",
            "3099 0.1806 100.0\n",
            "3149 0.0675 100.0\n",
            "3199 0.1599 93.75\n",
            "3249 0.0674 100.0\n",
            "3299 0.2426 93.75\n",
            "3349 0.0615 100.0\n",
            "3399 0.2563 93.75\n",
            "3449 0.0237 100.0\n",
            "3499 0.0621 100.0\n",
            "3549 0.0842 100.0\n",
            "3599 0.0561 100.0\n",
            "3649 0.0568 100.0\n",
            "3699 0.0852 100.0\n",
            "3749 0.1657 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0118, Accuracy: 9492/10000 (94.92%)\n",
            "\n",
            "--- 16.367965936660767 seconds ---\n",
            "Epoch: 4\n",
            "49 0.2104 93.75\n",
            "99 0.0184 100.0\n",
            "149 0.0295 100.0\n",
            "199 0.0404 100.0\n",
            "249 0.0847 100.0\n",
            "299 0.0995 100.0\n",
            "349 0.0757 100.0\n",
            "399 0.0975 100.0\n",
            "449 0.0575 100.0\n",
            "499 0.0461 100.0\n",
            "549 0.0942 100.0\n",
            "599 0.1371 93.75\n",
            "649 0.0751 100.0\n",
            "699 0.0587 100.0\n",
            "749 0.1208 100.0\n",
            "799 0.0795 100.0\n",
            "849 0.0633 100.0\n",
            "899 0.0866 100.0\n",
            "949 0.1585 93.75\n",
            "999 0.0592 100.0\n",
            "1049 0.1447 93.75\n",
            "1099 0.0847 100.0\n",
            "1149 0.1095 100.0\n",
            "1199 0.1045 100.0\n",
            "1249 0.0820 100.0\n",
            "1299 0.0993 93.75\n",
            "1349 0.0919 100.0\n",
            "1399 0.0362 100.0\n",
            "1449 0.0510 100.0\n",
            "1499 0.0723 100.0\n",
            "1549 0.0420 100.0\n",
            "1599 0.0721 100.0\n",
            "1649 0.0319 100.0\n",
            "1699 0.0258 100.0\n",
            "1749 0.3120 93.75\n",
            "1799 0.0746 100.0\n",
            "1849 0.0359 100.0\n",
            "1899 0.1800 93.75\n",
            "1949 0.1788 93.75\n",
            "1999 0.0452 100.0\n",
            "2049 0.1064 100.0\n",
            "2099 0.1019 100.0\n",
            "2149 0.1350 100.0\n",
            "2199 0.1062 100.0\n",
            "2249 0.1376 93.75\n",
            "2299 0.0669 100.0\n",
            "2349 0.1788 93.75\n",
            "2399 0.1234 93.75\n",
            "2449 0.2411 93.75\n",
            "2499 0.2882 93.75\n",
            "2549 0.0991 100.0\n",
            "2599 0.1121 93.75\n",
            "2649 0.0425 100.0\n",
            "2699 0.1347 100.0\n",
            "2749 0.1263 93.75\n",
            "2799 0.4147 87.5\n",
            "2849 0.1032 100.0\n",
            "2899 0.0395 100.0\n",
            "2949 0.0773 93.75\n",
            "2999 0.0396 100.0\n",
            "3049 0.1007 100.0\n",
            "3099 0.3592 93.75\n",
            "3149 0.1147 100.0\n",
            "3199 0.0458 100.0\n",
            "3249 0.1399 100.0\n",
            "3299 0.1792 93.75\n",
            "3349 0.1437 93.75\n",
            "3399 0.0191 100.0\n",
            "3449 0.0497 100.0\n",
            "3499 0.1395 100.0\n",
            "3549 0.0929 93.75\n",
            "3599 0.0857 100.0\n",
            "3649 0.0788 100.0\n",
            "3699 0.1511 93.75\n",
            "3749 0.1330 93.75\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0116, Accuracy: 9498/10000 (94.98%)\n",
            "\n",
            "--- 16.240647554397583 seconds ---\n",
            "Epoch: 5\n",
            "49 0.0821 100.0\n",
            "99 0.0852 100.0\n",
            "149 0.0305 100.0\n",
            "199 0.0528 100.0\n",
            "249 0.0291 100.0\n",
            "299 0.0476 100.0\n",
            "349 0.1516 100.0\n",
            "399 0.1608 93.75\n",
            "449 0.0959 100.0\n",
            "499 0.0498 100.0\n",
            "549 0.0514 100.0\n",
            "599 0.0695 100.0\n",
            "649 0.0518 100.0\n",
            "699 0.2327 93.75\n",
            "749 0.1107 100.0\n",
            "799 0.0317 100.0\n",
            "849 0.0636 100.0\n",
            "899 0.2414 87.5\n",
            "949 0.0348 100.0\n",
            "999 0.1541 100.0\n",
            "1049 0.0467 100.0\n",
            "1099 0.0820 100.0\n",
            "1149 0.0363 100.0\n",
            "1199 0.0677 100.0\n",
            "1249 0.1318 93.75\n",
            "1299 0.1096 100.0\n",
            "1349 0.2785 93.75\n",
            "1399 0.0186 100.0\n",
            "1449 0.0763 100.0\n",
            "1499 0.0918 100.0\n",
            "1549 0.0539 100.0\n",
            "1599 0.1167 100.0\n",
            "1649 0.0775 100.0\n",
            "1699 0.0549 100.0\n",
            "1749 0.2758 87.5\n",
            "1799 0.0897 100.0\n",
            "1849 0.0617 100.0\n",
            "1899 0.0206 100.0\n",
            "1949 0.1032 100.0\n",
            "1999 0.2144 87.5\n",
            "2049 0.0742 100.0\n",
            "2099 0.1059 100.0\n",
            "2149 0.0726 100.0\n",
            "2199 0.0188 100.0\n",
            "2249 0.0423 100.0\n",
            "2299 0.0825 100.0\n",
            "2349 0.0905 93.75\n",
            "2399 0.0310 100.0\n",
            "2449 0.2290 87.5\n",
            "2499 0.0438 100.0\n",
            "2549 0.0771 100.0\n",
            "2599 0.1920 93.75\n",
            "2649 0.0501 100.0\n",
            "2699 0.1117 100.0\n",
            "2749 0.0529 100.0\n",
            "2799 0.0472 100.0\n",
            "2849 0.1476 93.75\n",
            "2899 0.1415 100.0\n",
            "2949 0.1037 100.0\n",
            "2999 0.0524 100.0\n",
            "3049 0.0609 100.0\n",
            "3099 0.0540 100.0\n",
            "3149 0.2580 87.5\n",
            "3199 0.0308 100.0\n",
            "3249 0.1541 100.0\n",
            "3299 0.0905 100.0\n",
            "3349 0.0979 100.0\n",
            "3399 0.1701 93.75\n",
            "3449 0.0119 100.0\n",
            "3499 0.0620 100.0\n",
            "3549 0.0875 100.0\n",
            "3599 0.0605 100.0\n",
            "3649 0.0924 100.0\n",
            "3699 0.0757 100.0\n",
            "3749 0.1413 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0116, Accuracy: 9483/10000 (94.83%)\n",
            "\n",
            "--- 16.923381328582764 seconds ---\n",
            "Epoch: 6\n",
            "49 0.1089 100.0\n",
            "99 0.1097 100.0\n",
            "149 0.0365 100.0\n",
            "199 0.0287 100.0\n",
            "249 0.0819 100.0\n",
            "299 0.0528 100.0\n",
            "349 0.2589 93.75\n",
            "399 0.0538 100.0\n",
            "449 0.0632 100.0\n",
            "499 0.1690 93.75\n",
            "549 0.0909 100.0\n",
            "599 0.0572 100.0\n",
            "649 0.1195 100.0\n",
            "699 0.1232 100.0\n",
            "749 0.0466 100.0\n",
            "799 0.0734 100.0\n",
            "849 0.0401 100.0\n",
            "899 0.0765 100.0\n",
            "949 0.0588 100.0\n",
            "999 0.0255 100.0\n",
            "1049 0.1177 100.0\n",
            "1099 0.0290 100.0\n",
            "1149 0.0671 100.0\n",
            "1199 0.0316 100.0\n",
            "1249 0.0555 100.0\n",
            "1299 0.1229 100.0\n",
            "1349 0.0588 100.0\n",
            "1399 0.1010 100.0\n",
            "1449 0.1237 100.0\n",
            "1499 0.0957 100.0\n",
            "1549 0.3025 93.75\n",
            "1599 0.0384 100.0\n",
            "1649 0.1155 100.0\n",
            "1699 0.1186 100.0\n",
            "1749 0.0882 100.0\n",
            "1799 0.0751 100.0\n",
            "1849 0.0700 100.0\n",
            "1899 0.2505 93.75\n",
            "1949 0.0236 100.0\n",
            "1999 0.0644 100.0\n",
            "2049 0.0574 100.0\n",
            "2099 0.0296 100.0\n",
            "2149 0.0520 100.0\n",
            "2199 0.0318 100.0\n",
            "2249 0.0449 100.0\n",
            "2299 0.1201 93.75\n",
            "2349 0.0351 100.0\n",
            "2399 0.2859 87.5\n",
            "2449 0.0798 100.0\n",
            "2499 0.1089 100.0\n",
            "2549 0.1366 93.75\n",
            "2599 0.0786 100.0\n",
            "2649 0.0750 100.0\n",
            "2699 0.1581 100.0\n",
            "2749 0.0637 100.0\n",
            "2799 0.0199 100.0\n",
            "2849 0.0535 100.0\n",
            "2899 0.2204 93.75\n",
            "2949 0.1852 93.75\n",
            "2999 0.0332 100.0\n",
            "3049 0.0514 100.0\n",
            "3099 0.1489 93.75\n",
            "3149 0.1202 93.75\n",
            "3199 0.0444 100.0\n",
            "3249 0.0970 100.0\n",
            "3299 0.0822 100.0\n",
            "3349 0.1088 100.0\n",
            "3399 0.0400 100.0\n",
            "3449 0.0512 100.0\n",
            "3499 0.0275 100.0\n",
            "3549 0.1198 100.0\n",
            "3599 0.2297 93.75\n",
            "3649 0.1977 93.75\n",
            "3699 0.0603 100.0\n",
            "3749 0.0579 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0115, Accuracy: 9495/10000 (94.95%)\n",
            "\n",
            "--- 16.149495124816895 seconds ---\n",
            "Epoch: 7\n",
            "49 0.0972 100.0\n",
            "99 0.0316 100.0\n",
            "149 0.0853 100.0\n",
            "199 0.0945 100.0\n",
            "249 0.0499 100.0\n",
            "299 0.1050 100.0\n",
            "349 0.0355 100.0\n",
            "399 0.0594 100.0\n",
            "449 0.0556 100.0\n",
            "499 0.0673 100.0\n",
            "549 0.1365 93.75\n",
            "599 0.0301 100.0\n",
            "649 0.0693 100.0\n",
            "699 0.0611 100.0\n",
            "749 0.0606 100.0\n",
            "799 0.0915 100.0\n",
            "849 0.0446 100.0\n",
            "899 0.0797 100.0\n",
            "949 0.1039 100.0\n",
            "999 0.1572 100.0\n",
            "1049 0.0641 100.0\n",
            "1099 0.0732 100.0\n",
            "1149 0.1029 100.0\n",
            "1199 0.1566 93.75\n",
            "1249 0.0689 100.0\n",
            "1299 0.0393 100.0\n",
            "1349 0.1338 93.75\n",
            "1399 0.0888 100.0\n",
            "1449 0.1690 93.75\n",
            "1499 0.0696 100.0\n",
            "1549 0.0967 100.0\n",
            "1599 0.0768 100.0\n",
            "1649 0.0145 100.0\n",
            "1699 0.0205 100.0\n",
            "1749 0.1053 100.0\n",
            "1799 0.0752 100.0\n",
            "1849 0.2252 93.75\n",
            "1899 0.1099 100.0\n",
            "1949 0.1669 93.75\n",
            "1999 0.0538 100.0\n",
            "2049 0.0931 100.0\n",
            "2099 0.0307 100.0\n",
            "2149 0.0860 100.0\n",
            "2199 0.0979 100.0\n",
            "2249 0.0818 100.0\n",
            "2299 0.0267 100.0\n",
            "2349 0.0550 100.0\n",
            "2399 0.0970 100.0\n",
            "2449 0.0774 93.75\n",
            "2499 0.0927 100.0\n",
            "2549 0.0555 100.0\n",
            "2599 0.0521 100.0\n",
            "2649 0.0451 100.0\n",
            "2699 0.0420 100.0\n",
            "2749 0.2867 87.5\n",
            "2799 0.1893 93.75\n",
            "2849 0.1741 93.75\n",
            "2899 0.0365 100.0\n",
            "2949 0.0642 100.0\n",
            "2999 0.0546 100.0\n",
            "3049 0.0510 100.0\n",
            "3099 0.0785 100.0\n",
            "3149 0.1517 100.0\n",
            "3199 0.0557 100.0\n",
            "3249 0.0716 100.0\n",
            "3299 0.0324 100.0\n",
            "3349 0.1115 100.0\n",
            "3399 0.0302 100.0\n",
            "3449 0.1113 93.75\n",
            "3499 0.0589 100.0\n",
            "3549 0.0393 100.0\n",
            "3599 0.0808 100.0\n",
            "3649 0.1113 100.0\n",
            "3699 0.1120 100.0\n",
            "3749 0.0473 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0116, Accuracy: 9500/10000 (95.00%)\n",
            "\n",
            "--- 16.01712918281555 seconds ---\n",
            "Epoch: 8\n",
            "49 0.1269 93.75\n",
            "99 0.0659 100.0\n",
            "149 0.0414 100.0\n",
            "199 0.0782 100.0\n",
            "249 0.0495 100.0\n",
            "299 0.0409 100.0\n",
            "349 0.0542 100.0\n",
            "399 0.1120 100.0\n",
            "449 0.0497 100.0\n",
            "499 0.0774 100.0\n",
            "549 0.0454 100.0\n",
            "599 0.0821 100.0\n",
            "649 0.0465 100.0\n",
            "699 0.0889 100.0\n",
            "749 0.0482 100.0\n",
            "799 0.1382 100.0\n",
            "849 0.0348 100.0\n",
            "899 0.0949 100.0\n",
            "949 0.0334 100.0\n",
            "999 0.1245 100.0\n",
            "1049 0.1416 100.0\n",
            "1099 0.0716 100.0\n",
            "1149 0.0235 100.0\n",
            "1199 0.0757 100.0\n",
            "1249 0.2981 93.75\n",
            "1299 0.0503 100.0\n",
            "1349 0.0709 100.0\n",
            "1399 0.0764 100.0\n",
            "1449 0.1324 93.75\n",
            "1499 0.0105 100.0\n",
            "1549 0.0516 100.0\n",
            "1599 0.0423 100.0\n",
            "1649 0.0313 100.0\n",
            "1699 0.0407 100.0\n",
            "1749 0.0232 100.0\n",
            "1799 0.2017 100.0\n",
            "1849 0.1521 93.75\n",
            "1899 0.1474 100.0\n",
            "1949 0.0813 100.0\n",
            "1999 0.0320 100.0\n",
            "2049 0.0298 100.0\n",
            "2099 0.0560 100.0\n",
            "2149 0.1429 93.75\n",
            "2199 0.0371 100.0\n",
            "2249 0.1462 93.75\n",
            "2299 0.1471 100.0\n",
            "2349 0.0566 100.0\n",
            "2399 0.0696 100.0\n",
            "2449 0.1671 93.75\n",
            "2499 0.1842 93.75\n",
            "2549 0.1213 100.0\n",
            "2599 0.0659 100.0\n",
            "2649 0.0809 100.0\n",
            "2699 0.1508 100.0\n",
            "2749 0.1141 100.0\n",
            "2799 0.0341 100.0\n",
            "2849 0.0695 100.0\n",
            "2899 0.0979 100.0\n",
            "2949 0.1275 100.0\n",
            "2999 0.2001 93.75\n",
            "3049 0.0862 100.0\n",
            "3099 0.0971 100.0\n",
            "3149 0.1019 100.0\n",
            "3199 0.0807 100.0\n",
            "3249 0.0492 100.0\n",
            "3299 0.0393 100.0\n",
            "3349 0.0483 100.0\n",
            "3399 0.1212 100.0\n",
            "3449 0.1414 93.75\n",
            "3499 0.0893 100.0\n",
            "3549 0.0598 100.0\n",
            "3599 0.0310 100.0\n",
            "3649 0.2909 93.75\n",
            "3699 0.0624 100.0\n",
            "3749 0.0986 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0116, Accuracy: 9489/10000 (94.89%)\n",
            "\n",
            "--- 16.84584617614746 seconds ---\n",
            "Epoch: 9\n",
            "49 0.0385 100.0\n",
            "99 0.0312 100.0\n",
            "149 0.0711 100.0\n",
            "199 0.0448 100.0\n",
            "249 0.0858 100.0\n",
            "299 0.0583 100.0\n",
            "349 0.0154 100.0\n",
            "399 0.1232 100.0\n",
            "449 0.0747 100.0\n",
            "499 0.0950 100.0\n",
            "549 0.0545 100.0\n",
            "599 0.1156 100.0\n",
            "649 0.1229 100.0\n",
            "699 0.0285 100.0\n",
            "749 0.1198 100.0\n",
            "799 0.0581 100.0\n",
            "849 0.1955 93.75\n",
            "899 0.0494 100.0\n",
            "949 0.0698 100.0\n",
            "999 0.1303 100.0\n",
            "1049 0.0588 100.0\n",
            "1099 0.0588 100.0\n",
            "1149 0.0344 100.0\n",
            "1199 0.0948 100.0\n",
            "1249 0.0967 100.0\n",
            "1299 0.0555 100.0\n",
            "1349 0.0512 100.0\n",
            "1399 0.0727 100.0\n",
            "1449 0.0673 100.0\n",
            "1499 0.0553 100.0\n",
            "1549 0.0455 100.0\n",
            "1599 0.1250 100.0\n",
            "1649 0.0970 100.0\n",
            "1699 0.0792 100.0\n",
            "1749 0.1693 100.0\n",
            "1799 0.0609 100.0\n",
            "1849 0.0655 100.0\n",
            "1899 0.1107 100.0\n",
            "1949 0.0637 100.0\n",
            "1999 0.0425 100.0\n",
            "2049 0.0442 100.0\n",
            "2099 0.0900 100.0\n",
            "2149 0.0465 100.0\n",
            "2199 0.0479 100.0\n",
            "2249 0.0691 100.0\n",
            "2299 0.0896 100.0\n",
            "2349 0.0729 100.0\n",
            "2399 0.0424 100.0\n",
            "2449 0.0436 100.0\n",
            "2499 0.0921 100.0\n",
            "2549 0.1422 100.0\n",
            "2599 0.0650 100.0\n",
            "2649 0.0858 100.0\n",
            "2699 0.0819 100.0\n",
            "2749 0.1252 100.0\n",
            "2799 0.1160 100.0\n",
            "2849 0.1353 100.0\n",
            "2899 0.0515 100.0\n",
            "2949 0.0711 100.0\n",
            "2999 0.1487 100.0\n",
            "3049 0.1168 100.0\n",
            "3099 0.1300 93.75\n",
            "3149 0.0898 100.0\n",
            "3199 0.2037 93.75\n",
            "3249 0.1016 100.0\n",
            "3299 0.1341 100.0\n",
            "3349 0.1729 93.75\n",
            "3399 0.1894 93.75\n",
            "3449 0.1037 100.0\n",
            "3499 0.0453 100.0\n",
            "3549 0.3131 87.5\n",
            "3599 0.2395 93.75\n",
            "3649 0.1914 93.75\n",
            "3699 0.0945 100.0\n",
            "3749 0.0263 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0116, Accuracy: 9487/10000 (94.87%)\n",
            "\n",
            "--- 16.05297064781189 seconds ---\n",
            "Epoch: 10\n",
            "49 0.0703 100.0\n",
            "99 0.0459 100.0\n",
            "149 0.0645 100.0\n",
            "199 0.0384 100.0\n",
            "249 0.1073 100.0\n",
            "299 0.0937 100.0\n",
            "349 0.1175 100.0\n",
            "399 0.0285 100.0\n",
            "449 0.0402 100.0\n",
            "499 0.1131 100.0\n",
            "549 0.0587 100.0\n",
            "599 0.1142 93.75\n",
            "649 0.0648 100.0\n",
            "699 0.0247 100.0\n",
            "749 0.0464 100.0\n",
            "799 0.0829 100.0\n",
            "849 0.2518 93.75\n",
            "899 0.0155 100.0\n",
            "949 0.0796 100.0\n",
            "999 0.0670 100.0\n",
            "1049 0.0486 100.0\n",
            "1099 0.0821 100.0\n",
            "1149 0.1278 100.0\n",
            "1199 0.0577 100.0\n",
            "1249 0.2091 93.75\n",
            "1299 0.0728 100.0\n",
            "1349 0.0528 100.0\n",
            "1399 0.0666 100.0\n",
            "1449 0.0838 100.0\n",
            "1499 0.0622 100.0\n",
            "1549 0.1692 93.75\n",
            "1599 0.1145 100.0\n",
            "1649 0.0579 100.0\n",
            "1699 0.1395 100.0\n",
            "1749 0.1009 100.0\n",
            "1799 0.0867 100.0\n",
            "1849 0.0965 100.0\n",
            "1899 0.1687 93.75\n",
            "1949 0.0969 93.75\n",
            "1999 0.0109 100.0\n",
            "2049 0.0616 100.0\n",
            "2099 0.0269 100.0\n",
            "2149 0.0430 100.0\n",
            "2199 0.0507 100.0\n",
            "2249 0.0307 100.0\n",
            "2299 0.0617 100.0\n",
            "2349 0.0414 100.0\n",
            "2399 0.0340 100.0\n",
            "2449 0.0290 100.0\n",
            "2499 0.0391 100.0\n",
            "2549 0.0912 100.0\n",
            "2599 0.0738 100.0\n",
            "2649 0.0559 100.0\n",
            "2699 0.0940 100.0\n",
            "2749 0.0279 100.0\n",
            "2799 0.0758 100.0\n",
            "2849 0.1961 93.75\n",
            "2899 0.0530 100.0\n",
            "2949 0.0225 100.0\n",
            "2999 0.1160 100.0\n",
            "3049 0.1071 93.75\n",
            "3099 0.1545 93.75\n",
            "3149 0.0380 100.0\n",
            "3199 0.0507 100.0\n",
            "3249 0.0833 100.0\n",
            "3299 0.0904 100.0\n",
            "3349 0.0920 100.0\n",
            "3399 0.1868 100.0\n",
            "3449 0.1144 100.0\n",
            "3499 0.0647 100.0\n",
            "3549 0.0739 100.0\n",
            "3599 0.0342 100.0\n",
            "3649 0.0500 100.0\n",
            "3699 0.0730 100.0\n",
            "3749 0.2350 93.75\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0117, Accuracy: 9476/10000 (94.76%)\n",
            "\n",
            "--- 15.875035285949707 seconds ---\n"
          ]
        }
      ],
      "source": [
        "#mnist epoch 10 rff-hdc\n",
        "class ArgumentParser:\n",
        "    def __init__(self):\n",
        "        self.args = {\n",
        "            'lr': 0.01,            # Learning rate for optimizing class representative\n",
        "            'gamma': 0.3,          # Kernel parameter for computing covariance\n",
        "            'epoch': 10,            # Epochs of training\n",
        "            'gorder': 16,           # Order of the cyclic group required for G-VSA (8 ou 16)\n",
        "            'dim': 10000,          # Dimension of hypervectors c le grand D\n",
        "            'seed': 43,            # Random seed for reproducing results\n",
        "            'resume': True,       # Resume from existing encoded hypervectors\n",
        "            'data_dir': './encoded_data', # Directory to save encoded data (hypervectors)\n",
        "            'dataset': 'mnist',    # Dataset choice choices=['mnist', 'fmnist', 'cifar', 'isolet', 'ucihar']\n",
        "            'raw_data_dir': './dataset',   # Raw data directory\n",
        "            'model': 'rff-hdc'    # Model type choices=['rff-hdc', 'linear-hdc', 'rff-gvsa']\n",
        "        }\n",
        "        self.seed = self.args['seed']\n",
        "        self.lr = self.args['lr']\n",
        "        self.gamma = self.args['gamma']\n",
        "        self.epoch = self.args['epoch']\n",
        "        self.gorder = self.args['gorder']\n",
        "        self.dim = self.args['dim']\n",
        "        self.resume = self.args['resume']\n",
        "        self.data_dir = self.args['data_dir']\n",
        "        self.dataset = self.args['dataset']\n",
        "        self.raw_data_dir = self.args['raw_data_dir']\n",
        "        self.model = self.args['model']\n",
        "\n",
        "\n",
        "    def get_args(self):\n",
        "        return self.args\n",
        "args = ArgumentParser()\n",
        "torch.manual_seed(args.seed)\n",
        "np.random.seed(args.seed)\n",
        "if 'hdc' in args.model:\n",
        "    args.gorder = 2\n",
        "    print(\"Use binary HDC with random fourier features, ignoring gorder, set to 2.\")\n",
        "args.data_dir = f'{args.data_dir}/{args.dataset}_{args.model}_order{args.gorder}_gamma{args.gamma}_dim{args.dim}'\n",
        "try:\n",
        "    os.makedirs(args.data_dir)\n",
        "except FileExistsError:\n",
        "    print('Encoded data folder already exists')\n",
        "if not args.resume:\n",
        "    print('Encode the dataset into hypervectors and save')\n",
        "    encode_and_save(args)\n",
        "    print('Finish encoding and saving')\n",
        "print(f'Optimizing class representatives for {args.epoch} epochs')\n",
        "train(args)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FMNIST"
      ],
      "metadata": {
        "id": "zuJqncGzwtlR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#fminst epoch 1 rff-hdc\n",
        "class ArgumentParser:\n",
        "    def __init__(self):\n",
        "        self.args = {\n",
        "            'lr': 0.01,            # Learning rate for optimizing class representative\n",
        "            'gamma': 0.3,          # Kernel parameter for computing covariance\n",
        "            'epoch': 1,            # Epochs of training\n",
        "            'gorder': 16,           # Order of the cyclic group required for G-VSA (8 ou 16)\n",
        "            'dim': 10000,          # Dimension of hypervectors c le grand D\n",
        "            'seed': 43,            # Random seed for reproducing results\n",
        "            'resume': False,       # Resume from existing encoded hypervectors\n",
        "            'data_dir': './encoded_data', # Directory to save encoded data (hypervectors)\n",
        "            'dataset': 'fmnist',    # Dataset choice choices=['mnist', 'fmnist', 'cifar', 'isolet', 'ucihar']\n",
        "            'raw_data_dir': './dataset',   # Raw data directory\n",
        "            'model': 'rff-hdc'    # Model type choices=['rff-hdc', 'linear-hdc', 'rff-gvsa']\n",
        "        }\n",
        "        self.seed = self.args['seed']\n",
        "        self.lr = self.args['lr']\n",
        "        self.gamma = self.args['gamma']\n",
        "        self.epoch = self.args['epoch']\n",
        "        self.gorder = self.args['gorder']\n",
        "        self.dim = self.args['dim']\n",
        "        self.resume = self.args['resume']\n",
        "        self.data_dir = self.args['data_dir']\n",
        "        self.dataset = self.args['dataset']\n",
        "        self.raw_data_dir = self.args['raw_data_dir']\n",
        "        self.model = self.args['model']\n",
        "\n",
        "\n",
        "    def get_args(self):\n",
        "        return self.args\n",
        "\n",
        "args = ArgumentParser()\n",
        "torch.manual_seed(args.seed)\n",
        "np.random.seed(args.seed)\n",
        "if 'hdc' in args.model:\n",
        "    args.gorder = 2\n",
        "    print(\"Use binary HDC with random fourier features, ignoring gorder, set to 2.\")\n",
        "args.data_dir = f'{args.data_dir}/{args.dataset}_{args.model}_order{args.gorder}_gamma{args.gamma}_dim{args.dim}'\n",
        "try:\n",
        "    os.makedirs(args.data_dir)\n",
        "except FileExistsError:\n",
        "    print('Encoded data folder already exists')\n",
        "if not args.resume:\n",
        "    print('Encode the dataset into hypervectors and save')\n",
        "    encode_and_save(args)\n",
        "    print('Finish encoding and saving')\n",
        "print(f'Optimizing class representatives for {args.epoch} epochs')\n",
        "train(args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Ba7UWSPwtlR",
        "outputId": "d0eb5f64-cd58-45b1-9b81-6633d103321a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Use binary HDC with random fourier features, ignoring gorder, set to 2.\n",
            "Encode the dataset into hypervectors and save\n",
            "# of channels of data 1\n",
            "# of training samples and test samples 60000 10000\n",
            "Encoding with random fourier features encoder.\n",
            "the threshold to discretize fourier features to group elements tensor([0.])\n",
            "Encoded pixels to hypervectors with size:  torch.Size([256, 10000])\n",
            "Encoding training data...\n",
            "Start encoding data\n",
            "12800 images encoded. Total time elapse = 26.958014726638794\n",
            "25600 images encoded. Total time elapse = 54.09557890892029\n",
            "38400 images encoded. Total time elapse = 81.32328343391418\n",
            "51200 images encoded. Total time elapse = 108.40847945213318\n",
            "Finish encoding data\n",
            "Encoding test data...\n",
            "Start encoding data\n",
            "Finish encoding data\n",
            "Finish encoding and saving\n",
            "Optimizing class representatives for 1 epochs\n",
            "Loading encoded training data...\n",
            "Loading encoded test data...\n",
            "Size of encoded training data torch.Size([60000, 10000]) and test data torch.Size([10000, 10000])\n",
            "Epoch: 1\n",
            "49 0.5865 87.5\n",
            "99 1.2018 50.0\n",
            "149 1.2880 56.25\n",
            "199 0.3209 93.75\n",
            "249 0.3651 87.5\n",
            "299 0.6315 68.75\n",
            "349 1.4801 62.5\n",
            "399 0.6322 81.25\n",
            "449 0.2696 93.75\n",
            "499 0.9271 68.75\n",
            "549 0.3970 81.25\n",
            "599 0.3114 87.5\n",
            "649 0.5177 87.5\n",
            "699 0.3023 87.5\n",
            "749 0.1659 93.75\n",
            "799 0.5546 81.25\n",
            "849 0.2436 93.75\n",
            "899 0.3399 81.25\n",
            "949 0.7065 81.25\n",
            "999 0.9514 75.0\n",
            "1049 0.5060 87.5\n",
            "1099 0.4247 81.25\n",
            "1149 0.7287 75.0\n",
            "1199 0.7585 87.5\n",
            "1249 0.4307 87.5\n",
            "1299 0.4463 87.5\n",
            "1349 0.1470 93.75\n",
            "1399 0.6096 81.25\n",
            "1449 0.9092 68.75\n",
            "1499 0.1574 93.75\n",
            "1549 0.9057 68.75\n",
            "1599 0.4024 81.25\n",
            "1649 0.4688 75.0\n",
            "1699 0.1700 93.75\n",
            "1749 0.7489 68.75\n",
            "1799 0.4163 93.75\n",
            "1849 0.2869 93.75\n",
            "1899 0.4866 87.5\n",
            "1949 0.1386 100.0\n",
            "1999 0.5693 75.0\n",
            "2049 0.4266 75.0\n",
            "2099 0.8976 68.75\n",
            "2149 0.4086 81.25\n",
            "2199 0.6339 81.25\n",
            "2249 0.2437 93.75\n",
            "2299 0.5836 75.0\n",
            "2349 0.4944 87.5\n",
            "2399 0.4073 93.75\n",
            "2449 0.5937 87.5\n",
            "2499 0.4353 81.25\n",
            "2549 0.7116 68.75\n",
            "2599 0.5126 87.5\n",
            "2649 0.5439 87.5\n",
            "2699 0.6618 75.0\n",
            "2749 0.5889 81.25\n",
            "2799 0.2442 93.75\n",
            "2849 0.4909 87.5\n",
            "2899 0.8053 75.0\n",
            "2949 0.3529 81.25\n",
            "2999 0.4550 81.25\n",
            "3049 0.8331 62.5\n",
            "3099 0.4619 81.25\n",
            "3149 0.3856 81.25\n",
            "3199 0.1008 100.0\n",
            "3249 0.4037 87.5\n",
            "3299 0.5461 68.75\n",
            "3349 0.7189 62.5\n",
            "3399 0.8988 75.0\n",
            "3449 0.0864 100.0\n",
            "3499 0.6755 75.0\n",
            "3549 0.6128 81.25\n",
            "3599 0.2320 87.5\n",
            "3649 0.5551 75.0\n",
            "3699 0.6445 81.25\n",
            "3749 0.2225 93.75\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0326, Accuracy: 8195/10000 (81.95%)\n",
            "\n",
            "--- 16.81502676010132 seconds ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53e8ab8c-22fb-4354-8389-a258b284fb9b",
        "id": "ON8p0A12wtlS"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Use binary HDC with random fourier features, ignoring gorder, set to 2.\n",
            "Encoded data folder already exists\n",
            "Optimizing class representatives for 10 epochs\n",
            "Loading encoded training data...\n",
            "Loading encoded test data...\n",
            "Size of encoded training data torch.Size([60000, 10000]) and test data torch.Size([10000, 10000])\n",
            "Epoch: 1\n",
            "49 0.9590 68.75\n",
            "99 0.5808 81.25\n",
            "149 0.5005 87.5\n",
            "199 0.5304 87.5\n",
            "249 0.4107 87.5\n",
            "299 0.6679 75.0\n",
            "349 0.1480 100.0\n",
            "399 0.5236 81.25\n",
            "449 0.7350 62.5\n",
            "499 0.3320 87.5\n",
            "549 0.2477 93.75\n",
            "599 0.4652 81.25\n",
            "649 0.2684 93.75\n",
            "699 0.5048 81.25\n",
            "749 0.8694 75.0\n",
            "799 0.3789 81.25\n",
            "849 0.2121 100.0\n",
            "899 0.6613 75.0\n",
            "949 0.2467 93.75\n",
            "999 0.3966 87.5\n",
            "1049 1.0813 68.75\n",
            "1099 0.8678 68.75\n",
            "1149 0.1634 100.0\n",
            "1199 0.3428 87.5\n",
            "1249 0.2436 87.5\n",
            "1299 0.1887 100.0\n",
            "1349 0.1117 100.0\n",
            "1399 0.5783 75.0\n",
            "1449 0.3533 81.25\n",
            "1499 0.3452 87.5\n",
            "1549 0.7766 81.25\n",
            "1599 0.6449 68.75\n",
            "1649 0.5125 81.25\n",
            "1699 0.3682 87.5\n",
            "1749 0.7950 75.0\n",
            "1799 0.5008 81.25\n",
            "1849 0.2837 87.5\n",
            "1899 0.6861 75.0\n",
            "1949 0.4638 93.75\n",
            "1999 0.5054 87.5\n",
            "2049 0.2543 100.0\n",
            "2099 0.5258 87.5\n",
            "2149 0.1490 93.75\n",
            "2199 0.4147 87.5\n",
            "2249 0.4834 81.25\n",
            "2299 0.5280 87.5\n",
            "2349 0.4581 81.25\n",
            "2399 0.2526 87.5\n",
            "2449 0.2563 87.5\n",
            "2499 0.7410 68.75\n",
            "2549 0.3855 87.5\n",
            "2599 0.5284 81.25\n",
            "2649 0.1342 100.0\n",
            "2699 0.2662 81.25\n",
            "2749 0.2434 93.75\n",
            "2799 0.3206 87.5\n",
            "2849 0.5098 87.5\n",
            "2899 0.4190 81.25\n",
            "2949 0.9785 81.25\n",
            "2999 0.8276 75.0\n",
            "3049 0.7632 81.25\n",
            "3099 0.2562 93.75\n",
            "3149 0.7547 68.75\n",
            "3199 0.4985 75.0\n",
            "3249 0.1278 100.0\n",
            "3299 0.3666 87.5\n",
            "3349 0.7479 68.75\n",
            "3399 0.6132 68.75\n",
            "3449 0.6416 81.25\n",
            "3499 0.5116 93.75\n",
            "3549 0.5914 81.25\n",
            "3599 0.2252 87.5\n",
            "3649 0.9197 62.5\n",
            "3699 0.2571 93.75\n",
            "3749 0.2573 93.75\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0316, Accuracy: 8229/10000 (82.29%)\n",
            "\n",
            "--- 17.16812777519226 seconds ---\n",
            "Epoch: 2\n",
            "49 0.0597 100.0\n",
            "99 0.1823 93.75\n",
            "149 0.2687 93.75\n",
            "199 0.2816 87.5\n",
            "249 0.2011 93.75\n",
            "299 0.1287 100.0\n",
            "349 0.5726 87.5\n",
            "399 0.2276 93.75\n",
            "449 0.1937 93.75\n",
            "499 0.7133 81.25\n",
            "549 0.3258 93.75\n",
            "599 0.4263 93.75\n",
            "649 0.3287 87.5\n",
            "699 0.2985 87.5\n",
            "749 0.3832 87.5\n",
            "799 0.3320 87.5\n",
            "849 0.3968 87.5\n",
            "899 0.2520 93.75\n",
            "949 0.4775 81.25\n",
            "999 0.5552 87.5\n",
            "1049 0.1797 100.0\n",
            "1099 0.1913 100.0\n",
            "1149 0.4345 81.25\n",
            "1199 0.4084 93.75\n",
            "1249 0.6296 81.25\n",
            "1299 0.3825 81.25\n",
            "1349 0.2637 87.5\n",
            "1399 0.2997 75.0\n",
            "1449 0.2572 93.75\n",
            "1499 0.1953 93.75\n",
            "1549 0.4880 87.5\n",
            "1599 0.6670 81.25\n",
            "1649 0.2308 93.75\n",
            "1699 0.1639 100.0\n",
            "1749 0.3859 81.25\n",
            "1799 0.1743 93.75\n",
            "1849 0.5112 81.25\n",
            "1899 0.5088 75.0\n",
            "1949 0.9712 62.5\n",
            "1999 0.7720 75.0\n",
            "2049 0.1827 93.75\n",
            "2099 0.3007 81.25\n",
            "2149 0.3740 87.5\n",
            "2199 0.1768 93.75\n",
            "2249 0.2745 87.5\n",
            "2299 0.2310 87.5\n",
            "2349 0.3050 87.5\n",
            "2399 0.4092 93.75\n",
            "2449 0.1009 100.0\n",
            "2499 0.4613 87.5\n",
            "2549 0.4421 81.25\n",
            "2599 0.2843 93.75\n",
            "2649 0.4555 81.25\n",
            "2699 0.4616 93.75\n",
            "2749 0.3571 87.5\n",
            "2799 0.2510 87.5\n",
            "2849 0.8171 62.5\n",
            "2899 0.2170 93.75\n",
            "2949 0.4020 81.25\n",
            "2999 0.5464 81.25\n",
            "3049 0.3754 87.5\n",
            "3099 0.3442 93.75\n",
            "3149 0.5627 87.5\n",
            "3199 0.3743 93.75\n",
            "3249 0.3439 93.75\n",
            "3299 0.2006 100.0\n",
            "3349 0.1755 100.0\n",
            "3399 0.1980 93.75\n",
            "3449 0.0515 100.0\n",
            "3499 0.2490 93.75\n",
            "3549 0.2500 87.5\n",
            "3599 0.2228 93.75\n",
            "3649 0.2501 100.0\n",
            "3699 0.2329 93.75\n",
            "3749 0.5101 81.25\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0320, Accuracy: 8251/10000 (82.51%)\n",
            "\n",
            "--- 16.551995754241943 seconds ---\n",
            "Epoch: 3\n",
            "49 0.4116 87.5\n",
            "99 0.0997 100.0\n",
            "149 0.1566 100.0\n",
            "199 0.2234 100.0\n",
            "249 0.1491 100.0\n",
            "299 0.4156 93.75\n",
            "349 0.0820 100.0\n",
            "399 0.2715 81.25\n",
            "449 0.3021 100.0\n",
            "499 0.1039 100.0\n",
            "549 0.1887 100.0\n",
            "599 0.3525 93.75\n",
            "649 0.0547 100.0\n",
            "699 0.2776 87.5\n",
            "749 0.0869 93.75\n",
            "799 0.3236 87.5\n",
            "849 0.2866 100.0\n",
            "899 0.1448 100.0\n",
            "949 0.2651 93.75\n",
            "999 0.3618 87.5\n",
            "1049 0.3578 93.75\n",
            "1099 0.2365 93.75\n",
            "1149 0.2537 87.5\n",
            "1199 0.6319 81.25\n",
            "1249 0.1482 100.0\n",
            "1299 0.2527 100.0\n",
            "1349 0.2322 87.5\n",
            "1399 0.1468 100.0\n",
            "1449 0.3494 93.75\n",
            "1499 0.5492 87.5\n",
            "1549 0.3969 81.25\n",
            "1599 0.2464 93.75\n",
            "1649 0.4376 87.5\n",
            "1699 0.4758 87.5\n",
            "1749 0.6029 87.5\n",
            "1799 0.3143 93.75\n",
            "1849 0.3637 87.5\n",
            "1899 0.1490 100.0\n",
            "1949 0.1250 100.0\n",
            "1999 0.1988 93.75\n",
            "2049 0.3493 81.25\n",
            "2099 0.2384 93.75\n",
            "2149 0.1653 100.0\n",
            "2199 0.3034 87.5\n",
            "2249 0.2321 93.75\n",
            "2299 0.1118 100.0\n",
            "2349 0.3189 93.75\n",
            "2399 0.2652 87.5\n",
            "2449 0.1283 100.0\n",
            "2499 0.4238 87.5\n",
            "2549 0.2203 93.75\n",
            "2599 0.2561 100.0\n",
            "2649 0.1961 93.75\n",
            "2699 0.3098 87.5\n",
            "2749 0.3404 87.5\n",
            "2799 0.2532 87.5\n",
            "2849 0.0489 100.0\n",
            "2899 0.8023 75.0\n",
            "2949 0.3683 93.75\n",
            "2999 0.1813 100.0\n",
            "3049 0.3853 87.5\n",
            "3099 0.2439 87.5\n",
            "3149 0.2321 87.5\n",
            "3199 0.3196 93.75\n",
            "3249 0.2189 87.5\n",
            "3299 0.2424 93.75\n",
            "3349 0.2130 93.75\n",
            "3399 0.1679 100.0\n",
            "3449 0.3746 87.5\n",
            "3499 0.1598 100.0\n",
            "3549 0.4337 75.0\n",
            "3599 0.6227 68.75\n",
            "3649 0.1676 93.75\n",
            "3699 0.7237 75.0\n",
            "3749 0.2271 93.75\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0312, Accuracy: 8246/10000 (82.46%)\n",
            "\n",
            "--- 16.410593509674072 seconds ---\n",
            "Epoch: 4\n",
            "49 0.1478 93.75\n",
            "99 0.2069 93.75\n",
            "149 0.3532 81.25\n",
            "199 0.2330 93.75\n",
            "249 0.3604 93.75\n",
            "299 0.0915 100.0\n",
            "349 0.2953 100.0\n",
            "399 0.2351 93.75\n",
            "449 0.1113 100.0\n",
            "499 0.1512 93.75\n",
            "549 0.1594 93.75\n",
            "599 0.2222 93.75\n",
            "649 0.0998 100.0\n",
            "699 0.0885 100.0\n",
            "749 0.2188 87.5\n",
            "799 0.4125 87.5\n",
            "849 0.0487 100.0\n",
            "899 0.2559 93.75\n",
            "949 0.2986 93.75\n",
            "999 0.1197 100.0\n",
            "1049 0.0784 100.0\n",
            "1099 0.3534 87.5\n",
            "1149 0.2718 93.75\n",
            "1199 0.3274 87.5\n",
            "1249 0.3070 93.75\n",
            "1299 0.3790 93.75\n",
            "1349 0.2171 93.75\n",
            "1399 0.2667 93.75\n",
            "1449 0.3816 87.5\n",
            "1499 0.1258 93.75\n",
            "1549 0.2555 93.75\n",
            "1599 0.2637 93.75\n",
            "1649 0.2360 87.5\n",
            "1699 0.1891 93.75\n",
            "1749 0.5424 87.5\n",
            "1799 0.1414 100.0\n",
            "1849 0.4534 75.0\n",
            "1899 0.2395 93.75\n",
            "1949 0.2444 87.5\n",
            "1999 0.3308 100.0\n",
            "2049 0.2396 93.75\n",
            "2099 0.4136 81.25\n",
            "2149 0.2030 93.75\n",
            "2199 0.1805 93.75\n",
            "2249 0.3428 100.0\n",
            "2299 0.1079 100.0\n",
            "2349 0.6946 81.25\n",
            "2399 0.1553 93.75\n",
            "2449 0.2615 87.5\n",
            "2499 0.2720 93.75\n",
            "2549 0.2270 93.75\n",
            "2599 0.3018 87.5\n",
            "2649 0.2343 93.75\n",
            "2699 0.2462 87.5\n",
            "2749 0.1464 100.0\n",
            "2799 0.2521 93.75\n",
            "2849 0.1130 100.0\n",
            "2899 0.2289 87.5\n",
            "2949 0.0970 100.0\n",
            "2999 0.2277 100.0\n",
            "3049 0.3632 81.25\n",
            "3099 0.2843 93.75\n",
            "3149 0.2175 100.0\n",
            "3199 0.2273 93.75\n",
            "3249 0.1706 93.75\n",
            "3299 0.1669 100.0\n",
            "3349 0.1507 93.75\n",
            "3399 0.5250 87.5\n",
            "3449 0.1487 100.0\n",
            "3499 0.4608 81.25\n",
            "3549 0.1694 93.75\n",
            "3599 0.3733 93.75\n",
            "3649 0.1905 93.75\n",
            "3699 0.0986 100.0\n",
            "3749 0.2986 87.5\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0311, Accuracy: 8299/10000 (82.99%)\n",
            "\n",
            "--- 16.91629672050476 seconds ---\n",
            "Epoch: 5\n",
            "49 0.0977 100.0\n",
            "99 0.0912 100.0\n",
            "149 0.4106 81.25\n",
            "199 0.2757 100.0\n",
            "249 0.3898 93.75\n",
            "299 0.4315 93.75\n",
            "349 0.1975 100.0\n",
            "399 0.2153 93.75\n",
            "449 0.2622 93.75\n",
            "499 0.2186 100.0\n",
            "549 0.0847 100.0\n",
            "599 0.2149 100.0\n",
            "649 0.2237 93.75\n",
            "699 0.2534 93.75\n",
            "749 0.3116 81.25\n",
            "799 0.2838 87.5\n",
            "849 0.1805 93.75\n",
            "899 0.0989 100.0\n",
            "949 0.4177 87.5\n",
            "999 0.5047 87.5\n",
            "1049 0.5480 75.0\n",
            "1099 0.3514 81.25\n",
            "1149 0.2130 93.75\n",
            "1199 0.3753 81.25\n",
            "1249 0.2701 93.75\n",
            "1299 0.3034 100.0\n",
            "1349 0.1893 93.75\n",
            "1399 0.2193 93.75\n",
            "1449 0.5640 75.0\n",
            "1499 0.3163 93.75\n",
            "1549 0.3006 93.75\n",
            "1599 0.3327 87.5\n",
            "1649 0.2286 93.75\n",
            "1699 0.1272 100.0\n",
            "1749 0.2942 87.5\n",
            "1799 0.2580 93.75\n",
            "1849 0.2674 93.75\n",
            "1899 0.3610 81.25\n",
            "1949 0.1285 100.0\n",
            "1999 0.3831 81.25\n",
            "2049 0.4421 87.5\n",
            "2099 0.2129 93.75\n",
            "2149 0.3624 81.25\n",
            "2199 0.4892 81.25\n",
            "2249 0.1415 100.0\n",
            "2299 0.2748 87.5\n",
            "2349 0.1226 100.0\n",
            "2399 0.1855 100.0\n",
            "2449 0.4883 81.25\n",
            "2499 0.1200 100.0\n",
            "2549 0.6666 75.0\n",
            "2599 0.1082 100.0\n",
            "2649 0.5391 87.5\n",
            "2699 0.2931 93.75\n",
            "2749 0.2732 87.5\n",
            "2799 0.1660 100.0\n",
            "2849 0.3254 87.5\n",
            "2899 0.4946 87.5\n",
            "2949 0.2577 87.5\n",
            "2999 0.5415 68.75\n",
            "3049 0.1810 93.75\n",
            "3099 0.2405 93.75\n",
            "3149 0.2431 93.75\n",
            "3199 0.1804 87.5\n",
            "3249 0.1436 100.0\n",
            "3299 0.4081 87.5\n",
            "3349 0.2326 100.0\n",
            "3399 0.1992 93.75\n",
            "3449 0.5900 68.75\n",
            "3499 0.2324 93.75\n",
            "3549 0.1795 100.0\n",
            "3599 0.3297 93.75\n",
            "3649 0.3767 81.25\n",
            "3699 0.3747 87.5\n",
            "3749 0.3777 87.5\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0308, Accuracy: 8305/10000 (83.05%)\n",
            "\n",
            "--- 16.51513957977295 seconds ---\n",
            "Epoch: 6\n",
            "49 0.4237 93.75\n",
            "99 0.4729 87.5\n",
            "149 0.6086 87.5\n",
            "199 0.0815 100.0\n",
            "249 0.2251 93.75\n",
            "299 0.1725 93.75\n",
            "349 0.2087 87.5\n",
            "399 0.3008 87.5\n",
            "449 0.1765 100.0\n",
            "499 0.4437 87.5\n",
            "549 0.2142 93.75\n",
            "599 0.1688 100.0\n",
            "649 0.2281 81.25\n",
            "699 0.3939 81.25\n",
            "749 0.1907 93.75\n",
            "799 0.1376 100.0\n",
            "849 0.2276 100.0\n",
            "899 0.2051 93.75\n",
            "949 0.3544 87.5\n",
            "999 0.3572 87.5\n",
            "1049 0.1971 87.5\n",
            "1099 0.1784 100.0\n",
            "1149 0.2527 93.75\n",
            "1199 0.1586 93.75\n",
            "1249 0.4606 93.75\n",
            "1299 0.1702 93.75\n",
            "1349 0.4130 87.5\n",
            "1399 0.2338 93.75\n",
            "1449 0.1486 93.75\n",
            "1499 0.3747 81.25\n",
            "1549 0.2130 93.75\n",
            "1599 0.2462 93.75\n",
            "1649 0.3683 87.5\n",
            "1699 0.0848 100.0\n",
            "1749 0.2407 93.75\n",
            "1799 0.4355 81.25\n",
            "1849 0.1932 93.75\n",
            "1899 0.1989 93.75\n",
            "1949 0.4692 81.25\n",
            "1999 0.4782 87.5\n",
            "2049 0.3274 93.75\n",
            "2099 0.1216 100.0\n",
            "2149 0.0483 100.0\n",
            "2199 0.2012 93.75\n",
            "2249 0.2654 100.0\n",
            "2299 0.3457 93.75\n",
            "2349 0.4298 87.5\n",
            "2399 0.3510 87.5\n",
            "2449 0.3095 81.25\n",
            "2499 0.1249 100.0\n",
            "2549 0.7771 75.0\n",
            "2599 0.4246 87.5\n",
            "2649 0.4309 75.0\n",
            "2699 0.1068 93.75\n",
            "2749 0.5500 93.75\n",
            "2799 0.3015 87.5\n",
            "2849 0.2637 93.75\n",
            "2899 0.4145 87.5\n",
            "2949 0.3334 87.5\n",
            "2999 0.3794 75.0\n",
            "3049 0.4014 93.75\n",
            "3099 0.1567 93.75\n",
            "3149 0.5404 81.25\n",
            "3199 0.2043 87.5\n",
            "3249 0.4242 93.75\n",
            "3299 0.2942 81.25\n",
            "3349 0.2331 93.75\n",
            "3399 0.1523 100.0\n",
            "3449 0.2028 93.75\n",
            "3499 0.3868 87.5\n",
            "3549 0.2485 93.75\n",
            "3599 0.0713 100.0\n",
            "3649 0.2139 93.75\n",
            "3699 0.2932 93.75\n",
            "3749 0.4956 87.5\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0308, Accuracy: 8283/10000 (82.83%)\n",
            "\n",
            "--- 16.344928979873657 seconds ---\n",
            "Epoch: 7\n",
            "49 0.2225 93.75\n",
            "99 0.1879 100.0\n",
            "149 0.1965 100.0\n",
            "199 0.3464 81.25\n",
            "249 0.2249 93.75\n",
            "299 0.0878 100.0\n",
            "349 0.3804 87.5\n",
            "399 0.1783 100.0\n",
            "449 0.1315 100.0\n",
            "499 0.0980 100.0\n",
            "549 0.1352 100.0\n",
            "599 0.2229 93.75\n",
            "649 0.2882 87.5\n",
            "699 0.2871 93.75\n",
            "749 0.2958 93.75\n",
            "799 0.2369 87.5\n",
            "849 0.3550 87.5\n",
            "899 0.2864 93.75\n",
            "949 0.2356 100.0\n",
            "999 0.2761 93.75\n",
            "1049 0.1366 93.75\n",
            "1099 0.1143 93.75\n",
            "1149 0.1821 100.0\n",
            "1199 0.0822 100.0\n",
            "1249 0.4628 81.25\n",
            "1299 0.3467 87.5\n",
            "1349 0.1724 93.75\n",
            "1399 0.3049 87.5\n",
            "1449 0.2162 100.0\n",
            "1499 0.3049 93.75\n",
            "1549 0.3254 100.0\n",
            "1599 0.2801 93.75\n",
            "1649 0.4165 93.75\n",
            "1699 0.5419 75.0\n",
            "1749 0.3001 93.75\n",
            "1799 0.1772 93.75\n",
            "1849 0.2113 93.75\n",
            "1899 0.3693 87.5\n",
            "1949 0.2612 93.75\n",
            "1999 0.4887 87.5\n",
            "2049 0.2075 87.5\n",
            "2099 0.3062 93.75\n",
            "2149 0.0515 100.0\n",
            "2199 0.3949 93.75\n",
            "2249 0.1397 100.0\n",
            "2299 0.4443 81.25\n",
            "2349 0.1160 100.0\n",
            "2399 0.1385 100.0\n",
            "2449 0.0812 100.0\n",
            "2499 0.3444 87.5\n",
            "2549 0.3569 93.75\n",
            "2599 0.6168 75.0\n",
            "2649 0.4718 87.5\n",
            "2699 0.3589 87.5\n",
            "2749 0.1236 93.75\n",
            "2799 0.1266 100.0\n",
            "2849 0.2824 87.5\n",
            "2899 0.4431 81.25\n",
            "2949 0.2496 100.0\n",
            "2999 0.1564 100.0\n",
            "3049 0.2738 93.75\n",
            "3099 0.5744 81.25\n",
            "3149 0.1407 100.0\n",
            "3199 0.2843 87.5\n",
            "3249 0.2696 93.75\n",
            "3299 0.5773 81.25\n",
            "3349 0.3381 87.5\n",
            "3399 0.3914 81.25\n",
            "3449 0.4471 87.5\n",
            "3499 0.3093 87.5\n",
            "3549 0.6366 75.0\n",
            "3599 0.5540 87.5\n",
            "3649 0.3071 87.5\n",
            "3699 0.4290 87.5\n",
            "3749 0.2969 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0308, Accuracy: 8296/10000 (82.96%)\n",
            "\n",
            "--- 16.666164875030518 seconds ---\n",
            "Epoch: 8\n",
            "49 0.2452 87.5\n",
            "99 0.2835 81.25\n",
            "149 0.1358 100.0\n",
            "199 0.0957 100.0\n",
            "249 0.2639 100.0\n",
            "299 0.1836 93.75\n",
            "349 0.1919 87.5\n",
            "399 0.3538 87.5\n",
            "449 0.1244 100.0\n",
            "499 0.3843 81.25\n",
            "549 0.2127 100.0\n",
            "599 0.5788 87.5\n",
            "649 0.1783 93.75\n",
            "699 0.3437 87.5\n",
            "749 0.2951 87.5\n",
            "799 0.3244 93.75\n",
            "849 0.2614 93.75\n",
            "899 0.4915 81.25\n",
            "949 0.1345 100.0\n",
            "999 0.6133 68.75\n",
            "1049 0.6154 87.5\n",
            "1099 0.2253 93.75\n",
            "1149 0.2673 87.5\n",
            "1199 0.0615 100.0\n",
            "1249 0.3144 93.75\n",
            "1299 0.2559 87.5\n",
            "1349 0.1869 93.75\n",
            "1399 0.2607 87.5\n",
            "1449 0.1929 100.0\n",
            "1499 0.2470 87.5\n",
            "1549 0.4514 81.25\n",
            "1599 0.3134 87.5\n",
            "1649 0.6227 87.5\n",
            "1699 0.1069 100.0\n",
            "1749 0.4876 81.25\n",
            "1799 0.1975 93.75\n",
            "1849 0.2829 93.75\n",
            "1899 0.3910 87.5\n",
            "1949 0.2265 93.75\n",
            "1999 0.0906 100.0\n",
            "2049 0.2811 87.5\n",
            "2099 0.2036 93.75\n",
            "2149 0.1671 93.75\n",
            "2199 0.0987 100.0\n",
            "2249 0.2525 93.75\n",
            "2299 0.1158 100.0\n",
            "2349 0.3004 100.0\n",
            "2399 0.0539 100.0\n",
            "2449 0.3359 93.75\n",
            "2499 0.0967 100.0\n",
            "2549 0.2772 93.75\n",
            "2599 0.3831 87.5\n",
            "2649 0.5870 93.75\n",
            "2699 0.1827 93.75\n",
            "2749 0.3530 93.75\n",
            "2799 0.3736 87.5\n",
            "2849 0.3744 93.75\n",
            "2899 0.3499 87.5\n",
            "2949 0.2310 93.75\n",
            "2999 0.1013 100.0\n",
            "3049 0.4645 87.5\n",
            "3099 0.2817 87.5\n",
            "3149 0.3608 93.75\n",
            "3199 0.2893 87.5\n",
            "3249 0.0795 100.0\n",
            "3299 0.3200 87.5\n",
            "3349 0.3324 93.75\n",
            "3399 0.3359 81.25\n",
            "3449 0.2010 100.0\n",
            "3499 0.1066 100.0\n",
            "3549 0.2934 93.75\n",
            "3599 0.5322 81.25\n",
            "3649 0.4193 75.0\n",
            "3699 0.1278 100.0\n",
            "3749 0.5228 87.5\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0310, Accuracy: 8300/10000 (83.00%)\n",
            "\n",
            "--- 16.942968130111694 seconds ---\n",
            "Epoch: 9\n",
            "49 0.1100 100.0\n",
            "99 0.4905 81.25\n",
            "149 0.1284 100.0\n",
            "199 0.2929 93.75\n",
            "249 0.3688 87.5\n",
            "299 0.2715 87.5\n",
            "349 0.1526 100.0\n",
            "399 0.3098 93.75\n",
            "449 0.2404 93.75\n",
            "499 0.0432 100.0\n",
            "549 0.1310 100.0\n",
            "599 0.1638 100.0\n",
            "649 0.1115 100.0\n",
            "699 0.1967 93.75\n",
            "749 0.1494 93.75\n",
            "799 0.1743 100.0\n",
            "849 0.1621 100.0\n",
            "899 0.4890 75.0\n",
            "949 0.2620 93.75\n",
            "999 0.1008 100.0\n",
            "1049 0.5031 81.25\n",
            "1099 0.3351 93.75\n",
            "1149 0.2015 100.0\n",
            "1199 0.1790 87.5\n",
            "1249 0.2381 87.5\n",
            "1299 0.2072 87.5\n",
            "1349 0.1225 93.75\n",
            "1399 0.2094 93.75\n",
            "1449 0.2827 81.25\n",
            "1499 0.1058 100.0\n",
            "1549 0.2319 93.75\n",
            "1599 0.4973 75.0\n",
            "1649 0.3089 93.75\n",
            "1699 0.2961 93.75\n",
            "1749 0.5473 81.25\n",
            "1799 0.2692 81.25\n",
            "1849 0.3536 81.25\n",
            "1899 0.2318 93.75\n",
            "1949 0.2811 93.75\n",
            "1999 0.2081 93.75\n",
            "2049 0.5443 81.25\n",
            "2099 0.4678 81.25\n",
            "2149 0.3643 93.75\n",
            "2199 0.3305 93.75\n",
            "2249 0.4532 81.25\n",
            "2299 0.3448 93.75\n",
            "2349 0.2912 87.5\n",
            "2399 0.5353 81.25\n",
            "2449 0.6021 93.75\n",
            "2499 0.2791 93.75\n",
            "2549 0.3747 81.25\n",
            "2599 0.2820 87.5\n",
            "2649 0.6991 75.0\n",
            "2699 0.2562 93.75\n",
            "2749 0.1766 87.5\n",
            "2799 0.3148 87.5\n",
            "2849 0.0845 100.0\n",
            "2899 0.5029 68.75\n",
            "2949 0.2654 87.5\n",
            "2999 0.3637 87.5\n",
            "3049 0.4067 81.25\n",
            "3099 0.4662 75.0\n",
            "3149 0.1568 100.0\n",
            "3199 0.1436 100.0\n",
            "3249 0.2476 93.75\n",
            "3299 0.1269 100.0\n",
            "3349 0.3822 81.25\n",
            "3399 0.1999 93.75\n",
            "3449 0.1390 100.0\n",
            "3499 0.5953 81.25\n",
            "3549 0.2790 87.5\n",
            "3599 0.5813 81.25\n",
            "3649 0.2129 93.75\n",
            "3699 0.2937 93.75\n",
            "3749 0.9296 68.75\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0309, Accuracy: 8297/10000 (82.97%)\n",
            "\n",
            "--- 16.349224090576172 seconds ---\n",
            "Epoch: 10\n",
            "49 0.1625 93.75\n",
            "99 0.1456 100.0\n",
            "149 0.0820 100.0\n",
            "199 0.0978 100.0\n",
            "249 0.2141 93.75\n",
            "299 0.1363 100.0\n",
            "349 0.4363 93.75\n",
            "399 0.2668 93.75\n",
            "449 0.2423 93.75\n",
            "499 0.2717 87.5\n",
            "549 0.2478 93.75\n",
            "599 0.1879 100.0\n",
            "649 0.3090 81.25\n",
            "699 0.3631 87.5\n",
            "749 0.3053 87.5\n",
            "799 0.2349 93.75\n",
            "849 0.2219 93.75\n",
            "899 0.0960 100.0\n",
            "949 0.1837 93.75\n",
            "999 0.2328 93.75\n",
            "1049 0.2759 93.75\n",
            "1099 0.2559 81.25\n",
            "1149 0.1364 100.0\n",
            "1199 0.2843 87.5\n",
            "1249 0.1699 100.0\n",
            "1299 0.1859 87.5\n",
            "1349 0.2305 87.5\n",
            "1399 0.2364 87.5\n",
            "1449 0.3322 87.5\n",
            "1499 0.3616 87.5\n",
            "1549 0.5342 81.25\n",
            "1599 0.5601 81.25\n",
            "1649 0.3138 93.75\n",
            "1699 0.4775 87.5\n",
            "1749 0.1942 100.0\n",
            "1799 0.3462 93.75\n",
            "1849 0.3224 93.75\n",
            "1899 0.3691 87.5\n",
            "1949 0.2461 93.75\n",
            "1999 0.4226 81.25\n",
            "2049 0.3576 87.5\n",
            "2099 0.1007 100.0\n",
            "2149 0.1004 100.0\n",
            "2199 0.0644 100.0\n",
            "2249 0.4227 93.75\n",
            "2299 0.2207 100.0\n",
            "2349 0.2789 93.75\n",
            "2399 0.3240 93.75\n",
            "2449 0.0632 100.0\n",
            "2499 0.4686 81.25\n",
            "2549 0.2945 87.5\n",
            "2599 0.2309 93.75\n",
            "2649 0.1301 100.0\n",
            "2699 0.5629 81.25\n",
            "2749 0.3991 87.5\n",
            "2799 0.1263 100.0\n",
            "2849 0.2311 93.75\n",
            "2899 0.3268 87.5\n",
            "2949 0.3724 93.75\n",
            "2999 0.2766 93.75\n",
            "3049 0.3151 87.5\n",
            "3099 0.3496 87.5\n",
            "3149 0.4580 87.5\n",
            "3199 0.2812 87.5\n",
            "3249 0.1504 100.0\n",
            "3299 0.3887 87.5\n",
            "3349 0.2014 93.75\n",
            "3399 0.1247 100.0\n",
            "3449 0.3027 87.5\n",
            "3499 0.3859 87.5\n",
            "3549 0.2762 93.75\n",
            "3599 0.1530 100.0\n",
            "3649 0.4340 87.5\n",
            "3699 0.3748 87.5\n",
            "3749 0.2273 93.75\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0309, Accuracy: 8258/10000 (82.58%)\n",
            "\n",
            "--- 16.107399702072144 seconds ---\n"
          ]
        }
      ],
      "source": [
        "#fminst epoch 10 rff-hdc\n",
        "class ArgumentParser:\n",
        "    def __init__(self):\n",
        "        self.args = {\n",
        "            'lr': 0.01,            # Learning rate for optimizing class representative\n",
        "            'gamma': 0.3,          # Kernel parameter for computing covariance\n",
        "            'epoch': 10,            # Epochs of training\n",
        "            'gorder': 16,           # Order of the cyclic group required for G-VSA (8 ou 16)\n",
        "            'dim': 10000,          # Dimension of hypervectors c le grand D\n",
        "            'seed': 43,            # Random seed for reproducing results\n",
        "            'resume': True,       # Resume from existing encoded hypervectors\n",
        "            'data_dir': './encoded_data', # Directory to save encoded data (hypervectors)\n",
        "            'dataset': 'fmnist',    # Dataset choice choices=['mnist', 'fmnist', 'cifar', 'isolet', 'ucihar']\n",
        "            'raw_data_dir': './dataset',   # Raw data directory\n",
        "            'model': 'rff-hdc'    # Model type choices=['rff-hdc', 'linear-hdc', 'rff-gvsa']\n",
        "        }\n",
        "        self.seed = self.args['seed']\n",
        "        self.lr = self.args['lr']\n",
        "        self.gamma = self.args['gamma']\n",
        "        self.epoch = self.args['epoch']\n",
        "        self.gorder = self.args['gorder']\n",
        "        self.dim = self.args['dim']\n",
        "        self.resume = self.args['resume']\n",
        "        self.data_dir = self.args['data_dir']\n",
        "        self.dataset = self.args['dataset']\n",
        "        self.raw_data_dir = self.args['raw_data_dir']\n",
        "        self.model = self.args['model']\n",
        "\n",
        "\n",
        "    def get_args(self):\n",
        "        return self.args\n",
        "args = ArgumentParser()\n",
        "torch.manual_seed(args.seed)\n",
        "np.random.seed(args.seed)\n",
        "if 'hdc' in args.model:\n",
        "    args.gorder = 2\n",
        "    print(\"Use binary HDC with random fourier features, ignoring gorder, set to 2.\")\n",
        "args.data_dir = f'{args.data_dir}/{args.dataset}_{args.model}_order{args.gorder}_gamma{args.gamma}_dim{args.dim}'\n",
        "try:\n",
        "    os.makedirs(args.data_dir)\n",
        "except FileExistsError:\n",
        "    print('Encoded data folder already exists')\n",
        "if not args.resume:\n",
        "    print('Encode the dataset into hypervectors and save')\n",
        "    encode_and_save(args)\n",
        "    print('Finish encoding and saving')\n",
        "print(f'Optimizing class representatives for {args.epoch} epochs')\n",
        "train(args)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RFF G($2^1$)-VSA"
      ],
      "metadata": {
        "id": "hwNHKRIsQEEG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ISOLET"
      ],
      "metadata": {
        "id": "ZH6fdSxnQRNp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f303fb56-d1bc-40f6-fe19-41ac1aa8e357",
        "id": "4csPwwmIQRNp"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encode the dataset into hypervectors and save\n",
            "# of channels of data 1\n",
            "# of training samples and test samples 6238 1559\n",
            "Encoding with random fourier features encoder.\n",
            "the threshold to discretize fourier features to group elements tensor([0.])\n",
            "Encoded pixels to hypervectors with size:  torch.Size([256, 10000])\n",
            "Encoding training data...\n",
            "Start encoding data\n",
            "Finish encoding data\n",
            "Encoding test data...\n",
            "Start encoding data\n",
            "Finish encoding data\n",
            "Finish encoding and saving\n",
            "Optimizing class representatives for 1 epochs\n",
            "Loading encoded training data...\n",
            "Loading encoded test data...\n",
            "Size of encoded training data torch.Size([6238, 10000]) and test data torch.Size([1559, 10000])\n",
            "Epoch: 1\n",
            "49 1.9258 62.5\n",
            "99 0.5204 93.75\n",
            "149 0.4092 87.5\n",
            "199 0.1536 100.0\n",
            "249 0.2872 87.5\n",
            "299 0.3097 87.5\n",
            "349 0.0613 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0201, Accuracy: 1402/1559 (89.93%)\n",
            "\n",
            "--- 2.3380937576293945 seconds ---\n"
          ]
        }
      ],
      "source": [
        "#isolet epoch 1 rffg2vsa\n",
        "class ArgumentParser:\n",
        "    def __init__(self):\n",
        "        self.args = {\n",
        "            'lr': 0.01,            # Learning rate for optimizing class representative\n",
        "            'gamma': 0.3,          # Kernel parameter for computing covariance\n",
        "            'epoch': 1,            # Epochs of training\n",
        "            'gorder': 2,           # Order of the cyclic group required for G-VSA (8 ou 16)\n",
        "            'dim': 10000,          # Dimension of hypervectors c le grand D\n",
        "            'seed': 43,            # Random seed for reproducing results\n",
        "            'resume': False,       # Resume from existing encoded hypervectors\n",
        "            'data_dir': './encoded_data', # Directory to save encoded data (hypervectors)\n",
        "            'dataset': 'isolet',    # Dataset choice choices=['mnist', 'fmnist', 'cifar', 'isolet', 'ucihar']\n",
        "            'raw_data_dir': './dataset',   # Raw data directory\n",
        "            'model': 'rff-gvsa'    # Model type choices=['rff-hdc', 'linear-hdc', 'rff-gvsa']\n",
        "        }\n",
        "        self.seed = self.args['seed']\n",
        "        self.lr = self.args['lr']\n",
        "        self.gamma = self.args['gamma']\n",
        "        self.epoch = self.args['epoch']\n",
        "        self.gorder = self.args['gorder']\n",
        "        self.dim = self.args['dim']\n",
        "        self.resume = self.args['resume']\n",
        "        self.data_dir = self.args['data_dir']\n",
        "        self.dataset = self.args['dataset']\n",
        "        self.raw_data_dir = self.args['raw_data_dir']\n",
        "        self.model = self.args['model']\n",
        "\n",
        "\n",
        "    def get_args(self):\n",
        "        return self.args\n",
        "args = ArgumentParser()\n",
        "torch.manual_seed(args.seed)\n",
        "np.random.seed(args.seed)\n",
        "if 'hdc' in args.model:\n",
        "    args.gorder = 2\n",
        "    print(\"Use binary HDC with random fourier features, ignoring gorder, set to 2.\")\n",
        "args.data_dir = f'{args.data_dir}/{args.dataset}_{args.model}_order{args.gorder}_gamma{args.gamma}_dim{args.dim}'\n",
        "try:\n",
        "    os.makedirs(args.data_dir)\n",
        "except FileExistsError:\n",
        "    print('Encoded data folder already exists')\n",
        "if not args.resume:\n",
        "    print('Encode the dataset into hypervectors and save')\n",
        "    encode_and_save(args)\n",
        "    print('Finish encoding and saving')\n",
        "print(f'Optimizing class representatives for {args.epoch} epochs')\n",
        "train(args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8e1f7c9-391d-4303-d103-940d51a65253",
        "id": "V0w65FeiQRNr"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoded data folder already exists\n",
            "Optimizing class representatives for 10 epochs\n",
            "Loading encoded training data...\n",
            "Loading encoded test data...\n",
            "Size of encoded training data torch.Size([6238, 10000]) and test data torch.Size([1559, 10000])\n",
            "Epoch: 1\n",
            "49 1.9179 62.5\n",
            "99 0.7909 87.5\n",
            "149 0.1490 93.75\n",
            "199 0.7417 75.0\n",
            "249 0.4739 81.25\n",
            "299 0.5101 87.5\n",
            "349 0.1667 87.5\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0231, Accuracy: 1394/1559 (89.42%)\n",
            "\n",
            "--- 2.451706886291504 seconds ---\n",
            "Epoch: 2\n",
            "49 0.1609 93.75\n",
            "99 0.0116 100.0\n",
            "149 0.0032 100.0\n",
            "199 0.0029 100.0\n",
            "249 0.0116 100.0\n",
            "299 0.0044 100.0\n",
            "349 0.0050 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0169, Accuracy: 1434/1559 (91.98%)\n",
            "\n",
            "--- 1.9702389240264893 seconds ---\n",
            "Epoch: 3\n",
            "49 0.0032 100.0\n",
            "99 0.0005 100.0\n",
            "149 0.0011 100.0\n",
            "199 0.0028 100.0\n",
            "249 0.0006 100.0\n",
            "299 0.0012 100.0\n",
            "349 0.0026 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0155, Accuracy: 1439/1559 (92.30%)\n",
            "\n",
            "--- 1.9846508502960205 seconds ---\n",
            "Epoch: 4\n",
            "49 0.0007 100.0\n",
            "99 0.0007 100.0\n",
            "149 0.0018 100.0\n",
            "199 0.0004 100.0\n",
            "249 0.0010 100.0\n",
            "299 0.0011 100.0\n",
            "349 0.0007 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0153, Accuracy: 1439/1559 (92.30%)\n",
            "\n",
            "--- 1.983633279800415 seconds ---\n",
            "Epoch: 5\n",
            "49 0.0005 100.0\n",
            "99 0.0008 100.0\n",
            "149 0.0010 100.0\n",
            "199 0.0003 100.0\n",
            "249 0.0009 100.0\n",
            "299 0.0008 100.0\n",
            "349 0.0009 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0154, Accuracy: 1439/1559 (92.30%)\n",
            "\n",
            "--- 1.9757945537567139 seconds ---\n",
            "Epoch: 6\n",
            "49 0.0004 100.0\n",
            "99 0.0005 100.0\n",
            "149 0.0003 100.0\n",
            "199 0.0006 100.0\n",
            "249 0.0005 100.0\n",
            "299 0.0005 100.0\n",
            "349 0.0007 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0154, Accuracy: 1443/1559 (92.56%)\n",
            "\n",
            "--- 2.4049911499023438 seconds ---\n",
            "Epoch: 7\n",
            "49 0.0001 100.0\n",
            "99 0.0002 100.0\n",
            "149 0.0003 100.0\n",
            "199 0.0002 100.0\n",
            "249 0.0004 100.0\n",
            "299 0.0005 100.0\n",
            "349 0.0002 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0155, Accuracy: 1447/1559 (92.82%)\n",
            "\n",
            "--- 2.4636342525482178 seconds ---\n",
            "Epoch: 8\n",
            "49 0.0002 100.0\n",
            "99 0.0003 100.0\n",
            "149 0.0002 100.0\n",
            "199 0.0002 100.0\n",
            "249 0.0002 100.0\n",
            "299 0.0002 100.0\n",
            "349 0.0003 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0155, Accuracy: 1447/1559 (92.82%)\n",
            "\n",
            "--- 1.993129014968872 seconds ---\n",
            "Epoch: 9\n",
            "49 0.0002 100.0\n",
            "99 0.0001 100.0\n",
            "149 0.0002 100.0\n",
            "199 0.0001 100.0\n",
            "249 0.0002 100.0\n",
            "299 0.0001 100.0\n",
            "349 0.0002 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0156, Accuracy: 1441/1559 (92.43%)\n",
            "\n",
            "--- 2.020817518234253 seconds ---\n",
            "Epoch: 10\n",
            "49 0.0001 100.0\n",
            "99 0.0001 100.0\n",
            "149 0.0001 100.0\n",
            "199 0.0001 100.0\n",
            "249 0.0001 100.0\n",
            "299 0.0001 100.0\n",
            "349 0.0002 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0159, Accuracy: 1442/1559 (92.50%)\n",
            "\n",
            "--- 2.023744583129883 seconds ---\n"
          ]
        }
      ],
      "source": [
        "#isolet epoch 10 rffg2vsa\n",
        "class ArgumentParser:\n",
        "    def __init__(self):\n",
        "        self.args = {\n",
        "            'lr': 0.01,            # Learning rate for optimizing class representative\n",
        "            'gamma': 0.3,          # Kernel parameter for computing covariance\n",
        "            'epoch': 10,            # Epochs of training\n",
        "            'gorder': 2,           # Order of the cyclic group required for G-VSA (8 ou 16)\n",
        "            'dim': 10000,          # Dimension of hypervectors c le grand D\n",
        "            'seed': 43,            # Random seed for reproducing results\n",
        "            'resume': True,       # Resume from existing encoded hypervectors\n",
        "            'data_dir': './encoded_data', # Directory to save encoded data (hypervectors)\n",
        "            'dataset': 'isolet',    # Dataset choice choices=['mnist', 'fmnist', 'cifar', 'isolet', 'ucihar']\n",
        "            'raw_data_dir': './dataset',   # Raw data directory\n",
        "            'model': 'rff-gvsa'    # Model type choices=['rff-hdc', 'linear-hdc', 'rff-gvsa']\n",
        "        }\n",
        "        self.seed = self.args['seed']\n",
        "        self.lr = self.args['lr']\n",
        "        self.gamma = self.args['gamma']\n",
        "        self.epoch = self.args['epoch']\n",
        "        self.gorder = self.args['gorder']\n",
        "        self.dim = self.args['dim']\n",
        "        self.resume = self.args['resume']\n",
        "        self.data_dir = self.args['data_dir']\n",
        "        self.dataset = self.args['dataset']\n",
        "        self.raw_data_dir = self.args['raw_data_dir']\n",
        "        self.model = self.args['model']\n",
        "\n",
        "\n",
        "    def get_args(self):\n",
        "        return self.args\n",
        "args = ArgumentParser()\n",
        "torch.manual_seed(args.seed)\n",
        "np.random.seed(args.seed)\n",
        "if 'hdc' in args.model:\n",
        "    args.gorder = 2\n",
        "    print(\"Use binary HDC with random fourier features, ignoring gorder, set to 2.\")\n",
        "args.data_dir = f'{args.data_dir}/{args.dataset}_{args.model}_order{args.gorder}_gamma{args.gamma}_dim{args.dim}'\n",
        "try:\n",
        "    os.makedirs(args.data_dir)\n",
        "except FileExistsError:\n",
        "    print('Encoded data folder already exists')\n",
        "if not args.resume:\n",
        "    print('Encode the dataset into hypervectors and save')\n",
        "    encode_and_save(args)\n",
        "    print('Finish encoding and saving')\n",
        "print(f'Optimizing class representatives for {args.epoch} epochs')\n",
        "train(args)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## UCIHAR"
      ],
      "metadata": {
        "id": "lVntyNNmQRNs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "073af4f7-dbeb-4272-9ad7-44ceeb2ff311",
        "id": "mwG43AUPQRNs"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encode the dataset into hypervectors and save\n",
            "# of channels of data 1\n",
            "# of training samples and test samples 7352 2947\n",
            "Encoding with random fourier features encoder.\n",
            "the threshold to discretize fourier features to group elements tensor([0.])\n",
            "Encoded pixels to hypervectors with size:  torch.Size([256, 10000])\n",
            "Encoding training data...\n",
            "Start encoding data\n",
            "Finish encoding data\n",
            "Encoding test data...\n",
            "Start encoding data\n",
            "Finish encoding data\n",
            "Finish encoding and saving\n",
            "Optimizing class representatives for 1 epochs\n",
            "Loading encoded training data...\n",
            "Loading encoded test data...\n",
            "Size of encoded training data torch.Size([7352, 10000]) and test data torch.Size([2947, 10000])\n",
            "Epoch: 1\n",
            "49 0.5121 75.0\n",
            "99 0.0914 100.0\n",
            "149 0.0829 100.0\n",
            "199 0.0277 100.0\n",
            "249 0.0409 100.0\n",
            "299 0.0792 93.75\n",
            "349 0.0384 100.0\n",
            "399 0.0257 100.0\n",
            "449 0.0586 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0131, Accuracy: 2730/2947 (92.64%)\n",
            "\n",
            "--- 3.291971445083618 seconds ---\n"
          ]
        }
      ],
      "source": [
        "#ucihar epoch 1 rffg2vsa\n",
        "class ArgumentParser:\n",
        "    def __init__(self):\n",
        "        self.args = {\n",
        "            'lr': 0.01,            # Learning rate for optimizing class representative\n",
        "            'gamma': 0.3,          # Kernel parameter for computing covariance\n",
        "            'epoch': 1,            # Epochs of training\n",
        "            'gorder': 2,           # Order of the cyclic group required for G-VSA (8 ou 16)\n",
        "            'dim': 10000,          # Dimension of hypervectors c le grand D\n",
        "            'seed': 43,            # Random seed for reproducing results\n",
        "            'resume': False,       # Resume from existing encoded hypervectors\n",
        "            'data_dir': './encoded_data', # Directory to save encoded data (hypervectors)\n",
        "            'dataset': 'ucihar',    # Dataset choice choices=['mnist', 'fmnist', 'cifar', 'isolet', 'ucihar']\n",
        "            'raw_data_dir': './dataset',   # Raw data directory\n",
        "            'model': 'rff-gvsa'    # Model type choices=['rff-hdc', 'linear-hdc', 'rff-gvsa']\n",
        "        }\n",
        "        self.seed = self.args['seed']\n",
        "        self.lr = self.args['lr']\n",
        "        self.gamma = self.args['gamma']\n",
        "        self.epoch = self.args['epoch']\n",
        "        self.gorder = self.args['gorder']\n",
        "        self.dim = self.args['dim']\n",
        "        self.resume = self.args['resume']\n",
        "        self.data_dir = self.args['data_dir']\n",
        "        self.dataset = self.args['dataset']\n",
        "        self.raw_data_dir = self.args['raw_data_dir']\n",
        "        self.model = self.args['model']\n",
        "\n",
        "\n",
        "    def get_args(self):\n",
        "        return self.args\n",
        "args = ArgumentParser()\n",
        "torch.manual_seed(args.seed)\n",
        "np.random.seed(args.seed)\n",
        "if 'hdc' in args.model:\n",
        "    args.gorder = 2\n",
        "    print(\"Use binary HDC with random fourier features, ignoring gorder, set to 2.\")\n",
        "args.data_dir = f'{args.data_dir}/{args.dataset}_{args.model}_order{args.gorder}_gamma{args.gamma}_dim{args.dim}'\n",
        "try:\n",
        "    os.makedirs(args.data_dir)\n",
        "except FileExistsError:\n",
        "    print('Encoded data folder already exists')\n",
        "if not args.resume:\n",
        "    print('Encode the dataset into hypervectors and save')\n",
        "    encode_and_save(args)\n",
        "    print('Finish encoding and saving')\n",
        "print(f'Optimizing class representatives for {args.epoch} epochs')\n",
        "train(args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "555b00ae-9f76-4f6a-970d-ea0ee7d1539b",
        "id": "oNNyMyP6QRNt"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoded data folder already exists\n",
            "Optimizing class representatives for 10 epochs\n",
            "Loading encoded training data...\n",
            "Loading encoded test data...\n",
            "Size of encoded training data torch.Size([7352, 10000]) and test data torch.Size([2947, 10000])\n",
            "Epoch: 1\n",
            "49 0.2738 93.75\n",
            "99 0.1169 100.0\n",
            "149 0.1295 93.75\n",
            "199 0.0868 100.0\n",
            "249 0.0485 100.0\n",
            "299 0.0551 100.0\n",
            "349 0.0487 100.0\n",
            "399 0.0140 100.0\n",
            "449 0.0656 93.75\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0089, Accuracy: 2804/2947 (95.15%)\n",
            "\n",
            "--- 2.5409297943115234 seconds ---\n",
            "Epoch: 2\n",
            "49 0.0054 100.0\n",
            "99 0.0545 93.75\n",
            "149 0.0192 100.0\n",
            "199 0.0201 100.0\n",
            "249 0.0308 100.0\n",
            "299 0.0144 100.0\n",
            "349 0.0298 100.0\n",
            "399 0.0015 100.0\n",
            "449 0.0035 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0093, Accuracy: 2783/2947 (94.44%)\n",
            "\n",
            "--- 2.466930866241455 seconds ---\n",
            "Epoch: 3\n",
            "49 0.0018 100.0\n",
            "99 0.0002 100.0\n",
            "149 0.0003 100.0\n",
            "199 0.0141 100.0\n",
            "249 0.0005 100.0\n",
            "299 0.0008 100.0\n",
            "349 0.0054 100.0\n",
            "399 0.0116 100.0\n",
            "449 0.0139 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0093, Accuracy: 2789/2947 (94.64%)\n",
            "\n",
            "--- 2.508359670639038 seconds ---\n",
            "Epoch: 4\n",
            "49 0.0023 100.0\n",
            "99 0.0006 100.0\n",
            "149 0.0028 100.0\n",
            "199 0.0004 100.0\n",
            "249 0.0001 100.0\n",
            "299 0.0041 100.0\n",
            "349 0.0010 100.0\n",
            "399 0.0068 100.0\n",
            "449 0.0005 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0098, Accuracy: 2781/2947 (94.37%)\n",
            "\n",
            "--- 2.813627243041992 seconds ---\n",
            "Epoch: 5\n",
            "49 0.0002 100.0\n",
            "99 0.0021 100.0\n",
            "149 0.0002 100.0\n",
            "199 0.0001 100.0\n",
            "249 0.0043 100.0\n",
            "299 0.0005 100.0\n",
            "349 0.0009 100.0\n",
            "399 0.0008 100.0\n",
            "449 0.0065 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0108, Accuracy: 2766/2947 (93.86%)\n",
            "\n",
            "--- 3.0757362842559814 seconds ---\n",
            "Epoch: 6\n",
            "49 0.0006 100.0\n",
            "99 0.0000 100.0\n",
            "149 0.0003 100.0\n",
            "199 0.0003 100.0\n",
            "249 0.0008 100.0\n",
            "299 0.0028 100.0\n",
            "349 0.0002 100.0\n",
            "399 0.0024 100.0\n",
            "449 0.0004 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0112, Accuracy: 2771/2947 (94.03%)\n",
            "\n",
            "--- 2.5027520656585693 seconds ---\n",
            "Epoch: 7\n",
            "49 0.0001 100.0\n",
            "99 0.0004 100.0\n",
            "149 0.0011 100.0\n",
            "199 0.0017 100.0\n",
            "249 0.0004 100.0\n",
            "299 0.0001 100.0\n",
            "349 0.0001 100.0\n",
            "399 0.0000 100.0\n",
            "449 0.0000 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0110, Accuracy: 2771/2947 (94.03%)\n",
            "\n",
            "--- 2.491633653640747 seconds ---\n",
            "Epoch: 8\n",
            "49 0.0003 100.0\n",
            "99 0.0000 100.0\n",
            "149 0.0003 100.0\n",
            "199 0.0002 100.0\n",
            "249 0.0003 100.0\n",
            "299 0.0000 100.0\n",
            "349 0.0000 100.0\n",
            "399 0.0004 100.0\n",
            "449 0.0000 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0118, Accuracy: 2771/2947 (94.03%)\n",
            "\n",
            "--- 2.46460223197937 seconds ---\n",
            "Epoch: 9\n",
            "49 0.0006 100.0\n",
            "99 0.0004 100.0\n",
            "149 0.0002 100.0\n",
            "199 0.0004 100.0\n",
            "249 0.0002 100.0\n",
            "299 0.0004 100.0\n",
            "349 0.0000 100.0\n",
            "399 0.0009 100.0\n",
            "449 0.0002 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0117, Accuracy: 2774/2947 (94.13%)\n",
            "\n",
            "--- 3.024463415145874 seconds ---\n",
            "Epoch: 10\n",
            "49 0.0000 100.0\n",
            "99 0.0000 100.0\n",
            "149 0.0000 100.0\n",
            "199 0.0000 100.0\n",
            "249 0.0000 100.0\n",
            "299 0.0004 100.0\n",
            "349 0.0000 100.0\n",
            "399 0.0001 100.0\n",
            "449 0.0003 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0125, Accuracy: 2767/2947 (93.89%)\n",
            "\n",
            "--- 2.706324338912964 seconds ---\n"
          ]
        }
      ],
      "source": [
        "#ucihar epoch 10 rffg2vsa\n",
        "class ArgumentParser:\n",
        "    def __init__(self):\n",
        "        self.args = {\n",
        "            'lr': 0.01,            # Learning rate for optimizing class representative\n",
        "            'gamma': 0.3,          # Kernel parameter for computing covariance\n",
        "            'epoch': 10,            # Epochs of training\n",
        "            'gorder': 2,           # Order of the cyclic group required for G-VSA (8 ou 16)\n",
        "            'dim': 10000,          # Dimension of hypervectors c le grand D\n",
        "            'seed': 43,            # Random seed for reproducing results\n",
        "            'resume': True,       # Resume from existing encoded hypervectors\n",
        "            'data_dir': './encoded_data', # Directory to save encoded data (hypervectors)\n",
        "            'dataset': 'ucihar',    # Dataset choice choices=['mnist', 'fmnist', 'cifar', 'isolet', 'ucihar']\n",
        "            'raw_data_dir': './dataset',   # Raw data directory\n",
        "            'model': 'rff-gvsa'    # Model type choices=['rff-hdc', 'linear-hdc', 'rff-gvsa']\n",
        "        }\n",
        "        self.seed = self.args['seed']\n",
        "        self.lr = self.args['lr']\n",
        "        self.gamma = self.args['gamma']\n",
        "        self.epoch = self.args['epoch']\n",
        "        self.gorder = self.args['gorder']\n",
        "        self.dim = self.args['dim']\n",
        "        self.resume = self.args['resume']\n",
        "        self.data_dir = self.args['data_dir']\n",
        "        self.dataset = self.args['dataset']\n",
        "        self.raw_data_dir = self.args['raw_data_dir']\n",
        "        self.model = self.args['model']\n",
        "\n",
        "\n",
        "    def get_args(self):\n",
        "        return self.args\n",
        "args = ArgumentParser()\n",
        "torch.manual_seed(args.seed)\n",
        "np.random.seed(args.seed)\n",
        "if 'hdc' in args.model:\n",
        "    args.gorder = 2\n",
        "    print(\"Use binary HDC with random fourier features, ignoring gorder, set to 2.\")\n",
        "args.data_dir = f'{args.data_dir}/{args.dataset}_{args.model}_order{args.gorder}_gamma{args.gamma}_dim{args.dim}'\n",
        "try:\n",
        "    os.makedirs(args.data_dir)\n",
        "except FileExistsError:\n",
        "    print('Encoded data folder already exists')\n",
        "if not args.resume:\n",
        "    print('Encode the dataset into hypervectors and save')\n",
        "    encode_and_save(args)\n",
        "    print('Finish encoding and saving')\n",
        "print(f'Optimizing class representatives for {args.epoch} epochs')\n",
        "train(args)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MNIST"
      ],
      "metadata": {
        "id": "YDldc1YIQRNu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7386d26e-f2ce-4487-d5dd-d7406cf6b569",
        "id": "hmOpqHobQRNu"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encode the dataset into hypervectors and save\n",
            "# of channels of data 1\n",
            "# of training samples and test samples 60000 10000\n",
            "Encoding with random fourier features encoder.\n",
            "the threshold to discretize fourier features to group elements tensor([0.])\n",
            "Encoded pixels to hypervectors with size:  torch.Size([256, 10000])\n",
            "Encoding training data...\n",
            "Start encoding data\n",
            "12800 images encoded. Total time elapse = 22.91481375694275\n",
            "25600 images encoded. Total time elapse = 45.82794117927551\n",
            "38400 images encoded. Total time elapse = 68.59516954421997\n",
            "51200 images encoded. Total time elapse = 91.45209884643555\n",
            "Finish encoding data\n",
            "Encoding test data...\n",
            "Start encoding data\n",
            "Finish encoding data\n",
            "Finish encoding and saving\n",
            "Optimizing class representatives for 1 epochs\n",
            "Loading encoded training data...\n",
            "Loading encoded test data...\n",
            "Size of encoded training data torch.Size([60000, 10000]) and test data torch.Size([10000, 10000])\n",
            "Epoch: 1\n",
            "49 0.9618 100.0\n",
            "99 0.5312 75.0\n",
            "149 0.1918 93.75\n",
            "199 0.3259 87.5\n",
            "249 0.2240 93.75\n",
            "299 0.3518 87.5\n",
            "349 0.7673 81.25\n",
            "399 0.1703 100.0\n",
            "449 0.7174 87.5\n",
            "499 0.2519 93.75\n",
            "549 0.0492 100.0\n",
            "599 0.1087 100.0\n",
            "649 0.6592 75.0\n",
            "699 0.2424 93.75\n",
            "749 0.1137 100.0\n",
            "799 0.3490 87.5\n",
            "849 0.2614 93.75\n",
            "899 0.1149 93.75\n",
            "949 0.2539 87.5\n",
            "999 0.1170 93.75\n",
            "1049 0.1624 87.5\n",
            "1099 0.0586 100.0\n",
            "1149 0.1169 100.0\n",
            "1199 0.5673 81.25\n",
            "1249 0.1101 93.75\n",
            "1299 0.1388 93.75\n",
            "1349 0.1459 93.75\n",
            "1399 0.0487 100.0\n",
            "1449 0.3380 87.5\n",
            "1499 0.1298 93.75\n",
            "1549 0.0620 100.0\n",
            "1599 0.2228 93.75\n",
            "1649 0.2049 93.75\n",
            "1699 0.4694 87.5\n",
            "1749 0.1330 100.0\n",
            "1799 0.3857 81.25\n",
            "1849 0.4319 87.5\n",
            "1899 0.2447 93.75\n",
            "1949 0.5105 93.75\n",
            "1999 0.3919 87.5\n",
            "2049 0.8131 75.0\n",
            "2099 0.3632 87.5\n",
            "2149 0.0368 100.0\n",
            "2199 0.0880 100.0\n",
            "2249 0.2136 87.5\n",
            "2299 0.0691 100.0\n",
            "2349 0.1096 93.75\n",
            "2399 0.1367 100.0\n",
            "2449 0.2041 93.75\n",
            "2499 0.0209 100.0\n",
            "2549 0.1207 93.75\n",
            "2599 0.0432 100.0\n",
            "2649 0.1262 93.75\n",
            "2699 0.0150 100.0\n",
            "2749 0.3519 93.75\n",
            "2799 0.1616 93.75\n",
            "2849 0.2777 93.75\n",
            "2899 0.1396 100.0\n",
            "2949 0.0117 100.0\n",
            "2999 0.2240 87.5\n",
            "3049 0.2172 93.75\n",
            "3099 0.0188 100.0\n",
            "3149 0.0825 100.0\n",
            "3199 0.1800 93.75\n",
            "3249 0.1685 93.75\n",
            "3299 0.2283 87.5\n",
            "3349 0.3879 87.5\n",
            "3399 0.1103 93.75\n",
            "3449 0.3203 93.75\n",
            "3499 0.2069 93.75\n",
            "3549 0.1056 93.75\n",
            "3599 0.1899 87.5\n",
            "3649 0.2981 93.75\n",
            "3699 0.1487 100.0\n",
            "3749 0.0137 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0151, Accuracy: 9255/10000 (92.55%)\n",
            "\n",
            "--- 17.640981912612915 seconds ---\n"
          ]
        }
      ],
      "source": [
        "#mnist epoch 1 rffg2vsa\n",
        "class ArgumentParser:\n",
        "    def __init__(self):\n",
        "        self.args = {\n",
        "            'lr': 0.01,            # Learning rate for optimizing class representative\n",
        "            'gamma': 0.3,          # Kernel parameter for computing covariance\n",
        "            'epoch': 1,            # Epochs of training\n",
        "            'gorder': 2,           # Order of the cyclic group required for G-VSA (8 ou 16)\n",
        "            'dim': 10000,          # Dimension of hypervectors c le grand D\n",
        "            'seed': 43,            # Random seed for reproducing results\n",
        "            'resume': False,       # Resume from existing encoded hypervectors\n",
        "            'data_dir': './encoded_data', # Directory to save encoded data (hypervectors)\n",
        "            'dataset': 'mnist',    # Dataset choice choices=['mnist', 'fmnist', 'cifar', 'isolet', 'ucihar']\n",
        "            'raw_data_dir': './dataset',   # Raw data directory\n",
        "            'model': 'rff-gvsa'    # Model type choices=['rff-hdc', 'linear-hdc', 'rff-gvsa']\n",
        "        }\n",
        "        self.seed = self.args['seed']\n",
        "        self.lr = self.args['lr']\n",
        "        self.gamma = self.args['gamma']\n",
        "        self.epoch = self.args['epoch']\n",
        "        self.gorder = self.args['gorder']\n",
        "        self.dim = self.args['dim']\n",
        "        self.resume = self.args['resume']\n",
        "        self.data_dir = self.args['data_dir']\n",
        "        self.dataset = self.args['dataset']\n",
        "        self.raw_data_dir = self.args['raw_data_dir']\n",
        "        self.model = self.args['model']\n",
        "\n",
        "\n",
        "    def get_args(self):\n",
        "        return self.args\n",
        "args = ArgumentParser()\n",
        "torch.manual_seed(args.seed)\n",
        "np.random.seed(args.seed)\n",
        "if 'hdc' in args.model:\n",
        "    args.gorder = 2\n",
        "    print(\"Use binary HDC with random fourier features, ignoring gorder, set to 2.\")\n",
        "args.data_dir = f'{args.data_dir}/{args.dataset}_{args.model}_order{args.gorder}_gamma{args.gamma}_dim{args.dim}'\n",
        "try:\n",
        "    os.makedirs(args.data_dir)\n",
        "except FileExistsError:\n",
        "    print('Encoded data folder already exists')\n",
        "if not args.resume:\n",
        "    print('Encode the dataset into hypervectors and save')\n",
        "    encode_and_save(args)\n",
        "    print('Finish encoding and saving')\n",
        "print(f'Optimizing class representatives for {args.epoch} epochs')\n",
        "train(args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59e1ac10-09a9-4f18-fc94-478e62e82f05",
        "id": "jdrDn3jcQRNv"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoded data folder already exists\n",
            "Optimizing class representatives for 10 epochs\n",
            "Loading encoded training data...\n",
            "Loading encoded test data...\n",
            "Size of encoded training data torch.Size([60000, 10000]) and test data torch.Size([10000, 10000])\n",
            "Epoch: 1\n",
            "49 1.5304 62.5\n",
            "99 0.4867 87.5\n",
            "149 0.1419 93.75\n",
            "199 0.1182 100.0\n",
            "249 0.1092 93.75\n",
            "299 0.2597 93.75\n",
            "349 0.5081 81.25\n",
            "399 0.1487 93.75\n",
            "449 0.3419 87.5\n",
            "499 0.2892 81.25\n",
            "549 0.0666 100.0\n",
            "599 0.4737 81.25\n",
            "649 0.1359 93.75\n",
            "699 0.1921 93.75\n",
            "749 0.1802 93.75\n",
            "799 0.1127 100.0\n",
            "849 0.6034 87.5\n",
            "899 0.4002 75.0\n",
            "949 0.1544 93.75\n",
            "999 0.1460 93.75\n",
            "1049 0.2063 87.5\n",
            "1099 0.1659 93.75\n",
            "1149 0.1445 93.75\n",
            "1199 0.0405 100.0\n",
            "1249 0.0243 100.0\n",
            "1299 0.0858 93.75\n",
            "1349 0.0117 100.0\n",
            "1399 0.4560 87.5\n",
            "1449 0.0628 100.0\n",
            "1499 0.3606 81.25\n",
            "1549 0.0364 100.0\n",
            "1599 0.0215 100.0\n",
            "1649 0.0935 93.75\n",
            "1699 0.0750 100.0\n",
            "1749 0.0957 93.75\n",
            "1799 0.2145 87.5\n",
            "1849 0.2135 93.75\n",
            "1899 0.2517 93.75\n",
            "1949 0.2429 93.75\n",
            "1999 0.0327 100.0\n",
            "2049 0.0685 100.0\n",
            "2099 0.4189 93.75\n",
            "2149 0.0745 100.0\n",
            "2199 0.1112 100.0\n",
            "2249 0.4200 93.75\n",
            "2299 0.1400 93.75\n",
            "2349 0.0128 100.0\n",
            "2399 0.0834 93.75\n",
            "2449 0.0662 100.0\n",
            "2499 0.0636 100.0\n",
            "2549 0.2273 93.75\n",
            "2599 0.1861 93.75\n",
            "2649 0.1490 93.75\n",
            "2699 0.0414 100.0\n",
            "2749 0.4017 75.0\n",
            "2799 0.0336 100.0\n",
            "2849 0.1692 93.75\n",
            "2899 0.1269 93.75\n",
            "2949 0.5057 87.5\n",
            "2999 0.1636 93.75\n",
            "3049 0.1694 93.75\n",
            "3099 0.1470 93.75\n",
            "3149 0.0882 100.0\n",
            "3199 0.3768 93.75\n",
            "3249 0.0390 100.0\n",
            "3299 0.1861 93.75\n",
            "3349 0.0967 93.75\n",
            "3399 0.1972 93.75\n",
            "3449 0.1855 93.75\n",
            "3499 0.3956 87.5\n",
            "3549 0.1253 100.0\n",
            "3599 0.1667 87.5\n",
            "3649 0.5505 87.5\n",
            "3699 0.3593 93.75\n",
            "3749 0.0427 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0142, Accuracy: 9298/10000 (92.98%)\n",
            "\n",
            "--- 17.591790914535522 seconds ---\n",
            "Epoch: 2\n",
            "49 0.2352 93.75\n",
            "99 0.2413 93.75\n",
            "149 0.0680 100.0\n",
            "199 0.3704 81.25\n",
            "249 0.0353 100.0\n",
            "299 0.0539 100.0\n",
            "349 0.0298 100.0\n",
            "399 0.1454 93.75\n",
            "449 0.0185 100.0\n",
            "499 0.0286 100.0\n",
            "549 0.0652 100.0\n",
            "599 0.4249 87.5\n",
            "649 0.0143 100.0\n",
            "699 0.0598 100.0\n",
            "749 0.2039 93.75\n",
            "799 0.0272 100.0\n",
            "849 0.0196 100.0\n",
            "899 0.0014 100.0\n",
            "949 0.0119 100.0\n",
            "999 0.0057 100.0\n",
            "1049 0.0040 100.0\n",
            "1099 0.0392 100.0\n",
            "1149 0.3036 93.75\n",
            "1199 0.0231 100.0\n",
            "1249 0.1280 93.75\n",
            "1299 0.0609 93.75\n",
            "1349 0.0223 100.0\n",
            "1399 0.1604 93.75\n",
            "1449 0.5509 87.5\n",
            "1499 0.0075 100.0\n",
            "1549 0.0364 100.0\n",
            "1599 0.2523 87.5\n",
            "1649 0.1569 93.75\n",
            "1699 0.1430 93.75\n",
            "1749 0.0055 100.0\n",
            "1799 0.0297 100.0\n",
            "1849 0.0459 100.0\n",
            "1899 0.0222 100.0\n",
            "1949 0.0379 100.0\n",
            "1999 0.0667 93.75\n",
            "2049 0.1775 93.75\n",
            "2099 0.1415 87.5\n",
            "2149 0.3820 93.75\n",
            "2199 0.0089 100.0\n",
            "2249 0.0459 100.0\n",
            "2299 0.1175 93.75\n",
            "2349 0.0903 100.0\n",
            "2399 0.2721 93.75\n",
            "2449 0.2371 93.75\n",
            "2499 0.1352 87.5\n",
            "2549 0.0927 93.75\n",
            "2599 0.0734 100.0\n",
            "2649 0.3212 93.75\n",
            "2699 0.0517 100.0\n",
            "2749 0.1593 87.5\n",
            "2799 0.1811 87.5\n",
            "2849 0.1818 87.5\n",
            "2899 0.1898 93.75\n",
            "2949 0.1651 93.75\n",
            "2999 0.0136 100.0\n",
            "3049 0.1269 93.75\n",
            "3099 0.0536 100.0\n",
            "3149 0.0269 100.0\n",
            "3199 0.7236 93.75\n",
            "3249 0.2600 87.5\n",
            "3299 0.2404 87.5\n",
            "3349 0.0023 100.0\n",
            "3399 0.3638 81.25\n",
            "3449 0.2563 87.5\n",
            "3499 0.1461 93.75\n",
            "3549 0.0923 100.0\n",
            "3599 0.0081 100.0\n",
            "3649 0.0008 100.0\n",
            "3699 0.0076 100.0\n",
            "3749 0.0446 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0162, Accuracy: 9249/10000 (92.49%)\n",
            "\n",
            "--- 17.002947330474854 seconds ---\n",
            "Epoch: 3\n",
            "49 0.4066 93.75\n",
            "99 0.0482 100.0\n",
            "149 0.0260 100.0\n",
            "199 0.0689 93.75\n",
            "249 0.0443 100.0\n",
            "299 0.0374 100.0\n",
            "349 0.0589 93.75\n",
            "399 0.0309 100.0\n",
            "449 0.0087 100.0\n",
            "499 0.1442 93.75\n",
            "549 0.1775 93.75\n",
            "599 0.0301 100.0\n",
            "649 0.0577 100.0\n",
            "699 0.1608 93.75\n",
            "749 0.1074 93.75\n",
            "799 0.0142 100.0\n",
            "849 0.2610 93.75\n",
            "899 0.0406 100.0\n",
            "949 0.0225 100.0\n",
            "999 0.2942 87.5\n",
            "1049 0.0212 100.0\n",
            "1099 0.5383 93.75\n",
            "1149 0.1823 87.5\n",
            "1199 0.0090 100.0\n",
            "1249 0.4318 87.5\n",
            "1299 0.0342 100.0\n",
            "1349 0.0658 100.0\n",
            "1399 0.3335 87.5\n",
            "1449 0.0505 100.0\n",
            "1499 0.0665 100.0\n",
            "1549 0.2231 93.75\n",
            "1599 0.0903 93.75\n",
            "1649 0.1508 93.75\n",
            "1699 0.0272 100.0\n",
            "1749 0.0917 93.75\n",
            "1799 0.0729 100.0\n",
            "1849 0.2859 93.75\n",
            "1899 0.0063 100.0\n",
            "1949 0.0388 100.0\n",
            "1999 0.4137 87.5\n",
            "2049 0.0230 100.0\n",
            "2099 0.1428 87.5\n",
            "2149 0.1156 100.0\n",
            "2199 0.1046 100.0\n",
            "2249 0.0030 100.0\n",
            "2299 0.0197 100.0\n",
            "2349 0.1374 93.75\n",
            "2399 0.0599 100.0\n",
            "2449 0.0023 100.0\n",
            "2499 0.0111 100.0\n",
            "2549 0.3483 87.5\n",
            "2599 0.5101 81.25\n",
            "2649 0.0853 100.0\n",
            "2699 0.0326 100.0\n",
            "2749 0.0355 100.0\n",
            "2799 0.0376 100.0\n",
            "2849 0.0246 100.0\n",
            "2899 0.0123 100.0\n",
            "2949 0.0499 100.0\n",
            "2999 0.6808 81.25\n",
            "3049 0.1937 87.5\n",
            "3099 0.3134 87.5\n",
            "3149 0.0340 100.0\n",
            "3199 0.1627 87.5\n",
            "3249 0.0546 100.0\n",
            "3299 0.2353 93.75\n",
            "3349 0.0592 100.0\n",
            "3399 0.3183 87.5\n",
            "3449 0.0832 93.75\n",
            "3499 0.0063 100.0\n",
            "3549 0.0624 100.0\n",
            "3599 0.1199 93.75\n",
            "3649 0.0163 100.0\n",
            "3699 0.0177 100.0\n",
            "3749 0.1766 93.75\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0171, Accuracy: 9182/10000 (91.82%)\n",
            "\n",
            "--- 16.905325651168823 seconds ---\n",
            "Epoch: 4\n",
            "49 0.0826 93.75\n",
            "99 0.0034 100.0\n",
            "149 0.0050 100.0\n",
            "199 0.0759 93.75\n",
            "249 0.0663 100.0\n",
            "299 0.2075 93.75\n",
            "349 0.0581 100.0\n",
            "399 0.0707 93.75\n",
            "449 0.1707 93.75\n",
            "499 0.0204 100.0\n",
            "549 0.2247 93.75\n",
            "599 0.0784 100.0\n",
            "649 0.1494 93.75\n",
            "699 0.0062 100.0\n",
            "749 0.1792 87.5\n",
            "799 0.0066 100.0\n",
            "849 0.4851 93.75\n",
            "899 0.1905 87.5\n",
            "949 0.2028 93.75\n",
            "999 0.0141 100.0\n",
            "1049 0.1645 87.5\n",
            "1099 0.1005 100.0\n",
            "1149 0.1093 93.75\n",
            "1199 0.1035 93.75\n",
            "1249 0.0287 100.0\n",
            "1299 0.2708 93.75\n",
            "1349 0.0393 100.0\n",
            "1399 0.2464 93.75\n",
            "1449 0.0847 93.75\n",
            "1499 0.0172 100.0\n",
            "1549 0.0444 100.0\n",
            "1599 0.0307 100.0\n",
            "1649 0.0420 100.0\n",
            "1699 0.0699 100.0\n",
            "1749 0.7753 87.5\n",
            "1799 0.1393 100.0\n",
            "1849 0.0194 100.0\n",
            "1899 0.0726 93.75\n",
            "1949 0.1422 93.75\n",
            "1999 0.0597 100.0\n",
            "2049 0.0103 100.0\n",
            "2099 0.6462 93.75\n",
            "2149 0.0373 100.0\n",
            "2199 0.0807 100.0\n",
            "2249 0.0521 100.0\n",
            "2299 0.0180 100.0\n",
            "2349 0.6557 87.5\n",
            "2399 0.3335 93.75\n",
            "2449 0.3154 81.25\n",
            "2499 0.3683 93.75\n",
            "2549 0.0988 93.75\n",
            "2599 0.4335 87.5\n",
            "2649 0.0933 100.0\n",
            "2699 0.2211 93.75\n",
            "2749 0.1435 93.75\n",
            "2799 0.7270 87.5\n",
            "2849 0.0850 100.0\n",
            "2899 0.1025 93.75\n",
            "2949 0.3997 93.75\n",
            "2999 0.0599 100.0\n",
            "3049 0.0947 93.75\n",
            "3099 0.6423 93.75\n",
            "3149 0.3043 87.5\n",
            "3199 0.0233 100.0\n",
            "3249 0.1490 93.75\n",
            "3299 0.2843 81.25\n",
            "3349 0.0805 100.0\n",
            "3399 0.0051 100.0\n",
            "3449 0.0218 100.0\n",
            "3499 0.2326 87.5\n",
            "3549 0.0846 93.75\n",
            "3599 0.1337 93.75\n",
            "3649 0.0362 100.0\n",
            "3699 0.4117 93.75\n",
            "3749 0.2469 93.75\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0163, Accuracy: 9213/10000 (92.13%)\n",
            "\n",
            "--- 18.083059310913086 seconds ---\n",
            "Epoch: 5\n",
            "49 0.3486 87.5\n",
            "99 0.0108 100.0\n",
            "149 0.0402 100.0\n",
            "199 0.0241 100.0\n",
            "249 0.0081 100.0\n",
            "299 0.0915 93.75\n",
            "349 0.2263 93.75\n",
            "399 0.0891 93.75\n",
            "449 0.0264 100.0\n",
            "499 0.1732 87.5\n",
            "549 0.0112 100.0\n",
            "599 0.2305 87.5\n",
            "649 0.0159 100.0\n",
            "699 0.1967 93.75\n",
            "749 0.3212 81.25\n",
            "799 0.0385 100.0\n",
            "849 0.1507 93.75\n",
            "899 0.2571 87.5\n",
            "949 0.1021 93.75\n",
            "999 0.1645 93.75\n",
            "1049 0.0877 93.75\n",
            "1099 0.0465 100.0\n",
            "1149 0.0232 100.0\n",
            "1199 0.0691 100.0\n",
            "1249 0.4196 81.25\n",
            "1299 0.0895 93.75\n",
            "1349 0.4739 81.25\n",
            "1399 0.0253 100.0\n",
            "1449 0.0364 100.0\n",
            "1499 0.1587 93.75\n",
            "1549 0.0768 93.75\n",
            "1599 0.0326 100.0\n",
            "1649 0.0173 100.0\n",
            "1699 0.0085 100.0\n",
            "1749 0.5059 81.25\n",
            "1799 0.0442 100.0\n",
            "1849 0.0065 100.0\n",
            "1899 0.0069 100.0\n",
            "1949 0.2617 93.75\n",
            "1999 0.4343 81.25\n",
            "2049 0.0192 100.0\n",
            "2099 0.0248 100.0\n",
            "2149 0.3014 93.75\n",
            "2199 0.0033 100.0\n",
            "2249 0.1501 93.75\n",
            "2299 0.0823 100.0\n",
            "2349 0.0371 100.0\n",
            "2399 0.0391 100.0\n",
            "2449 0.2955 87.5\n",
            "2499 0.0136 100.0\n",
            "2549 0.1692 93.75\n",
            "2599 0.2323 87.5\n",
            "2649 0.0723 100.0\n",
            "2699 0.0355 100.0\n",
            "2749 0.1692 93.75\n",
            "2799 0.0180 100.0\n",
            "2849 0.4229 87.5\n",
            "2899 0.7245 87.5\n",
            "2949 0.2628 87.5\n",
            "2999 0.0666 100.0\n",
            "3049 0.2501 93.75\n",
            "3099 0.1818 93.75\n",
            "3149 0.3505 93.75\n",
            "3199 0.0387 100.0\n",
            "3249 0.1435 93.75\n",
            "3299 0.0626 100.0\n",
            "3349 0.0718 100.0\n",
            "3399 0.4862 81.25\n",
            "3449 0.0176 100.0\n",
            "3499 0.1936 93.75\n",
            "3549 0.0572 100.0\n",
            "3599 0.1283 93.75\n",
            "3649 0.0883 93.75\n",
            "3699 0.1450 87.5\n",
            "3749 0.2492 93.75\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0170, Accuracy: 9176/10000 (91.76%)\n",
            "\n",
            "--- 17.03489375114441 seconds ---\n",
            "Epoch: 6\n",
            "49 0.2243 87.5\n",
            "99 0.0076 100.0\n",
            "149 0.0250 100.0\n",
            "199 0.0429 100.0\n",
            "249 0.0759 100.0\n",
            "299 0.0182 100.0\n",
            "349 0.4658 75.0\n",
            "399 0.0471 100.0\n",
            "449 0.0653 100.0\n",
            "499 0.1905 87.5\n",
            "549 0.1377 93.75\n",
            "599 0.0142 100.0\n",
            "649 0.2180 93.75\n",
            "699 0.2583 87.5\n",
            "749 0.1004 93.75\n",
            "799 0.0193 100.0\n",
            "849 0.0069 100.0\n",
            "899 0.0552 100.0\n",
            "949 0.0808 100.0\n",
            "999 0.0263 100.0\n",
            "1049 0.0243 100.0\n",
            "1099 0.0009 100.0\n",
            "1149 0.2195 87.5\n",
            "1199 0.0053 100.0\n",
            "1249 0.0389 100.0\n",
            "1299 0.1054 93.75\n",
            "1349 0.0782 100.0\n",
            "1399 0.3068 93.75\n",
            "1449 0.2438 87.5\n",
            "1499 0.0228 100.0\n",
            "1549 0.1441 93.75\n",
            "1599 0.0342 100.0\n",
            "1649 0.0757 100.0\n",
            "1699 0.1018 93.75\n",
            "1749 0.0899 93.75\n",
            "1799 0.0165 100.0\n",
            "1849 0.3638 93.75\n",
            "1899 0.6405 81.25\n",
            "1949 0.0678 100.0\n",
            "1999 0.0281 100.0\n",
            "2049 0.0055 100.0\n",
            "2099 0.0127 100.0\n",
            "2149 0.0304 100.0\n",
            "2199 0.0051 100.0\n",
            "2249 0.0369 100.0\n",
            "2299 0.0972 100.0\n",
            "2349 0.0517 100.0\n",
            "2399 0.4148 87.5\n",
            "2449 0.1051 93.75\n",
            "2499 0.0623 100.0\n",
            "2549 0.2573 93.75\n",
            "2599 0.1387 93.75\n",
            "2649 0.1066 100.0\n",
            "2699 0.3004 87.5\n",
            "2749 0.0648 100.0\n",
            "2799 0.0624 93.75\n",
            "2849 0.2448 93.75\n",
            "2899 0.2657 93.75\n",
            "2949 0.1259 100.0\n",
            "2999 0.0511 100.0\n",
            "3049 0.0114 100.0\n",
            "3099 0.2764 93.75\n",
            "3149 0.2142 87.5\n",
            "3199 0.0383 100.0\n",
            "3249 0.2787 93.75\n",
            "3299 0.4388 93.75\n",
            "3349 0.1621 87.5\n",
            "3399 0.0279 100.0\n",
            "3449 0.1255 93.75\n",
            "3499 0.0104 100.0\n",
            "3549 0.1261 93.75\n",
            "3599 0.4735 93.75\n",
            "3649 0.3694 87.5\n",
            "3699 0.0929 100.0\n",
            "3749 0.0382 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0169, Accuracy: 9172/10000 (91.72%)\n",
            "\n",
            "--- 17.076942443847656 seconds ---\n",
            "Epoch: 7\n",
            "49 0.0985 100.0\n",
            "99 0.0211 100.0\n",
            "149 0.1131 93.75\n",
            "199 0.0952 93.75\n",
            "249 0.0157 100.0\n",
            "299 0.0224 100.0\n",
            "349 0.0784 100.0\n",
            "399 0.0849 100.0\n",
            "449 0.0078 100.0\n",
            "499 0.0250 100.0\n",
            "549 0.3561 87.5\n",
            "599 0.0025 100.0\n",
            "649 0.1588 93.75\n",
            "699 0.0226 100.0\n",
            "749 0.0196 100.0\n",
            "799 0.2139 87.5\n",
            "849 0.1732 87.5\n",
            "899 0.0316 100.0\n",
            "949 0.1959 93.75\n",
            "999 0.1787 87.5\n",
            "1049 0.0605 100.0\n",
            "1099 0.0057 100.0\n",
            "1149 0.0746 100.0\n",
            "1199 0.1331 93.75\n",
            "1249 0.0536 100.0\n",
            "1299 0.0269 100.0\n",
            "1349 0.1045 93.75\n",
            "1399 0.1496 93.75\n",
            "1449 0.3912 93.75\n",
            "1499 0.0418 100.0\n",
            "1549 0.1264 93.75\n",
            "1599 0.1852 87.5\n",
            "1649 0.0036 100.0\n",
            "1699 0.0074 100.0\n",
            "1749 0.3396 87.5\n",
            "1799 0.1199 93.75\n",
            "1849 0.0437 100.0\n",
            "1899 0.1390 93.75\n",
            "1949 0.5309 87.5\n",
            "1999 0.0471 100.0\n",
            "2049 0.0486 100.0\n",
            "2099 0.0149 100.0\n",
            "2149 0.0881 100.0\n",
            "2199 0.0892 93.75\n",
            "2249 0.1179 100.0\n",
            "2299 0.0021 100.0\n",
            "2349 0.1353 93.75\n",
            "2399 0.2042 93.75\n",
            "2449 0.0550 100.0\n",
            "2499 0.2830 93.75\n",
            "2549 0.1861 87.5\n",
            "2599 0.0160 100.0\n",
            "2649 0.0517 100.0\n",
            "2699 0.0442 100.0\n",
            "2749 0.7669 93.75\n",
            "2799 0.2025 93.75\n",
            "2849 0.1370 93.75\n",
            "2899 0.0128 100.0\n",
            "2949 0.0256 100.0\n",
            "2999 0.1095 93.75\n",
            "3049 0.0039 100.0\n",
            "3099 0.1018 93.75\n",
            "3149 0.4802 81.25\n",
            "3199 0.0951 93.75\n",
            "3249 0.1531 93.75\n",
            "3299 0.0104 100.0\n",
            "3349 0.2735 87.5\n",
            "3399 0.0476 100.0\n",
            "3449 0.5201 81.25\n",
            "3499 0.0452 100.0\n",
            "3549 0.0204 100.0\n",
            "3599 0.1423 93.75\n",
            "3649 0.0898 100.0\n",
            "3699 0.4029 81.25\n",
            "3749 0.0528 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0174, Accuracy: 9154/10000 (91.54%)\n",
            "\n",
            "--- 17.994259357452393 seconds ---\n",
            "Epoch: 8\n",
            "49 0.2408 87.5\n",
            "99 0.0367 100.0\n",
            "149 0.0216 100.0\n",
            "199 0.1088 93.75\n",
            "249 0.1932 93.75\n",
            "299 0.3103 93.75\n",
            "349 0.0488 100.0\n",
            "399 0.1836 87.5\n",
            "449 0.0443 100.0\n",
            "499 0.0285 100.0\n",
            "549 0.0028 100.0\n",
            "599 0.0079 100.0\n",
            "649 0.0431 100.0\n",
            "699 0.0604 100.0\n",
            "749 0.0115 100.0\n",
            "799 0.0687 100.0\n",
            "849 0.0101 100.0\n",
            "899 0.0365 100.0\n",
            "949 0.0117 100.0\n",
            "999 0.0252 100.0\n",
            "1049 0.4012 87.5\n",
            "1099 0.0201 100.0\n",
            "1149 0.0236 100.0\n",
            "1199 0.0959 93.75\n",
            "1249 0.0988 93.75\n",
            "1299 0.0081 100.0\n",
            "1349 0.0750 100.0\n",
            "1399 0.4926 93.75\n",
            "1449 0.2129 93.75\n",
            "1499 0.0051 100.0\n",
            "1549 0.2046 87.5\n",
            "1599 0.1112 100.0\n",
            "1649 0.0105 100.0\n",
            "1699 0.0400 100.0\n",
            "1749 0.0510 93.75\n",
            "1799 0.1673 93.75\n",
            "1849 0.2201 87.5\n",
            "1899 0.1979 87.5\n",
            "1949 0.1083 93.75\n",
            "1999 0.0161 100.0\n",
            "2049 0.0127 100.0\n",
            "2099 0.1923 93.75\n",
            "2149 0.1644 93.75\n",
            "2199 0.0079 100.0\n",
            "2249 0.3467 93.75\n",
            "2299 0.0417 100.0\n",
            "2349 0.2314 87.5\n",
            "2399 0.0266 100.0\n",
            "2449 0.0853 93.75\n",
            "2499 0.2304 93.75\n",
            "2549 0.1946 87.5\n",
            "2599 0.3040 93.75\n",
            "2649 0.1235 93.75\n",
            "2699 0.2254 93.75\n",
            "2749 0.3717 81.25\n",
            "2799 0.1223 87.5\n",
            "2849 0.0244 100.0\n",
            "2899 0.0491 100.0\n",
            "2949 0.1518 100.0\n",
            "2999 0.3784 93.75\n",
            "3049 0.0205 100.0\n",
            "3099 0.0625 100.0\n",
            "3149 0.2191 93.75\n",
            "3199 0.1126 93.75\n",
            "3249 0.0691 100.0\n",
            "3299 0.0025 100.0\n",
            "3349 0.0206 100.0\n",
            "3399 0.1883 87.5\n",
            "3449 0.2028 87.5\n",
            "3499 0.0561 100.0\n",
            "3549 0.0896 100.0\n",
            "3599 0.0329 100.0\n",
            "3649 0.2475 93.75\n",
            "3699 0.1005 93.75\n",
            "3749 0.0728 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0173, Accuracy: 9176/10000 (91.76%)\n",
            "\n",
            "--- 16.991377115249634 seconds ---\n",
            "Epoch: 9\n",
            "49 0.0346 100.0\n",
            "99 0.0124 100.0\n",
            "149 0.2292 93.75\n",
            "199 0.0254 100.0\n",
            "249 0.0222 100.0\n",
            "299 0.0149 100.0\n",
            "349 0.0600 93.75\n",
            "399 0.6949 87.5\n",
            "449 0.0738 100.0\n",
            "499 0.1713 93.75\n",
            "549 0.0144 100.0\n",
            "599 0.0836 93.75\n",
            "649 0.0575 100.0\n",
            "699 0.0049 100.0\n",
            "749 0.0840 93.75\n",
            "799 0.0185 100.0\n",
            "849 0.1303 93.75\n",
            "899 0.0037 100.0\n",
            "949 0.1216 93.75\n",
            "999 0.2814 81.25\n",
            "1049 0.1868 93.75\n",
            "1099 0.0338 100.0\n",
            "1149 0.0507 100.0\n",
            "1199 0.0119 100.0\n",
            "1249 0.1563 87.5\n",
            "1299 0.1029 93.75\n",
            "1349 0.0380 100.0\n",
            "1399 0.0652 100.0\n",
            "1449 0.0400 100.0\n",
            "1499 0.1027 93.75\n",
            "1549 0.0042 100.0\n",
            "1599 0.2886 87.5\n",
            "1649 0.1145 93.75\n",
            "1699 0.0587 93.75\n",
            "1749 0.5525 87.5\n",
            "1799 0.1395 93.75\n",
            "1849 0.0454 100.0\n",
            "1899 0.3127 93.75\n",
            "1949 0.2932 87.5\n",
            "1999 0.1394 93.75\n",
            "2049 0.0972 93.75\n",
            "2099 0.0390 100.0\n",
            "2149 0.0161 100.0\n",
            "2199 0.0182 100.0\n",
            "2249 0.1501 93.75\n",
            "2299 0.0521 100.0\n",
            "2349 0.4247 93.75\n",
            "2399 0.2484 93.75\n",
            "2449 0.0202 100.0\n",
            "2499 0.2909 93.75\n",
            "2549 0.1117 100.0\n",
            "2599 0.0415 100.0\n",
            "2649 0.0659 100.0\n",
            "2699 0.5425 93.75\n",
            "2749 0.5343 81.25\n",
            "2799 0.0749 100.0\n",
            "2849 0.0760 100.0\n",
            "2899 0.1355 93.75\n",
            "2949 0.0337 100.0\n",
            "2999 0.1737 93.75\n",
            "3049 0.0274 100.0\n",
            "3099 0.1252 93.75\n",
            "3149 0.1914 93.75\n",
            "3199 0.9127 75.0\n",
            "3249 0.1588 100.0\n",
            "3299 0.2262 93.75\n",
            "3349 0.8581 87.5\n",
            "3399 0.2623 93.75\n",
            "3449 0.3178 87.5\n",
            "3499 0.0453 100.0\n",
            "3549 0.4717 81.25\n",
            "3599 0.6391 81.25\n",
            "3649 0.0997 93.75\n",
            "3699 0.1010 100.0\n",
            "3749 0.0110 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0181, Accuracy: 9127/10000 (91.27%)\n",
            "\n",
            "--- 17.053439617156982 seconds ---\n",
            "Epoch: 10\n",
            "49 0.0430 100.0\n",
            "99 0.0609 100.0\n",
            "149 0.0526 100.0\n",
            "199 0.0202 100.0\n",
            "249 0.2259 93.75\n",
            "299 0.1583 93.75\n",
            "349 0.1274 93.75\n",
            "399 0.0068 100.0\n",
            "449 0.0067 100.0\n",
            "499 0.0911 93.75\n",
            "549 0.0192 100.0\n",
            "599 0.0237 100.0\n",
            "649 0.1175 93.75\n",
            "699 0.0046 100.0\n",
            "749 0.0046 100.0\n",
            "799 0.1830 87.5\n",
            "849 0.4409 87.5\n",
            "899 0.1465 93.75\n",
            "949 0.0703 100.0\n",
            "999 0.1840 87.5\n",
            "1049 0.0266 100.0\n",
            "1099 0.1016 93.75\n",
            "1149 0.1196 93.75\n",
            "1199 0.0731 100.0\n",
            "1249 0.3849 87.5\n",
            "1299 0.2212 93.75\n",
            "1349 0.0261 100.0\n",
            "1399 0.0213 100.0\n",
            "1449 0.2893 87.5\n",
            "1499 0.0506 100.0\n",
            "1549 0.1547 93.75\n",
            "1599 0.0545 100.0\n",
            "1649 0.0938 93.75\n",
            "1699 0.1412 93.75\n",
            "1749 0.2253 87.5\n",
            "1799 0.0848 93.75\n",
            "1849 0.0598 100.0\n",
            "1899 0.2189 93.75\n",
            "1949 0.2011 93.75\n",
            "1999 0.0104 100.0\n",
            "2049 0.0235 100.0\n",
            "2099 0.0123 100.0\n",
            "2149 0.1900 87.5\n",
            "2199 0.0496 100.0\n",
            "2249 0.0202 100.0\n",
            "2299 0.0386 100.0\n",
            "2349 0.0839 93.75\n",
            "2399 0.0367 100.0\n",
            "2449 0.1089 93.75\n",
            "2499 0.0671 100.0\n",
            "2549 0.0607 100.0\n",
            "2599 0.0730 93.75\n",
            "2649 0.0829 100.0\n",
            "2699 0.1431 93.75\n",
            "2749 0.0069 100.0\n",
            "2799 0.1605 93.75\n",
            "2849 0.4450 87.5\n",
            "2899 0.0392 100.0\n",
            "2949 0.0131 100.0\n",
            "2999 0.1297 93.75\n",
            "3049 0.2078 93.75\n",
            "3099 0.1364 100.0\n",
            "3149 0.0501 100.0\n",
            "3199 0.0184 100.0\n",
            "3249 0.2810 93.75\n",
            "3299 0.1119 93.75\n",
            "3349 0.0792 100.0\n",
            "3399 0.3282 87.5\n",
            "3449 0.2593 93.75\n",
            "3499 0.1300 93.75\n",
            "3549 0.0216 100.0\n",
            "3599 0.0434 100.0\n",
            "3649 0.0490 100.0\n",
            "3699 0.1064 87.5\n",
            "3749 0.2308 93.75\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0188, Accuracy: 9115/10000 (91.15%)\n",
            "\n",
            "--- 17.80075240135193 seconds ---\n"
          ]
        }
      ],
      "source": [
        "#mnist epoch 10 rffg2vsa\n",
        "class ArgumentParser:\n",
        "    def __init__(self):\n",
        "        self.args = {\n",
        "            'lr': 0.01,            # Learning rate for optimizing class representative\n",
        "            'gamma': 0.3,          # Kernel parameter for computing covariance\n",
        "            'epoch': 10,            # Epochs of training\n",
        "            'gorder': 2,           # Order of the cyclic group required for G-VSA (8 ou 16)\n",
        "            'dim': 10000,          # Dimension of hypervectors c le grand D\n",
        "            'seed': 43,            # Random seed for reproducing results\n",
        "            'resume': True,       # Resume from existing encoded hypervectors\n",
        "            'data_dir': './encoded_data', # Directory to save encoded data (hypervectors)\n",
        "            'dataset': 'mnist',    # Dataset choice choices=['mnist', 'fmnist', 'cifar', 'isolet', 'ucihar']\n",
        "            'raw_data_dir': './dataset',   # Raw data directory\n",
        "            'model': 'rff-gvsa'    # Model type choices=['rff-hdc', 'linear-hdc', 'rff-gvsa']\n",
        "        }\n",
        "        self.seed = self.args['seed']\n",
        "        self.lr = self.args['lr']\n",
        "        self.gamma = self.args['gamma']\n",
        "        self.epoch = self.args['epoch']\n",
        "        self.gorder = self.args['gorder']\n",
        "        self.dim = self.args['dim']\n",
        "        self.resume = self.args['resume']\n",
        "        self.data_dir = self.args['data_dir']\n",
        "        self.dataset = self.args['dataset']\n",
        "        self.raw_data_dir = self.args['raw_data_dir']\n",
        "        self.model = self.args['model']\n",
        "\n",
        "\n",
        "    def get_args(self):\n",
        "        return self.args\n",
        "args = ArgumentParser()\n",
        "torch.manual_seed(args.seed)\n",
        "np.random.seed(args.seed)\n",
        "if 'hdc' in args.model:\n",
        "    args.gorder = 2\n",
        "    print(\"Use binary HDC with random fourier features, ignoring gorder, set to 2.\")\n",
        "args.data_dir = f'{args.data_dir}/{args.dataset}_{args.model}_order{args.gorder}_gamma{args.gamma}_dim{args.dim}'\n",
        "try:\n",
        "    os.makedirs(args.data_dir)\n",
        "except FileExistsError:\n",
        "    print('Encoded data folder already exists')\n",
        "if not args.resume:\n",
        "    print('Encode the dataset into hypervectors and save')\n",
        "    encode_and_save(args)\n",
        "    print('Finish encoding and saving')\n",
        "print(f'Optimizing class representatives for {args.epoch} epochs')\n",
        "train(args)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FMNIST"
      ],
      "metadata": {
        "id": "Mr_FrqzGQRNw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#fminst epoch 1 rffg2vsa\n",
        "class ArgumentParser:\n",
        "    def __init__(self):\n",
        "        self.args = {\n",
        "            'lr': 0.01,            # Learning rate for optimizing class representative\n",
        "            'gamma': 0.3,          # Kernel parameter for computing covariance\n",
        "            'epoch': 1,            # Epochs of training\n",
        "            'gorder': 2,           # Order of the cyclic group required for G-VSA (8 ou 16)\n",
        "            'dim': 10000,          # Dimension of hypervectors c le grand D\n",
        "            'seed': 43,            # Random seed for reproducing results\n",
        "            'resume': False,       # Resume from existing encoded hypervectors\n",
        "            'data_dir': './encoded_data', # Directory to save encoded data (hypervectors)\n",
        "            'dataset': 'fmnist',    # Dataset choice choices=['mnist', 'fmnist', 'cifar', 'isolet', 'ucihar']\n",
        "            'raw_data_dir': './dataset',   # Raw data directory\n",
        "            'model': 'rff-gvsa'    # Model type choices=['rff-hdc', 'linear-hdc', 'rff-gvsa']\n",
        "        }\n",
        "        self.seed = self.args['seed']\n",
        "        self.lr = self.args['lr']\n",
        "        self.gamma = self.args['gamma']\n",
        "        self.epoch = self.args['epoch']\n",
        "        self.gorder = self.args['gorder']\n",
        "        self.dim = self.args['dim']\n",
        "        self.resume = self.args['resume']\n",
        "        self.data_dir = self.args['data_dir']\n",
        "        self.dataset = self.args['dataset']\n",
        "        self.raw_data_dir = self.args['raw_data_dir']\n",
        "        self.model = self.args['model']\n",
        "\n",
        "\n",
        "    def get_args(self):\n",
        "        return self.args\n",
        "\n",
        "args = ArgumentParser()\n",
        "torch.manual_seed(args.seed)\n",
        "np.random.seed(args.seed)\n",
        "if 'hdc' in args.model:\n",
        "    args.gorder = 2\n",
        "    print(\"Use binary HDC with random fourier features, ignoring gorder, set to 2.\")\n",
        "args.data_dir = f'{args.data_dir}/{args.dataset}_{args.model}_order{args.gorder}_gamma{args.gamma}_dim{args.dim}'\n",
        "try:\n",
        "    os.makedirs(args.data_dir)\n",
        "except FileExistsError:\n",
        "    print('Encoded data folder already exists')\n",
        "if not args.resume:\n",
        "    print('Encode the dataset into hypervectors and save')\n",
        "    encode_and_save(args)\n",
        "    print('Finish encoding and saving')\n",
        "print(f'Optimizing class representatives for {args.epoch} epochs')\n",
        "train(args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1900bde3-ae98-4f67-9297-9852b70c9699",
        "id": "_7VIOOv8QRNw"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encode the dataset into hypervectors and save\n",
            "# of channels of data 1\n",
            "# of training samples and test samples 60000 10000\n",
            "Encoding with random fourier features encoder.\n",
            "the threshold to discretize fourier features to group elements tensor([0.])\n",
            "Encoded pixels to hypervectors with size:  torch.Size([256, 10000])\n",
            "Encoding training data...\n",
            "Start encoding data\n",
            "12800 images encoded. Total time elapse = 26.832853317260742\n",
            "25600 images encoded. Total time elapse = 53.6350998878479\n",
            "38400 images encoded. Total time elapse = 80.54456806182861\n",
            "51200 images encoded. Total time elapse = 107.44296073913574\n",
            "Finish encoding data\n",
            "Encoding test data...\n",
            "Start encoding data\n",
            "Finish encoding data\n",
            "Finish encoding and saving\n",
            "Optimizing class representatives for 1 epochs\n",
            "Loading encoded training data...\n",
            "Loading encoded test data...\n",
            "Size of encoded training data torch.Size([60000, 10000]) and test data torch.Size([10000, 10000])\n",
            "Epoch: 1\n",
            "49 1.1921 81.25\n",
            "99 1.1890 62.5\n",
            "149 1.0346 56.25\n",
            "199 0.2096 93.75\n",
            "249 0.2326 93.75\n",
            "299 0.8049 56.25\n",
            "349 1.1779 68.75\n",
            "399 0.5819 75.0\n",
            "449 0.2708 93.75\n",
            "499 0.8006 68.75\n",
            "549 0.4437 93.75\n",
            "599 0.3135 93.75\n",
            "649 0.4785 87.5\n",
            "699 0.1760 93.75\n",
            "749 0.0864 100.0\n",
            "799 0.3991 87.5\n",
            "849 0.2818 93.75\n",
            "899 0.3491 93.75\n",
            "949 0.6995 81.25\n",
            "999 0.7765 68.75\n",
            "1049 0.6042 87.5\n",
            "1099 0.3935 81.25\n",
            "1149 0.6911 75.0\n",
            "1199 0.7225 81.25\n",
            "1249 0.4728 93.75\n",
            "1299 0.3868 93.75\n",
            "1349 0.1187 100.0\n",
            "1399 0.7157 81.25\n",
            "1449 0.8049 75.0\n",
            "1499 0.1897 87.5\n",
            "1549 0.6259 75.0\n",
            "1599 0.3389 87.5\n",
            "1649 0.4217 81.25\n",
            "1699 0.0998 100.0\n",
            "1749 0.8257 62.5\n",
            "1799 0.3277 93.75\n",
            "1849 0.2808 93.75\n",
            "1899 0.5157 81.25\n",
            "1949 0.0964 100.0\n",
            "1999 0.5364 75.0\n",
            "2049 0.3799 81.25\n",
            "2099 0.8538 68.75\n",
            "2149 0.4584 87.5\n",
            "2199 0.7143 75.0\n",
            "2249 0.0954 93.75\n",
            "2299 0.5918 75.0\n",
            "2349 0.3753 81.25\n",
            "2399 0.5987 87.5\n",
            "2449 0.7285 87.5\n",
            "2499 0.2897 93.75\n",
            "2549 0.7274 75.0\n",
            "2599 0.5299 87.5\n",
            "2649 0.4177 87.5\n",
            "2699 0.4727 87.5\n",
            "2749 0.5490 81.25\n",
            "2799 0.1840 93.75\n",
            "2849 0.2810 93.75\n",
            "2899 0.7563 81.25\n",
            "2949 0.2786 87.5\n",
            "2999 0.5463 87.5\n",
            "3049 0.7381 75.0\n",
            "3099 0.7219 75.0\n",
            "3149 0.3517 93.75\n",
            "3199 0.1760 100.0\n",
            "3249 0.4355 81.25\n",
            "3299 0.4844 81.25\n",
            "3349 0.7574 62.5\n",
            "3399 0.6459 75.0\n",
            "3449 0.1119 100.0\n",
            "3499 0.5912 81.25\n",
            "3549 0.5887 81.25\n",
            "3599 0.2519 87.5\n",
            "3649 0.7372 75.0\n",
            "3699 0.5804 81.25\n",
            "3749 0.1698 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0347, Accuracy: 8129/10000 (81.29%)\n",
            "\n",
            "--- 17.596538543701172 seconds ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06ca0d9e-b39f-4b9b-deca-1782dc29264e",
        "id": "t_SDinV-QRNx"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoded data folder already exists\n",
            "Optimizing class representatives for 10 epochs\n",
            "Loading encoded training data...\n",
            "Loading encoded test data...\n",
            "Size of encoded training data torch.Size([60000, 10000]) and test data torch.Size([10000, 10000])\n",
            "Epoch: 1\n",
            "49 1.0141 75.0\n",
            "99 0.7540 75.0\n",
            "149 0.6296 87.5\n",
            "199 0.5660 68.75\n",
            "249 0.4189 81.25\n",
            "299 0.5472 81.25\n",
            "349 0.1106 100.0\n",
            "399 0.6538 75.0\n",
            "449 0.4522 87.5\n",
            "499 0.2475 93.75\n",
            "549 0.4516 93.75\n",
            "599 0.2532 93.75\n",
            "649 0.1557 100.0\n",
            "699 0.3438 93.75\n",
            "749 0.7175 81.25\n",
            "799 0.3458 87.5\n",
            "849 0.2399 100.0\n",
            "899 0.6001 75.0\n",
            "949 0.2190 93.75\n",
            "999 0.5107 75.0\n",
            "1049 0.8740 75.0\n",
            "1099 0.6294 75.0\n",
            "1149 0.1861 100.0\n",
            "1199 0.3055 93.75\n",
            "1249 0.1911 93.75\n",
            "1299 0.1205 100.0\n",
            "1349 0.2276 93.75\n",
            "1399 0.6187 87.5\n",
            "1449 0.4791 87.5\n",
            "1499 0.5584 81.25\n",
            "1549 0.7981 81.25\n",
            "1599 0.4997 75.0\n",
            "1649 0.5629 81.25\n",
            "1699 0.3080 87.5\n",
            "1749 0.6065 81.25\n",
            "1799 0.3636 93.75\n",
            "1849 0.2220 93.75\n",
            "1899 0.5691 68.75\n",
            "1949 0.5324 75.0\n",
            "1999 0.4261 87.5\n",
            "2049 0.4685 87.5\n",
            "2099 0.6243 75.0\n",
            "2149 0.3425 87.5\n",
            "2199 0.5475 93.75\n",
            "2249 0.5639 81.25\n",
            "2299 0.5641 81.25\n",
            "2349 0.4647 87.5\n",
            "2399 0.3410 81.25\n",
            "2449 0.5029 75.0\n",
            "2499 0.6121 87.5\n",
            "2549 0.4008 81.25\n",
            "2599 0.4861 75.0\n",
            "2649 0.2269 87.5\n",
            "2699 0.3042 87.5\n",
            "2749 0.2078 93.75\n",
            "2799 0.3412 93.75\n",
            "2849 0.4695 75.0\n",
            "2899 0.3487 81.25\n",
            "2949 0.8849 81.25\n",
            "2999 0.8200 75.0\n",
            "3049 0.8704 87.5\n",
            "3099 0.4070 87.5\n",
            "3149 0.3424 81.25\n",
            "3199 0.6838 75.0\n",
            "3249 0.1203 100.0\n",
            "3299 0.3272 93.75\n",
            "3349 0.7166 62.5\n",
            "3399 0.5956 75.0\n",
            "3449 0.5595 81.25\n",
            "3499 0.4589 87.5\n",
            "3549 0.5412 87.5\n",
            "3599 0.1510 100.0\n",
            "3649 0.6238 81.25\n",
            "3699 0.2139 93.75\n",
            "3749 0.3985 87.5\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0344, Accuracy: 8162/10000 (81.62%)\n",
            "\n",
            "--- 17.840992212295532 seconds ---\n",
            "Epoch: 2\n",
            "49 0.0648 100.0\n",
            "99 0.1706 93.75\n",
            "149 0.2266 93.75\n",
            "199 0.3309 87.5\n",
            "249 0.1762 93.75\n",
            "299 0.1257 100.0\n",
            "349 0.9395 81.25\n",
            "399 0.3691 87.5\n",
            "449 0.1835 93.75\n",
            "499 0.6897 81.25\n",
            "549 0.5336 81.25\n",
            "599 0.2847 93.75\n",
            "649 0.3119 87.5\n",
            "699 0.2788 87.5\n",
            "749 0.1723 87.5\n",
            "799 0.3436 87.5\n",
            "849 0.3286 87.5\n",
            "899 0.1823 93.75\n",
            "949 0.3957 87.5\n",
            "999 0.7523 75.0\n",
            "1049 0.2520 93.75\n",
            "1099 0.1738 100.0\n",
            "1149 0.4964 81.25\n",
            "1199 0.5147 87.5\n",
            "1249 0.5088 75.0\n",
            "1299 0.5083 81.25\n",
            "1349 0.5962 87.5\n",
            "1399 0.1808 93.75\n",
            "1449 0.3675 87.5\n",
            "1499 0.3549 81.25\n",
            "1549 0.6501 81.25\n",
            "1599 0.7201 81.25\n",
            "1649 0.1856 93.75\n",
            "1699 0.2283 93.75\n",
            "1749 0.5748 75.0\n",
            "1799 0.2531 87.5\n",
            "1849 0.3386 81.25\n",
            "1899 0.6682 75.0\n",
            "1949 0.9850 56.25\n",
            "1999 0.8076 75.0\n",
            "2049 0.1517 93.75\n",
            "2099 0.3426 81.25\n",
            "2149 0.2958 93.75\n",
            "2199 0.1428 100.0\n",
            "2249 0.2038 93.75\n",
            "2299 0.4410 87.5\n",
            "2349 0.2472 87.5\n",
            "2399 0.5948 81.25\n",
            "2449 0.2600 93.75\n",
            "2499 0.8436 75.0\n",
            "2549 0.4162 81.25\n",
            "2599 0.2266 87.5\n",
            "2649 0.4397 87.5\n",
            "2699 0.7212 81.25\n",
            "2749 0.1682 100.0\n",
            "2799 0.2548 93.75\n",
            "2849 0.6798 81.25\n",
            "2899 0.1954 93.75\n",
            "2949 0.5941 87.5\n",
            "2999 0.5820 87.5\n",
            "3049 0.6632 87.5\n",
            "3099 0.5122 81.25\n",
            "3149 0.5084 87.5\n",
            "3199 0.4575 87.5\n",
            "3249 0.3131 93.75\n",
            "3299 0.3262 93.75\n",
            "3349 0.2027 93.75\n",
            "3399 0.2952 93.75\n",
            "3449 0.0602 100.0\n",
            "3499 0.3059 87.5\n",
            "3549 0.3060 87.5\n",
            "3599 0.1918 93.75\n",
            "3649 0.1829 100.0\n",
            "3699 0.2473 93.75\n",
            "3749 0.5658 81.25\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0368, Accuracy: 8048/10000 (80.48%)\n",
            "\n",
            "--- 16.81853151321411 seconds ---\n",
            "Epoch: 3\n",
            "49 0.5192 81.25\n",
            "99 0.1718 93.75\n",
            "149 0.0934 100.0\n",
            "199 0.1501 100.0\n",
            "249 0.1601 93.75\n",
            "299 0.4500 87.5\n",
            "349 0.0254 100.0\n",
            "399 0.2295 93.75\n",
            "449 0.5836 75.0\n",
            "499 0.0188 100.0\n",
            "549 0.2613 93.75\n",
            "599 0.4175 81.25\n",
            "649 0.0651 100.0\n",
            "699 0.2290 87.5\n",
            "749 0.1193 93.75\n",
            "799 0.4636 81.25\n",
            "849 0.1095 100.0\n",
            "899 0.2492 93.75\n",
            "949 0.5764 81.25\n",
            "999 0.5928 81.25\n",
            "1049 0.2475 93.75\n",
            "1099 0.3112 87.5\n",
            "1149 0.3583 81.25\n",
            "1199 0.8359 75.0\n",
            "1249 0.0887 100.0\n",
            "1299 0.3520 93.75\n",
            "1349 0.3279 87.5\n",
            "1399 0.2554 87.5\n",
            "1449 0.2224 93.75\n",
            "1499 0.7148 87.5\n",
            "1549 0.7542 81.25\n",
            "1599 0.2783 93.75\n",
            "1649 0.5610 87.5\n",
            "1699 0.3597 87.5\n",
            "1749 0.7287 81.25\n",
            "1799 0.6320 81.25\n",
            "1849 0.4041 87.5\n",
            "1899 0.0561 100.0\n",
            "1949 0.1379 93.75\n",
            "1999 0.1506 93.75\n",
            "2049 0.2252 93.75\n",
            "2099 0.3944 87.5\n",
            "2149 0.3354 81.25\n",
            "2199 0.3928 87.5\n",
            "2249 0.3016 87.5\n",
            "2299 0.3228 87.5\n",
            "2349 0.3964 87.5\n",
            "2399 0.1772 100.0\n",
            "2449 0.2894 87.5\n",
            "2499 0.3987 93.75\n",
            "2549 0.2721 87.5\n",
            "2599 0.4831 81.25\n",
            "2649 0.2122 87.5\n",
            "2699 0.3021 93.75\n",
            "2749 0.5180 81.25\n",
            "2799 0.2642 93.75\n",
            "2849 0.0564 100.0\n",
            "2899 0.8324 81.25\n",
            "2949 0.2692 93.75\n",
            "2999 0.1969 93.75\n",
            "3049 0.4941 87.5\n",
            "3099 0.2851 93.75\n",
            "3149 0.2639 87.5\n",
            "3199 0.4836 81.25\n",
            "3249 0.3881 87.5\n",
            "3299 0.3131 87.5\n",
            "3349 0.2478 93.75\n",
            "3399 0.2363 93.75\n",
            "3449 0.3755 93.75\n",
            "3499 0.3150 87.5\n",
            "3549 0.7036 62.5\n",
            "3599 0.5905 68.75\n",
            "3649 0.3976 93.75\n",
            "3699 0.6369 62.5\n",
            "3749 0.1528 93.75\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0385, Accuracy: 7917/10000 (79.17%)\n",
            "\n",
            "--- 16.67950749397278 seconds ---\n",
            "Epoch: 4\n",
            "49 0.2335 87.5\n",
            "99 0.0663 100.0\n",
            "149 0.3078 93.75\n",
            "199 0.1799 93.75\n",
            "249 0.5648 87.5\n",
            "299 0.0915 100.0\n",
            "349 0.2176 87.5\n",
            "399 0.4730 81.25\n",
            "449 0.1124 93.75\n",
            "499 0.1638 93.75\n",
            "549 0.1404 93.75\n",
            "599 0.1319 93.75\n",
            "649 0.2735 87.5\n",
            "699 0.0780 100.0\n",
            "749 0.2290 87.5\n",
            "799 0.3012 87.5\n",
            "849 0.0885 100.0\n",
            "899 0.2794 81.25\n",
            "949 0.2784 87.5\n",
            "999 0.2327 93.75\n",
            "1049 0.0785 93.75\n",
            "1099 0.2448 87.5\n",
            "1149 0.3579 87.5\n",
            "1199 0.2541 93.75\n",
            "1249 0.3254 93.75\n",
            "1299 0.3688 87.5\n",
            "1349 0.2489 87.5\n",
            "1399 0.3258 93.75\n",
            "1449 0.7050 68.75\n",
            "1499 0.2141 93.75\n",
            "1549 0.2612 93.75\n",
            "1599 0.2802 87.5\n",
            "1649 0.4204 87.5\n",
            "1699 0.2575 93.75\n",
            "1749 0.7571 93.75\n",
            "1799 0.1618 100.0\n",
            "1849 0.7781 68.75\n",
            "1899 0.3632 81.25\n",
            "1949 0.3812 87.5\n",
            "1999 0.4728 75.0\n",
            "2049 0.2813 93.75\n",
            "2099 0.5552 81.25\n",
            "2149 0.2395 87.5\n",
            "2199 0.0969 100.0\n",
            "2249 0.3070 87.5\n",
            "2299 0.1132 100.0\n",
            "2349 0.6898 81.25\n",
            "2399 0.1455 100.0\n",
            "2449 0.2439 81.25\n",
            "2499 0.4968 81.25\n",
            "2549 0.2962 81.25\n",
            "2599 0.5487 81.25\n",
            "2649 0.2561 93.75\n",
            "2699 0.2165 93.75\n",
            "2749 0.3919 81.25\n",
            "2799 0.1599 93.75\n",
            "2849 0.0754 100.0\n",
            "2899 0.2977 87.5\n",
            "2949 0.1876 93.75\n",
            "2999 0.4896 75.0\n",
            "3049 0.7666 68.75\n",
            "3099 0.7591 68.75\n",
            "3149 0.3307 87.5\n",
            "3199 0.2810 87.5\n",
            "3249 0.1745 100.0\n",
            "3299 0.3708 81.25\n",
            "3349 0.3752 87.5\n",
            "3399 0.3299 93.75\n",
            "3449 0.2936 87.5\n",
            "3499 0.8317 68.75\n",
            "3549 0.7114 75.0\n",
            "3599 0.3269 93.75\n",
            "3649 0.4453 87.5\n",
            "3699 0.1317 93.75\n",
            "3749 0.5089 68.75\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0376, Accuracy: 8021/10000 (80.21%)\n",
            "\n",
            "--- 17.538370370864868 seconds ---\n",
            "Epoch: 5\n",
            "49 0.0637 100.0\n",
            "99 0.1762 100.0\n",
            "149 0.6985 81.25\n",
            "199 0.4821 87.5\n",
            "249 0.3514 87.5\n",
            "299 0.5691 81.25\n",
            "349 0.1610 93.75\n",
            "399 0.1490 100.0\n",
            "449 0.3103 81.25\n",
            "499 0.1228 93.75\n",
            "549 0.0655 100.0\n",
            "599 0.1633 100.0\n",
            "649 0.1886 87.5\n",
            "699 0.2400 93.75\n",
            "749 0.7243 81.25\n",
            "799 0.5514 75.0\n",
            "849 0.2230 93.75\n",
            "899 0.0744 100.0\n",
            "949 0.4077 81.25\n",
            "999 0.5660 75.0\n",
            "1049 0.4539 87.5\n",
            "1099 0.5440 75.0\n",
            "1149 0.2947 87.5\n",
            "1199 0.7811 75.0\n",
            "1249 0.1277 93.75\n",
            "1299 0.3489 87.5\n",
            "1349 0.1531 93.75\n",
            "1399 0.3664 81.25\n",
            "1449 0.8102 56.25\n",
            "1499 0.2356 93.75\n",
            "1549 0.2766 93.75\n",
            "1599 0.4748 81.25\n",
            "1649 0.2494 93.75\n",
            "1699 0.3221 87.5\n",
            "1749 0.2343 93.75\n",
            "1799 0.2358 93.75\n",
            "1849 0.2429 93.75\n",
            "1899 0.5583 75.0\n",
            "1949 0.1763 93.75\n",
            "1999 0.4633 93.75\n",
            "2049 0.7624 68.75\n",
            "2099 0.2115 93.75\n",
            "2149 0.3862 87.5\n",
            "2199 0.7612 81.25\n",
            "2249 0.0866 100.0\n",
            "2299 0.2173 87.5\n",
            "2349 0.3830 87.5\n",
            "2399 0.3978 81.25\n",
            "2449 0.5043 81.25\n",
            "2499 0.1027 93.75\n",
            "2549 0.6380 75.0\n",
            "2599 0.1409 93.75\n",
            "2649 0.7977 75.0\n",
            "2699 1.0508 62.5\n",
            "2749 0.6501 87.5\n",
            "2799 0.2858 87.5\n",
            "2849 0.6799 81.25\n",
            "2899 0.5741 87.5\n",
            "2949 0.3890 87.5\n",
            "2999 1.0187 62.5\n",
            "3049 0.1856 93.75\n",
            "3099 0.2939 87.5\n",
            "3149 0.3913 81.25\n",
            "3199 0.2367 93.75\n",
            "3249 0.1493 93.75\n",
            "3299 0.4823 81.25\n",
            "3349 0.2938 87.5\n",
            "3399 0.2698 87.5\n",
            "3449 0.4448 87.5\n",
            "3499 0.2293 93.75\n",
            "3549 0.2547 87.5\n",
            "3599 0.2267 87.5\n",
            "3649 0.4795 75.0\n",
            "3699 0.2858 87.5\n",
            "3749 0.3729 93.75\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0377, Accuracy: 7979/10000 (79.79%)\n",
            "\n",
            "--- 16.98568034172058 seconds ---\n",
            "Epoch: 6\n",
            "49 0.5635 81.25\n",
            "99 0.4226 87.5\n",
            "149 0.7088 87.5\n",
            "199 0.1865 87.5\n",
            "249 0.0832 100.0\n",
            "299 0.0995 100.0\n",
            "349 0.1129 93.75\n",
            "399 0.2658 87.5\n",
            "449 0.2949 81.25\n",
            "499 0.3632 81.25\n",
            "549 0.3186 87.5\n",
            "599 0.3347 87.5\n",
            "649 0.3531 81.25\n",
            "699 0.3636 87.5\n",
            "749 0.3053 93.75\n",
            "799 0.1167 100.0\n",
            "849 0.4615 81.25\n",
            "899 0.2478 87.5\n",
            "949 0.6300 75.0\n",
            "999 0.5798 87.5\n",
            "1049 0.2826 81.25\n",
            "1099 0.3747 87.5\n",
            "1149 0.3722 81.25\n",
            "1199 0.2080 93.75\n",
            "1249 0.4993 87.5\n",
            "1299 0.3237 87.5\n",
            "1349 0.4586 81.25\n",
            "1399 0.1128 100.0\n",
            "1449 0.5897 75.0\n",
            "1499 0.4863 81.25\n",
            "1549 0.4083 81.25\n",
            "1599 0.5802 87.5\n",
            "1649 0.3505 75.0\n",
            "1699 0.2698 93.75\n",
            "1749 0.2472 87.5\n",
            "1799 0.6399 75.0\n",
            "1849 0.4774 87.5\n",
            "1899 0.1495 100.0\n",
            "1949 0.7434 81.25\n",
            "1999 0.5642 81.25\n",
            "2049 0.7112 75.0\n",
            "2099 0.2000 93.75\n",
            "2149 0.0208 100.0\n",
            "2199 0.7119 68.75\n",
            "2249 0.8499 75.0\n",
            "2299 0.6691 87.5\n",
            "2349 0.5825 75.0\n",
            "2399 0.3320 87.5\n",
            "2449 0.3399 93.75\n",
            "2499 0.4083 81.25\n",
            "2549 0.8675 68.75\n",
            "2599 0.3840 81.25\n",
            "2649 0.7264 81.25\n",
            "2699 0.3042 87.5\n",
            "2749 0.8105 81.25\n",
            "2799 0.5111 81.25\n",
            "2849 0.1150 100.0\n",
            "2899 0.6830 75.0\n",
            "2949 0.6234 68.75\n",
            "2999 0.5186 75.0\n",
            "3049 0.3442 87.5\n",
            "3099 0.2785 87.5\n",
            "3149 0.5839 81.25\n",
            "3199 0.1780 87.5\n",
            "3249 0.4628 87.5\n",
            "3299 0.5656 75.0\n",
            "3349 0.3555 87.5\n",
            "3399 0.2789 87.5\n",
            "3449 0.2468 93.75\n",
            "3499 0.4101 81.25\n",
            "3549 0.5042 81.25\n",
            "3599 0.2512 93.75\n",
            "3649 0.4219 81.25\n",
            "3699 0.4878 81.25\n",
            "3749 0.5781 75.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0383, Accuracy: 8005/10000 (80.05%)\n",
            "\n",
            "--- 17.007688522338867 seconds ---\n",
            "Epoch: 7\n",
            "49 0.1965 100.0\n",
            "99 0.3898 75.0\n",
            "149 0.3192 81.25\n",
            "199 0.3657 87.5\n",
            "249 0.4023 87.5\n",
            "299 0.0871 100.0\n",
            "349 0.5011 75.0\n",
            "399 0.1566 93.75\n",
            "449 0.0796 100.0\n",
            "499 0.1662 93.75\n",
            "549 0.2091 87.5\n",
            "599 0.4560 81.25\n",
            "649 0.2159 87.5\n",
            "699 0.4691 81.25\n",
            "749 0.4560 81.25\n",
            "799 0.5039 81.25\n",
            "849 0.3972 93.75\n",
            "899 0.6556 68.75\n",
            "949 0.2605 87.5\n",
            "999 0.3723 87.5\n",
            "1049 0.1647 93.75\n",
            "1099 0.1611 93.75\n",
            "1149 0.2032 100.0\n",
            "1199 0.2083 87.5\n",
            "1249 0.4982 75.0\n",
            "1299 0.5732 87.5\n",
            "1349 0.2413 93.75\n",
            "1399 0.1757 93.75\n",
            "1449 0.2609 87.5\n",
            "1499 0.5449 75.0\n",
            "1549 0.2364 100.0\n",
            "1599 0.4508 93.75\n",
            "1649 0.7296 68.75\n",
            "1699 0.5467 75.0\n",
            "1749 0.4167 87.5\n",
            "1799 0.2856 87.5\n",
            "1849 0.1878 100.0\n",
            "1899 0.8453 75.0\n",
            "1949 0.5739 81.25\n",
            "1999 0.5514 75.0\n",
            "2049 0.4002 81.25\n",
            "2099 0.8234 68.75\n",
            "2149 0.0752 100.0\n",
            "2199 0.5797 75.0\n",
            "2249 0.2924 87.5\n",
            "2299 0.8152 68.75\n",
            "2349 0.2985 87.5\n",
            "2399 0.1597 87.5\n",
            "2449 0.0330 100.0\n",
            "2499 0.2674 87.5\n",
            "2549 0.6087 68.75\n",
            "2599 0.6541 68.75\n",
            "2649 0.7807 75.0\n",
            "2699 0.6475 75.0\n",
            "2749 0.0984 100.0\n",
            "2799 0.2035 93.75\n",
            "2849 0.4715 81.25\n",
            "2899 0.5904 68.75\n",
            "2949 0.4089 87.5\n",
            "2999 0.3681 87.5\n",
            "3049 0.8007 75.0\n",
            "3099 0.9795 68.75\n",
            "3149 0.2802 87.5\n",
            "3199 0.1728 100.0\n",
            "3249 0.1723 100.0\n",
            "3299 0.8066 81.25\n",
            "3349 0.5783 75.0\n",
            "3399 0.3930 81.25\n",
            "3449 0.6075 81.25\n",
            "3499 0.7259 75.0\n",
            "3549 0.9718 81.25\n",
            "3599 0.6517 81.25\n",
            "3649 0.4475 81.25\n",
            "3699 0.6829 81.25\n",
            "3749 0.3010 87.5\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0378, Accuracy: 7980/10000 (79.80%)\n",
            "\n",
            "--- 17.630035638809204 seconds ---\n",
            "Epoch: 8\n",
            "49 0.4031 87.5\n",
            "99 0.4126 81.25\n",
            "149 0.1162 93.75\n",
            "199 0.0820 100.0\n",
            "249 0.3355 93.75\n",
            "299 0.2535 87.5\n",
            "349 0.3318 87.5\n",
            "399 0.6560 81.25\n",
            "449 0.0941 100.0\n",
            "499 0.8119 68.75\n",
            "549 0.2941 87.5\n",
            "599 0.6510 75.0\n",
            "649 0.1255 100.0\n",
            "699 0.2395 93.75\n",
            "749 0.1828 100.0\n",
            "799 0.3856 87.5\n",
            "849 0.3413 81.25\n",
            "899 0.3982 93.75\n",
            "949 0.1337 93.75\n",
            "999 0.7034 75.0\n",
            "1049 0.8404 75.0\n",
            "1099 0.1857 100.0\n",
            "1149 0.3987 75.0\n",
            "1199 0.1384 100.0\n",
            "1249 0.4722 87.5\n",
            "1299 0.5816 87.5\n",
            "1349 0.1632 100.0\n",
            "1399 0.1773 93.75\n",
            "1449 0.1281 100.0\n",
            "1499 0.3471 81.25\n",
            "1549 0.7612 75.0\n",
            "1599 0.4179 81.25\n",
            "1649 0.6851 75.0\n",
            "1699 0.0775 100.0\n",
            "1749 0.5751 81.25\n",
            "1799 0.1978 93.75\n",
            "1849 0.3496 81.25\n",
            "1899 0.2798 87.5\n",
            "1949 0.5047 87.5\n",
            "1999 0.1026 100.0\n",
            "2049 0.3571 87.5\n",
            "2099 0.3741 87.5\n",
            "2149 0.2086 87.5\n",
            "2199 0.2160 93.75\n",
            "2249 0.6785 75.0\n",
            "2299 0.1891 93.75\n",
            "2349 0.3926 81.25\n",
            "2399 0.1526 93.75\n",
            "2449 0.6272 87.5\n",
            "2499 0.1993 87.5\n",
            "2549 0.4612 75.0\n",
            "2599 0.4807 75.0\n",
            "2649 0.5595 87.5\n",
            "2699 0.3320 87.5\n",
            "2749 0.1957 93.75\n",
            "2799 0.4619 81.25\n",
            "2849 0.6169 81.25\n",
            "2899 0.7682 81.25\n",
            "2949 0.3568 87.5\n",
            "2999 0.2209 87.5\n",
            "3049 0.8260 75.0\n",
            "3099 0.2700 93.75\n",
            "3149 0.5416 68.75\n",
            "3199 0.4655 75.0\n",
            "3249 0.0299 100.0\n",
            "3299 0.5324 68.75\n",
            "3349 0.5041 81.25\n",
            "3399 0.3674 87.5\n",
            "3449 0.3562 81.25\n",
            "3499 0.1492 100.0\n",
            "3549 0.4907 81.25\n",
            "3599 0.6850 68.75\n",
            "3649 0.6720 81.25\n",
            "3699 0.2177 87.5\n",
            "3749 0.8209 68.75\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0383, Accuracy: 7978/10000 (79.78%)\n",
            "\n",
            "--- 16.886629819869995 seconds ---\n",
            "Epoch: 9\n",
            "49 0.1376 100.0\n",
            "99 0.5621 81.25\n",
            "149 0.2715 93.75\n",
            "199 0.3173 93.75\n",
            "249 0.3978 81.25\n",
            "299 0.2615 87.5\n",
            "349 0.2667 93.75\n",
            "399 0.5829 81.25\n",
            "449 0.2656 87.5\n",
            "499 0.0443 100.0\n",
            "549 0.0772 100.0\n",
            "599 0.1154 100.0\n",
            "649 0.3310 87.5\n",
            "699 0.1663 93.75\n",
            "749 0.2086 87.5\n",
            "799 0.1673 87.5\n",
            "849 0.2451 100.0\n",
            "899 0.9993 75.0\n",
            "949 0.5889 93.75\n",
            "999 0.2046 93.75\n",
            "1049 0.3365 87.5\n",
            "1099 0.4353 75.0\n",
            "1149 0.3349 87.5\n",
            "1199 0.3973 93.75\n",
            "1249 0.6223 81.25\n",
            "1299 0.1705 93.75\n",
            "1349 0.4930 87.5\n",
            "1399 0.2826 87.5\n",
            "1449 0.4902 81.25\n",
            "1499 0.1771 93.75\n",
            "1549 0.3982 81.25\n",
            "1599 0.7812 81.25\n",
            "1649 0.2136 93.75\n",
            "1699 0.2045 87.5\n",
            "1749 0.5938 81.25\n",
            "1799 0.4034 87.5\n",
            "1849 0.2226 93.75\n",
            "1899 0.2979 87.5\n",
            "1949 0.5431 75.0\n",
            "1999 0.4689 93.75\n",
            "2049 0.6720 75.0\n",
            "2099 0.9651 75.0\n",
            "2149 0.4050 75.0\n",
            "2199 0.2861 87.5\n",
            "2249 0.3981 87.5\n",
            "2299 0.4744 81.25\n",
            "2349 0.2722 93.75\n",
            "2399 0.8932 68.75\n",
            "2449 0.6596 75.0\n",
            "2499 0.4844 87.5\n",
            "2549 0.3836 81.25\n",
            "2599 0.3435 81.25\n",
            "2649 1.0152 68.75\n",
            "2699 0.2401 93.75\n",
            "2749 0.2747 87.5\n",
            "2799 0.4273 81.25\n",
            "2849 0.0852 100.0\n",
            "2899 0.3635 93.75\n",
            "2949 0.3884 87.5\n",
            "2999 0.4816 81.25\n",
            "3049 0.5977 81.25\n",
            "3099 0.4174 81.25\n",
            "3149 0.2711 93.75\n",
            "3199 0.1064 100.0\n",
            "3249 0.6247 75.0\n",
            "3299 0.4106 87.5\n",
            "3349 0.1663 93.75\n",
            "3399 0.2229 100.0\n",
            "3449 0.1825 93.75\n",
            "3499 0.7596 81.25\n",
            "3549 0.4470 87.5\n",
            "3599 0.4408 87.5\n",
            "3649 0.2116 93.75\n",
            "3699 0.3789 87.5\n",
            "3749 0.8831 68.75\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0384, Accuracy: 8005/10000 (80.05%)\n",
            "\n",
            "--- 17.03627109527588 seconds ---\n",
            "Epoch: 10\n",
            "49 0.1260 100.0\n",
            "99 0.1415 93.75\n",
            "149 0.0421 100.0\n",
            "199 0.0476 100.0\n",
            "249 0.2991 87.5\n",
            "299 0.2219 87.5\n",
            "349 0.5291 81.25\n",
            "399 0.1879 93.75\n",
            "449 0.3450 81.25\n",
            "499 0.8013 81.25\n",
            "549 0.3918 75.0\n",
            "599 0.0954 100.0\n",
            "649 0.1878 93.75\n",
            "699 0.4666 81.25\n",
            "749 0.4009 87.5\n",
            "799 0.2847 93.75\n",
            "849 0.3042 93.75\n",
            "899 0.1295 93.75\n",
            "949 0.1390 93.75\n",
            "999 0.1366 100.0\n",
            "1049 0.4454 81.25\n",
            "1099 0.4238 81.25\n",
            "1149 0.1414 100.0\n",
            "1199 0.5002 87.5\n",
            "1249 0.1435 100.0\n",
            "1299 0.3502 87.5\n",
            "1349 0.1529 93.75\n",
            "1399 0.3165 81.25\n",
            "1449 0.3751 87.5\n",
            "1499 0.6921 81.25\n",
            "1549 0.5695 75.0\n",
            "1599 0.7515 62.5\n",
            "1649 0.5030 87.5\n",
            "1699 0.7763 81.25\n",
            "1749 0.1977 93.75\n",
            "1799 0.5821 81.25\n",
            "1849 0.2177 93.75\n",
            "1899 0.3464 93.75\n",
            "1949 0.3435 81.25\n",
            "1999 0.5354 81.25\n",
            "2049 0.7291 81.25\n",
            "2099 0.0945 93.75\n",
            "2149 0.1476 93.75\n",
            "2199 0.0765 100.0\n",
            "2249 0.6885 68.75\n",
            "2299 0.1957 93.75\n",
            "2349 0.6561 81.25\n",
            "2399 0.4944 87.5\n",
            "2449 0.0652 100.0\n",
            "2499 0.5440 87.5\n",
            "2549 0.1746 93.75\n",
            "2599 0.3982 87.5\n",
            "2649 0.4482 87.5\n",
            "2699 0.7272 68.75\n",
            "2749 0.4368 81.25\n",
            "2799 0.1804 100.0\n",
            "2849 0.2671 87.5\n",
            "2899 0.2117 87.5\n",
            "2949 0.4959 75.0\n",
            "2999 0.3344 87.5\n",
            "3049 0.5985 75.0\n",
            "3099 0.3377 87.5\n",
            "3149 0.4842 81.25\n",
            "3199 0.5376 75.0\n",
            "3249 0.1463 93.75\n",
            "3299 0.7263 56.25\n",
            "3349 0.5377 81.25\n",
            "3399 0.2480 93.75\n",
            "3449 0.3592 81.25\n",
            "3499 0.6791 75.0\n",
            "3549 0.3996 87.5\n",
            "3599 0.2824 87.5\n",
            "3649 0.4856 87.5\n",
            "3699 0.3888 87.5\n",
            "3749 0.2532 87.5\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0387, Accuracy: 7934/10000 (79.34%)\n",
            "\n",
            "--- 17.6374351978302 seconds ---\n"
          ]
        }
      ],
      "source": [
        "#fminst epoch 10 rffg2vsa\n",
        "class ArgumentParser:\n",
        "    def __init__(self):\n",
        "        self.args = {\n",
        "            'lr': 0.01,            # Learning rate for optimizing class representative\n",
        "            'gamma': 0.3,          # Kernel parameter for computing covariance\n",
        "            'epoch': 10,            # Epochs of training\n",
        "            'gorder': 2,           # Order of the cyclic group required for G-VSA (8 ou 16)\n",
        "            'dim': 10000,          # Dimension of hypervectors c le grand D\n",
        "            'seed': 43,            # Random seed for reproducing results\n",
        "            'resume': True,       # Resume from existing encoded hypervectors\n",
        "            'data_dir': './encoded_data', # Directory to save encoded data (hypervectors)\n",
        "            'dataset': 'fmnist',    # Dataset choice choices=['mnist', 'fmnist', 'cifar', 'isolet', 'ucihar']\n",
        "            'raw_data_dir': './dataset',   # Raw data directory\n",
        "            'model': 'rff-gvsa'    # Model type choices=['rff-hdc', 'linear-hdc', 'rff-gvsa']\n",
        "        }\n",
        "        self.seed = self.args['seed']\n",
        "        self.lr = self.args['lr']\n",
        "        self.gamma = self.args['gamma']\n",
        "        self.epoch = self.args['epoch']\n",
        "        self.gorder = self.args['gorder']\n",
        "        self.dim = self.args['dim']\n",
        "        self.resume = self.args['resume']\n",
        "        self.data_dir = self.args['data_dir']\n",
        "        self.dataset = self.args['dataset']\n",
        "        self.raw_data_dir = self.args['raw_data_dir']\n",
        "        self.model = self.args['model']\n",
        "\n",
        "\n",
        "    def get_args(self):\n",
        "        return self.args\n",
        "args = ArgumentParser()\n",
        "torch.manual_seed(args.seed)\n",
        "np.random.seed(args.seed)\n",
        "if 'hdc' in args.model:\n",
        "    args.gorder = 2\n",
        "    print(\"Use binary HDC with random fourier features, ignoring gorder, set to 2.\")\n",
        "args.data_dir = f'{args.data_dir}/{args.dataset}_{args.model}_order{args.gorder}_gamma{args.gamma}_dim{args.dim}'\n",
        "try:\n",
        "    os.makedirs(args.data_dir)\n",
        "except FileExistsError:\n",
        "    print('Encoded data folder already exists')\n",
        "if not args.resume:\n",
        "    print('Encode the dataset into hypervectors and save')\n",
        "    encode_and_save(args)\n",
        "    print('Finish encoding and saving')\n",
        "print(f'Optimizing class representatives for {args.epoch} epochs')\n",
        "train(args)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RFF G($2^2$)-VSA"
      ],
      "metadata": {
        "id": "wFj0ownaVn0C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ISOLET"
      ],
      "metadata": {
        "id": "WXkkYdTpVn0D"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9251f21c-e31a-4a21-b977-036c517b2f7d",
        "id": "sElvK745Vn0E"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encode the dataset into hypervectors and save\n",
            "# of channels of data 1\n",
            "# of training samples and test samples 6238 1559\n",
            "Encoding with random fourier features encoder.\n",
            "the threshold to discretize fourier features to group elements tensor([-0.6746,  0.0000,  0.6746])\n",
            "Encoded pixels to hypervectors with size:  torch.Size([256, 10000])\n",
            "Encoding training data...\n",
            "Start encoding data\n",
            "Finish encoding data\n",
            "Encoding test data...\n",
            "Start encoding data\n",
            "Finish encoding data\n",
            "Finish encoding and saving\n",
            "Optimizing class representatives for 1 epochs\n",
            "Loading encoded training data...\n",
            "Loading encoded test data...\n",
            "Size of encoded training data torch.Size([6238, 10000]) and test data torch.Size([1559, 10000])\n",
            "Epoch: 1\n",
            "49 2.0968 62.5\n",
            "99 1.1331 75.0\n",
            "149 0.5840 81.25\n",
            "199 0.3061 93.75\n",
            "249 0.4890 81.25\n",
            "299 0.5003 75.0\n",
            "349 0.1824 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0597, Accuracy: 1342/1559 (86.08%)\n",
            "\n",
            "--- 2.604689836502075 seconds ---\n"
          ]
        }
      ],
      "source": [
        "#isolet epoch 1 rffg4vsa\n",
        "class ArgumentParser:\n",
        "    def __init__(self):\n",
        "        self.args = {\n",
        "            'lr': 0.01,            # Learning rate for optimizing class representative\n",
        "            'gamma': 0.3,          # Kernel parameter for computing covariance\n",
        "            'epoch': 1,            # Epochs of training\n",
        "            'gorder': 4,           # Order of the cyclic group required for G-VSA (8 ou 16)\n",
        "            'dim': 10000,          # Dimension of hypervectors c le grand D\n",
        "            'seed': 43,            # Random seed for reproducing results\n",
        "            'resume': False,       # Resume from existing encoded hypervectors\n",
        "            'data_dir': './encoded_data', # Directory to save encoded data (hypervectors)\n",
        "            'dataset': 'isolet',    # Dataset choice choices=['mnist', 'fmnist', 'cifar', 'isolet', 'ucihar']\n",
        "            'raw_data_dir': './dataset',   # Raw data directory\n",
        "            'model': 'rff-gvsa'    # Model type choices=['rff-hdc', 'linear-hdc', 'rff-gvsa']\n",
        "        }\n",
        "        self.seed = self.args['seed']\n",
        "        self.lr = self.args['lr']\n",
        "        self.gamma = self.args['gamma']\n",
        "        self.epoch = self.args['epoch']\n",
        "        self.gorder = self.args['gorder']\n",
        "        self.dim = self.args['dim']\n",
        "        self.resume = self.args['resume']\n",
        "        self.data_dir = self.args['data_dir']\n",
        "        self.dataset = self.args['dataset']\n",
        "        self.raw_data_dir = self.args['raw_data_dir']\n",
        "        self.model = self.args['model']\n",
        "\n",
        "\n",
        "    def get_args(self):\n",
        "        return self.args\n",
        "args = ArgumentParser()\n",
        "torch.manual_seed(args.seed)\n",
        "np.random.seed(args.seed)\n",
        "if 'hdc' in args.model:\n",
        "    args.gorder = 2\n",
        "    print(\"Use binary HDC with random fourier features, ignoring gorder, set to 2.\")\n",
        "args.data_dir = f'{args.data_dir}/{args.dataset}_{args.model}_order{args.gorder}_gamma{args.gamma}_dim{args.dim}'\n",
        "try:\n",
        "    os.makedirs(args.data_dir)\n",
        "except FileExistsError:\n",
        "    print('Encoded data folder already exists')\n",
        "if not args.resume:\n",
        "    print('Encode the dataset into hypervectors and save')\n",
        "    encode_and_save(args)\n",
        "    print('Finish encoding and saving')\n",
        "print(f'Optimizing class representatives for {args.epoch} epochs')\n",
        "train(args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01ef1154-fe11-4f75-bb90-b2fc08676b6d",
        "id": "VsLVhOaNVn0F"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoded data folder already exists\n",
            "Optimizing class representatives for 10 epochs\n",
            "Loading encoded training data...\n",
            "Loading encoded test data...\n",
            "Size of encoded training data torch.Size([6238, 10000]) and test data torch.Size([1559, 10000])\n",
            "Epoch: 1\n",
            "49 1.9322 68.75\n",
            "99 1.2453 68.75\n",
            "149 0.1989 100.0\n",
            "199 0.7047 81.25\n",
            "249 0.3389 100.0\n",
            "299 0.5219 87.5\n",
            "349 0.1699 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0590, Accuracy: 1355/1559 (86.91%)\n",
            "\n",
            "--- 2.1330695152282715 seconds ---\n",
            "Epoch: 2\n",
            "49 0.0041 100.0\n",
            "99 0.0106 100.0\n",
            "149 0.0087 100.0\n",
            "199 0.0040 100.0\n",
            "249 0.0053 100.0\n",
            "299 0.0116 100.0\n",
            "349 0.0018 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0491, Accuracy: 1365/1559 (87.56%)\n",
            "\n",
            "--- 1.9538180828094482 seconds ---\n",
            "Epoch: 3\n",
            "49 0.0031 100.0\n",
            "99 0.0015 100.0\n",
            "149 0.0028 100.0\n",
            "199 0.0028 100.0\n",
            "249 0.0006 100.0\n",
            "299 0.0019 100.0\n",
            "349 0.0018 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0443, Accuracy: 1382/1559 (88.65%)\n",
            "\n",
            "--- 1.9314517974853516 seconds ---\n",
            "Epoch: 4\n",
            "49 0.0011 100.0\n",
            "99 0.0016 100.0\n",
            "149 0.0025 100.0\n",
            "199 0.0015 100.0\n",
            "249 0.0013 100.0\n",
            "299 0.0010 100.0\n",
            "349 0.0008 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0407, Accuracy: 1381/1559 (88.58%)\n",
            "\n",
            "--- 1.9807045459747314 seconds ---\n",
            "Epoch: 5\n",
            "49 0.0014 100.0\n",
            "99 0.0007 100.0\n",
            "149 0.0009 100.0\n",
            "199 0.0010 100.0\n",
            "249 0.0010 100.0\n",
            "299 0.0016 100.0\n",
            "349 0.0011 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0381, Accuracy: 1383/1559 (88.71%)\n",
            "\n",
            "--- 1.940675973892212 seconds ---\n",
            "Epoch: 6\n",
            "49 0.0006 100.0\n",
            "99 0.0010 100.0\n",
            "149 0.0008 100.0\n",
            "199 0.0006 100.0\n",
            "249 0.0006 100.0\n",
            "299 0.0004 100.0\n",
            "349 0.0006 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0359, Accuracy: 1391/1559 (89.22%)\n",
            "\n",
            "--- 2.807755470275879 seconds ---\n",
            "Epoch: 7\n",
            "49 0.0004 100.0\n",
            "99 0.0003 100.0\n",
            "149 0.0004 100.0\n",
            "199 0.0004 100.0\n",
            "249 0.0006 100.0\n",
            "299 0.0005 100.0\n",
            "349 0.0003 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0339, Accuracy: 1395/1559 (89.48%)\n",
            "\n",
            "--- 2.0732920169830322 seconds ---\n",
            "Epoch: 8\n",
            "49 0.0006 100.0\n",
            "99 0.0004 100.0\n",
            "149 0.0002 100.0\n",
            "199 0.0004 100.0\n",
            "249 0.0002 100.0\n",
            "299 0.0004 100.0\n",
            "349 0.0003 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0326, Accuracy: 1392/1559 (89.29%)\n",
            "\n",
            "--- 1.9635717868804932 seconds ---\n",
            "Epoch: 9\n",
            "49 0.0003 100.0\n",
            "99 0.0002 100.0\n",
            "149 0.0002 100.0\n",
            "199 0.0002 100.0\n",
            "249 0.0002 100.0\n",
            "299 0.0001 100.0\n",
            "349 0.0002 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0312, Accuracy: 1398/1559 (89.67%)\n",
            "\n",
            "--- 1.9985921382904053 seconds ---\n",
            "Epoch: 10\n",
            "49 0.0002 100.0\n",
            "99 0.0001 100.0\n",
            "149 0.0001 100.0\n",
            "199 0.0002 100.0\n",
            "249 0.0001 100.0\n",
            "299 0.0002 100.0\n",
            "349 0.0002 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0301, Accuracy: 1402/1559 (89.93%)\n",
            "\n",
            "--- 1.9804143905639648 seconds ---\n"
          ]
        }
      ],
      "source": [
        "#isolet epoch 10 rffg4vsa\n",
        "class ArgumentParser:\n",
        "    def __init__(self):\n",
        "        self.args = {\n",
        "            'lr': 0.01,            # Learning rate for optimizing class representative\n",
        "            'gamma': 0.3,          # Kernel parameter for computing covariance\n",
        "            'epoch': 10,            # Epochs of training\n",
        "            'gorder': 4,           # Order of the cyclic group required for G-VSA (8 ou 16)\n",
        "            'dim': 10000,          # Dimension of hypervectors c le grand D\n",
        "            'seed': 43,            # Random seed for reproducing results\n",
        "            'resume': True,       # Resume from existing encoded hypervectors\n",
        "            'data_dir': './encoded_data', # Directory to save encoded data (hypervectors)\n",
        "            'dataset': 'isolet',    # Dataset choice choices=['mnist', 'fmnist', 'cifar', 'isolet', 'ucihar']\n",
        "            'raw_data_dir': './dataset',   # Raw data directory\n",
        "            'model': 'rff-gvsa'    # Model type choices=['rff-hdc', 'linear-hdc', 'rff-gvsa']\n",
        "        }\n",
        "        self.seed = self.args['seed']\n",
        "        self.lr = self.args['lr']\n",
        "        self.gamma = self.args['gamma']\n",
        "        self.epoch = self.args['epoch']\n",
        "        self.gorder = self.args['gorder']\n",
        "        self.dim = self.args['dim']\n",
        "        self.resume = self.args['resume']\n",
        "        self.data_dir = self.args['data_dir']\n",
        "        self.dataset = self.args['dataset']\n",
        "        self.raw_data_dir = self.args['raw_data_dir']\n",
        "        self.model = self.args['model']\n",
        "\n",
        "\n",
        "    def get_args(self):\n",
        "        return self.args\n",
        "args = ArgumentParser()\n",
        "torch.manual_seed(args.seed)\n",
        "np.random.seed(args.seed)\n",
        "if 'hdc' in args.model:\n",
        "    args.gorder = 2\n",
        "    print(\"Use binary HDC with random fourier features, ignoring gorder, set to 2.\")\n",
        "args.data_dir = f'{args.data_dir}/{args.dataset}_{args.model}_order{args.gorder}_gamma{args.gamma}_dim{args.dim}'\n",
        "try:\n",
        "    os.makedirs(args.data_dir)\n",
        "except FileExistsError:\n",
        "    print('Encoded data folder already exists')\n",
        "if not args.resume:\n",
        "    print('Encode the dataset into hypervectors and save')\n",
        "    encode_and_save(args)\n",
        "    print('Finish encoding and saving')\n",
        "print(f'Optimizing class representatives for {args.epoch} epochs')\n",
        "train(args)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## UCIHAR"
      ],
      "metadata": {
        "id": "fPHUS_f9Vn0G"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba31fa75-f08a-43e6-fec5-9ea246d223eb",
        "id": "Hr5PP_5aVn0G"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encode the dataset into hypervectors and save\n",
            "# of channels of data 1\n",
            "# of training samples and test samples 7352 2947\n",
            "Encoding with random fourier features encoder.\n",
            "the threshold to discretize fourier features to group elements tensor([-0.6746,  0.0000,  0.6746])\n",
            "Encoded pixels to hypervectors with size:  torch.Size([256, 10000])\n",
            "Encoding training data...\n",
            "Start encoding data\n",
            "Finish encoding data\n",
            "Encoding test data...\n",
            "Start encoding data\n",
            "Finish encoding data\n",
            "Finish encoding and saving\n",
            "Optimizing class representatives for 1 epochs\n",
            "Loading encoded training data...\n",
            "Loading encoded test data...\n",
            "Size of encoded training data torch.Size([7352, 10000]) and test data torch.Size([2947, 10000])\n",
            "Epoch: 1\n",
            "49 0.4102 87.5\n",
            "99 0.0826 100.0\n",
            "149 0.0697 100.0\n",
            "199 0.0237 100.0\n",
            "249 0.0495 100.0\n",
            "299 0.1174 93.75\n",
            "349 0.0587 100.0\n",
            "399 0.0211 100.0\n",
            "449 0.1339 93.75\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0350, Accuracy: 2671/2947 (90.63%)\n",
            "\n",
            "--- 3.3710241317749023 seconds ---\n"
          ]
        }
      ],
      "source": [
        "#ucihar epoch 1 rffg4vsa\n",
        "class ArgumentParser:\n",
        "    def __init__(self):\n",
        "        self.args = {\n",
        "            'lr': 0.01,            # Learning rate for optimizing class representative\n",
        "            'gamma': 0.3,          # Kernel parameter for computing covariance\n",
        "            'epoch': 1,            # Epochs of training\n",
        "            'gorder': 4,           # Order of the cyclic group required for G-VSA (8 ou 16)\n",
        "            'dim': 10000,          # Dimension of hypervectors c le grand D\n",
        "            'seed': 43,            # Random seed for reproducing results\n",
        "            'resume': False,       # Resume from existing encoded hypervectors\n",
        "            'data_dir': './encoded_data', # Directory to save encoded data (hypervectors)\n",
        "            'dataset': 'ucihar',    # Dataset choice choices=['mnist', 'fmnist', 'cifar', 'isolet', 'ucihar']\n",
        "            'raw_data_dir': './dataset',   # Raw data directory\n",
        "            'model': 'rff-gvsa'    # Model type choices=['rff-hdc', 'linear-hdc', 'rff-gvsa']\n",
        "        }\n",
        "        self.seed = self.args['seed']\n",
        "        self.lr = self.args['lr']\n",
        "        self.gamma = self.args['gamma']\n",
        "        self.epoch = self.args['epoch']\n",
        "        self.gorder = self.args['gorder']\n",
        "        self.dim = self.args['dim']\n",
        "        self.resume = self.args['resume']\n",
        "        self.data_dir = self.args['data_dir']\n",
        "        self.dataset = self.args['dataset']\n",
        "        self.raw_data_dir = self.args['raw_data_dir']\n",
        "        self.model = self.args['model']\n",
        "\n",
        "\n",
        "    def get_args(self):\n",
        "        return self.args\n",
        "args = ArgumentParser()\n",
        "torch.manual_seed(args.seed)\n",
        "np.random.seed(args.seed)\n",
        "if 'hdc' in args.model:\n",
        "    args.gorder = 2\n",
        "    print(\"Use binary HDC with random fourier features, ignoring gorder, set to 2.\")\n",
        "args.data_dir = f'{args.data_dir}/{args.dataset}_{args.model}_order{args.gorder}_gamma{args.gamma}_dim{args.dim}'\n",
        "try:\n",
        "    os.makedirs(args.data_dir)\n",
        "except FileExistsError:\n",
        "    print('Encoded data folder already exists')\n",
        "if not args.resume:\n",
        "    print('Encode the dataset into hypervectors and save')\n",
        "    encode_and_save(args)\n",
        "    print('Finish encoding and saving')\n",
        "print(f'Optimizing class representatives for {args.epoch} epochs')\n",
        "train(args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4493809-3034-492f-c093-e721e69c3d29",
        "id": "qow9MQNEVn0H"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoded data folder already exists\n",
            "Optimizing class representatives for 10 epochs\n",
            "Loading encoded training data...\n",
            "Loading encoded test data...\n",
            "Size of encoded training data torch.Size([7352, 10000]) and test data torch.Size([2947, 10000])\n",
            "Epoch: 1\n",
            "49 0.2205 93.75\n",
            "99 0.0896 100.0\n",
            "149 0.1381 93.75\n",
            "199 0.1145 93.75\n",
            "249 0.0700 93.75\n",
            "299 0.0570 100.0\n",
            "349 0.0749 93.75\n",
            "399 0.0180 100.0\n",
            "449 0.0863 93.75\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0341, Accuracy: 2733/2947 (92.74%)\n",
            "\n",
            "--- 2.577092170715332 seconds ---\n",
            "Epoch: 2\n",
            "49 0.0062 100.0\n",
            "99 0.0227 100.0\n",
            "149 0.0161 100.0\n",
            "199 0.0069 100.0\n",
            "249 0.0041 100.0\n",
            "299 0.0197 100.0\n",
            "349 0.0089 100.0\n",
            "399 0.0008 100.0\n",
            "449 0.0034 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0189, Accuracy: 2766/2947 (93.86%)\n",
            "\n",
            "--- 2.490126132965088 seconds ---\n",
            "Epoch: 3\n",
            "49 0.0008 100.0\n",
            "99 0.0004 100.0\n",
            "149 0.0006 100.0\n",
            "199 0.0220 100.0\n",
            "249 0.0008 100.0\n",
            "299 0.0001 100.0\n",
            "349 0.0006 100.0\n",
            "399 0.0018 100.0\n",
            "449 0.0143 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0148, Accuracy: 2770/2947 (93.99%)\n",
            "\n",
            "--- 2.520430326461792 seconds ---\n",
            "Epoch: 4\n",
            "49 0.0027 100.0\n",
            "99 0.0003 100.0\n",
            "149 0.0017 100.0\n",
            "199 0.0004 100.0\n",
            "249 0.0003 100.0\n",
            "299 0.0007 100.0\n",
            "349 0.0002 100.0\n",
            "399 0.0001 100.0\n",
            "449 0.0007 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0131, Accuracy: 2773/2947 (94.10%)\n",
            "\n",
            "--- 2.957874059677124 seconds ---\n",
            "Epoch: 5\n",
            "49 0.0000 100.0\n",
            "99 0.0002 100.0\n",
            "149 0.0001 100.0\n",
            "199 0.0003 100.0\n",
            "249 0.0003 100.0\n",
            "299 0.0008 100.0\n",
            "349 0.0012 100.0\n",
            "399 0.0006 100.0\n",
            "449 0.0013 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0127, Accuracy: 2771/2947 (94.03%)\n",
            "\n",
            "--- 2.995058298110962 seconds ---\n",
            "Epoch: 6\n",
            "49 0.0002 100.0\n",
            "99 0.0001 100.0\n",
            "149 0.0001 100.0\n",
            "199 0.0002 100.0\n",
            "249 0.0002 100.0\n",
            "299 0.0007 100.0\n",
            "349 0.0002 100.0\n",
            "399 0.0004 100.0\n",
            "449 0.0002 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0117, Accuracy: 2789/2947 (94.64%)\n",
            "\n",
            "--- 2.532350778579712 seconds ---\n",
            "Epoch: 7\n",
            "49 0.0002 100.0\n",
            "99 0.0002 100.0\n",
            "149 0.0002 100.0\n",
            "199 0.0008 100.0\n",
            "249 0.0001 100.0\n",
            "299 0.0002 100.0\n",
            "349 0.0001 100.0\n",
            "399 0.0001 100.0\n",
            "449 0.0001 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0113, Accuracy: 2788/2947 (94.60%)\n",
            "\n",
            "--- 2.512664318084717 seconds ---\n",
            "Epoch: 8\n",
            "49 0.0002 100.0\n",
            "99 0.0000 100.0\n",
            "149 0.0004 100.0\n",
            "199 0.0004 100.0\n",
            "249 0.0003 100.0\n",
            "299 0.0001 100.0\n",
            "349 0.0000 100.0\n",
            "399 0.0004 100.0\n",
            "449 0.0001 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0109, Accuracy: 2794/2947 (94.81%)\n",
            "\n",
            "--- 2.504828453063965 seconds ---\n",
            "Epoch: 9\n",
            "49 0.0004 100.0\n",
            "99 0.0002 100.0\n",
            "149 0.0002 100.0\n",
            "199 0.0001 100.0\n",
            "249 0.0000 100.0\n",
            "299 0.0002 100.0\n",
            "349 0.0001 100.0\n",
            "399 0.0001 100.0\n",
            "449 0.0000 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0104, Accuracy: 2796/2947 (94.88%)\n",
            "\n",
            "--- 3.300791025161743 seconds ---\n",
            "Epoch: 10\n",
            "49 0.0000 100.0\n",
            "99 0.0000 100.0\n",
            "149 0.0000 100.0\n",
            "199 0.0000 100.0\n",
            "249 0.0000 100.0\n",
            "299 0.0002 100.0\n",
            "349 0.0000 100.0\n",
            "399 0.0000 100.0\n",
            "449 0.0001 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0106, Accuracy: 2784/2947 (94.47%)\n",
            "\n",
            "--- 2.6052260398864746 seconds ---\n"
          ]
        }
      ],
      "source": [
        "#ucihar epoch 10 rffg4vsa\n",
        "class ArgumentParser:\n",
        "    def __init__(self):\n",
        "        self.args = {\n",
        "            'lr': 0.01,            # Learning rate for optimizing class representative\n",
        "            'gamma': 0.3,          # Kernel parameter for computing covariance\n",
        "            'epoch': 10,            # Epochs of training\n",
        "            'gorder': 4,           # Order of the cyclic group required for G-VSA (8 ou 16)\n",
        "            'dim': 10000,          # Dimension of hypervectors c le grand D\n",
        "            'seed': 43,            # Random seed for reproducing results\n",
        "            'resume': True,       # Resume from existing encoded hypervectors\n",
        "            'data_dir': './encoded_data', # Directory to save encoded data (hypervectors)\n",
        "            'dataset': 'ucihar',    # Dataset choice choices=['mnist', 'fmnist', 'cifar', 'isolet', 'ucihar']\n",
        "            'raw_data_dir': './dataset',   # Raw data directory\n",
        "            'model': 'rff-gvsa'    # Model type choices=['rff-hdc', 'linear-hdc', 'rff-gvsa']\n",
        "        }\n",
        "        self.seed = self.args['seed']\n",
        "        self.lr = self.args['lr']\n",
        "        self.gamma = self.args['gamma']\n",
        "        self.epoch = self.args['epoch']\n",
        "        self.gorder = self.args['gorder']\n",
        "        self.dim = self.args['dim']\n",
        "        self.resume = self.args['resume']\n",
        "        self.data_dir = self.args['data_dir']\n",
        "        self.dataset = self.args['dataset']\n",
        "        self.raw_data_dir = self.args['raw_data_dir']\n",
        "        self.model = self.args['model']\n",
        "\n",
        "\n",
        "    def get_args(self):\n",
        "        return self.args\n",
        "args = ArgumentParser()\n",
        "torch.manual_seed(args.seed)\n",
        "np.random.seed(args.seed)\n",
        "if 'hdc' in args.model:\n",
        "    args.gorder = 2\n",
        "    print(\"Use binary HDC with random fourier features, ignoring gorder, set to 2.\")\n",
        "args.data_dir = f'{args.data_dir}/{args.dataset}_{args.model}_order{args.gorder}_gamma{args.gamma}_dim{args.dim}'\n",
        "try:\n",
        "    os.makedirs(args.data_dir)\n",
        "except FileExistsError:\n",
        "    print('Encoded data folder already exists')\n",
        "if not args.resume:\n",
        "    print('Encode the dataset into hypervectors and save')\n",
        "    encode_and_save(args)\n",
        "    print('Finish encoding and saving')\n",
        "print(f'Optimizing class representatives for {args.epoch} epochs')\n",
        "train(args)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MNIST"
      ],
      "metadata": {
        "id": "tME7dZOaVn0H"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7db0022f-4fd0-47f0-94b3-0d0098c3a0df",
        "id": "S7mJFu1UVn0I"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encode the dataset into hypervectors and save\n",
            "# of channels of data 1\n",
            "# of training samples and test samples 60000 10000\n",
            "Encoding with random fourier features encoder.\n",
            "the threshold to discretize fourier features to group elements tensor([-0.6746,  0.0000,  0.6746])\n",
            "Encoded pixels to hypervectors with size:  torch.Size([256, 10000])\n",
            "Encoding training data...\n",
            "Start encoding data\n",
            "12800 images encoded. Total time elapse = 22.603414297103882\n",
            "25600 images encoded. Total time elapse = 45.186030626297\n",
            "38400 images encoded. Total time elapse = 67.87335205078125\n",
            "51200 images encoded. Total time elapse = 90.62863206863403\n",
            "Finish encoding data\n",
            "Encoding test data...\n",
            "Start encoding data\n",
            "Finish encoding data\n",
            "Finish encoding and saving\n",
            "Optimizing class representatives for 1 epochs\n",
            "Loading encoded training data...\n",
            "Loading encoded test data...\n",
            "Size of encoded training data torch.Size([60000, 10000]) and test data torch.Size([10000, 10000])\n",
            "Epoch: 1\n",
            "49 1.3103 93.75\n",
            "99 0.8486 75.0\n",
            "149 0.3778 93.75\n",
            "199 0.4051 93.75\n",
            "249 0.3764 93.75\n",
            "299 0.3305 93.75\n",
            "349 0.6430 75.0\n",
            "399 0.4489 93.75\n",
            "449 0.6036 75.0\n",
            "499 0.3907 81.25\n",
            "549 0.2534 93.75\n",
            "599 0.2487 87.5\n",
            "649 0.5045 75.0\n",
            "699 0.1319 93.75\n",
            "749 0.2424 87.5\n",
            "799 1.0916 75.0\n",
            "849 0.0990 100.0\n",
            "899 0.2526 93.75\n",
            "949 0.3453 93.75\n",
            "999 0.2915 87.5\n",
            "1049 0.4880 93.75\n",
            "1099 0.2896 87.5\n",
            "1149 0.2810 93.75\n",
            "1199 0.5220 87.5\n",
            "1249 0.1043 100.0\n",
            "1299 0.1402 93.75\n",
            "1349 0.2844 87.5\n",
            "1399 0.0759 100.0\n",
            "1449 0.1358 100.0\n",
            "1499 0.1239 100.0\n",
            "1549 0.2509 93.75\n",
            "1599 0.2275 93.75\n",
            "1649 0.2117 93.75\n",
            "1699 0.5701 87.5\n",
            "1749 0.1977 87.5\n",
            "1799 0.4833 87.5\n",
            "1849 0.4902 81.25\n",
            "1899 0.1570 100.0\n",
            "1949 0.7150 81.25\n",
            "1999 0.1150 93.75\n",
            "2049 0.8407 87.5\n",
            "2099 0.3249 87.5\n",
            "2149 0.0299 100.0\n",
            "2199 0.0976 100.0\n",
            "2249 0.6323 87.5\n",
            "2299 0.5224 87.5\n",
            "2349 0.0827 100.0\n",
            "2399 0.2353 93.75\n",
            "2449 0.2878 93.75\n",
            "2499 0.0373 100.0\n",
            "2549 0.1896 93.75\n",
            "2599 0.0372 100.0\n",
            "2649 0.3915 87.5\n",
            "2699 0.0842 93.75\n",
            "2749 0.2020 93.75\n",
            "2799 0.0237 100.0\n",
            "2849 0.2305 93.75\n",
            "2899 0.3351 93.75\n",
            "2949 0.3754 81.25\n",
            "2999 0.2009 87.5\n",
            "3049 0.0353 100.0\n",
            "3099 0.1243 93.75\n",
            "3149 0.0283 100.0\n",
            "3199 0.2468 93.75\n",
            "3249 0.1800 93.75\n",
            "3299 0.3872 75.0\n",
            "3349 0.5207 75.0\n",
            "3399 0.2072 93.75\n",
            "3449 0.1125 93.75\n",
            "3499 0.1172 93.75\n",
            "3549 0.0837 100.0\n",
            "3599 0.0392 100.0\n",
            "3649 0.2145 93.75\n",
            "3699 0.0249 100.0\n",
            "3749 0.0218 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0177, Accuracy: 9128/10000 (91.28%)\n",
            "\n",
            "--- 17.624178171157837 seconds ---\n"
          ]
        }
      ],
      "source": [
        "#mnist epoch 1 rffg4vsa\n",
        "class ArgumentParser:\n",
        "    def __init__(self):\n",
        "        self.args = {\n",
        "            'lr': 0.01,            # Learning rate for optimizing class representative\n",
        "            'gamma': 0.3,          # Kernel parameter for computing covariance\n",
        "            'epoch': 1,            # Epochs of training\n",
        "            'gorder': 4,           # Order of the cyclic group required for G-VSA (8 ou 16)\n",
        "            'dim': 10000,          # Dimension of hypervectors c le grand D\n",
        "            'seed': 43,            # Random seed for reproducing results\n",
        "            'resume': False,       # Resume from existing encoded hypervectors\n",
        "            'data_dir': './encoded_data', # Directory to save encoded data (hypervectors)\n",
        "            'dataset': 'mnist',    # Dataset choice choices=['mnist', 'fmnist', 'cifar', 'isolet', 'ucihar']\n",
        "            'raw_data_dir': './dataset',   # Raw data directory\n",
        "            'model': 'rff-gvsa'    # Model type choices=['rff-hdc', 'linear-hdc', 'rff-gvsa']\n",
        "        }\n",
        "        self.seed = self.args['seed']\n",
        "        self.lr = self.args['lr']\n",
        "        self.gamma = self.args['gamma']\n",
        "        self.epoch = self.args['epoch']\n",
        "        self.gorder = self.args['gorder']\n",
        "        self.dim = self.args['dim']\n",
        "        self.resume = self.args['resume']\n",
        "        self.data_dir = self.args['data_dir']\n",
        "        self.dataset = self.args['dataset']\n",
        "        self.raw_data_dir = self.args['raw_data_dir']\n",
        "        self.model = self.args['model']\n",
        "\n",
        "\n",
        "    def get_args(self):\n",
        "        return self.args\n",
        "args = ArgumentParser()\n",
        "torch.manual_seed(args.seed)\n",
        "np.random.seed(args.seed)\n",
        "if 'hdc' in args.model:\n",
        "    args.gorder = 2\n",
        "    print(\"Use binary HDC with random fourier features, ignoring gorder, set to 2.\")\n",
        "args.data_dir = f'{args.data_dir}/{args.dataset}_{args.model}_order{args.gorder}_gamma{args.gamma}_dim{args.dim}'\n",
        "try:\n",
        "    os.makedirs(args.data_dir)\n",
        "except FileExistsError:\n",
        "    print('Encoded data folder already exists')\n",
        "if not args.resume:\n",
        "    print('Encode the dataset into hypervectors and save')\n",
        "    encode_and_save(args)\n",
        "    print('Finish encoding and saving')\n",
        "print(f'Optimizing class representatives for {args.epoch} epochs')\n",
        "train(args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc31b42a-9d91-48d1-c0b8-c10dc4612fc7",
        "id": "OaWf5PH3Vn0I"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoded data folder already exists\n",
            "Optimizing class representatives for 10 epochs\n",
            "Loading encoded training data...\n",
            "Loading encoded test data...\n",
            "Size of encoded training data torch.Size([60000, 10000]) and test data torch.Size([10000, 10000])\n",
            "Epoch: 1\n",
            "49 1.7337 56.25\n",
            "99 0.7384 81.25\n",
            "149 0.6409 87.5\n",
            "199 0.3822 93.75\n",
            "249 0.1441 100.0\n",
            "299 0.2714 93.75\n",
            "349 0.6173 75.0\n",
            "399 0.4280 81.25\n",
            "449 0.3399 93.75\n",
            "499 0.5066 81.25\n",
            "549 0.2266 87.5\n",
            "599 0.7567 81.25\n",
            "649 0.1944 93.75\n",
            "699 0.2545 93.75\n",
            "749 0.0634 100.0\n",
            "799 0.6474 68.75\n",
            "849 0.3484 87.5\n",
            "899 0.4407 81.25\n",
            "949 0.1556 93.75\n",
            "999 0.1685 93.75\n",
            "1049 0.1616 93.75\n",
            "1099 0.3389 93.75\n",
            "1149 0.4818 87.5\n",
            "1199 0.0472 100.0\n",
            "1249 0.0362 100.0\n",
            "1299 0.6640 87.5\n",
            "1349 0.2134 87.5\n",
            "1399 0.3297 87.5\n",
            "1449 0.0425 100.0\n",
            "1499 0.1942 100.0\n",
            "1549 0.0748 93.75\n",
            "1599 0.1170 93.75\n",
            "1649 0.3775 87.5\n",
            "1699 0.0483 100.0\n",
            "1749 0.2319 93.75\n",
            "1799 0.1033 100.0\n",
            "1849 0.1058 93.75\n",
            "1899 0.2278 93.75\n",
            "1949 0.2042 87.5\n",
            "1999 0.0881 100.0\n",
            "2049 0.1311 93.75\n",
            "2099 0.4273 75.0\n",
            "2149 0.0866 93.75\n",
            "2199 0.0362 100.0\n",
            "2249 0.3362 93.75\n",
            "2299 0.4166 93.75\n",
            "2349 0.0127 100.0\n",
            "2399 0.0456 100.0\n",
            "2449 0.2545 93.75\n",
            "2499 0.0361 100.0\n",
            "2549 0.0983 100.0\n",
            "2599 0.0924 100.0\n",
            "2649 0.1280 93.75\n",
            "2699 0.3004 93.75\n",
            "2749 0.2854 87.5\n",
            "2799 0.0536 100.0\n",
            "2849 0.0671 100.0\n",
            "2899 0.0299 100.0\n",
            "2949 0.5561 87.5\n",
            "2999 0.5050 81.25\n",
            "3049 0.0943 93.75\n",
            "3099 0.4083 93.75\n",
            "3149 0.2515 93.75\n",
            "3199 0.4715 87.5\n",
            "3249 0.0970 93.75\n",
            "3299 0.3812 75.0\n",
            "3349 0.3053 93.75\n",
            "3399 0.1297 93.75\n",
            "3449 0.1009 100.0\n",
            "3499 0.1671 93.75\n",
            "3549 0.0204 100.0\n",
            "3599 0.1910 87.5\n",
            "3649 0.4424 81.25\n",
            "3699 0.2859 93.75\n",
            "3749 0.0833 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0182, Accuracy: 9091/10000 (90.91%)\n",
            "\n",
            "--- 17.228752613067627 seconds ---\n",
            "Epoch: 2\n",
            "49 0.0147 100.0\n",
            "99 0.0504 100.0\n",
            "149 0.0072 100.0\n",
            "199 0.0296 100.0\n",
            "249 0.1970 93.75\n",
            "299 0.0106 100.0\n",
            "349 0.0388 100.0\n",
            "399 0.0024 100.0\n",
            "449 0.0087 100.0\n",
            "499 0.0136 100.0\n",
            "549 0.0651 100.0\n",
            "599 0.1898 93.75\n",
            "649 0.0031 100.0\n",
            "699 0.0135 100.0\n",
            "749 0.1361 93.75\n",
            "799 0.0019 100.0\n",
            "849 0.0079 100.0\n",
            "899 0.0020 100.0\n",
            "949 0.0234 100.0\n",
            "999 0.0125 100.0\n",
            "1049 0.0105 100.0\n",
            "1099 0.0025 100.0\n",
            "1149 0.0483 93.75\n",
            "1199 0.0037 100.0\n",
            "1249 0.0924 93.75\n",
            "1299 0.0022 100.0\n",
            "1349 0.0054 100.0\n",
            "1399 0.0120 100.0\n",
            "1449 0.0312 100.0\n",
            "1499 0.0007 100.0\n",
            "1549 0.0082 100.0\n",
            "1599 0.0139 100.0\n",
            "1649 0.0005 100.0\n",
            "1699 0.1229 93.75\n",
            "1749 0.0011 100.0\n",
            "1799 0.0038 100.0\n",
            "1849 0.0734 93.75\n",
            "1899 0.0064 100.0\n",
            "1949 0.1946 93.75\n",
            "1999 0.1207 93.75\n",
            "2049 0.1058 93.75\n",
            "2099 0.0674 93.75\n",
            "2149 0.0346 100.0\n",
            "2199 0.0008 100.0\n",
            "2249 0.0126 100.0\n",
            "2299 0.0765 100.0\n",
            "2349 0.0928 93.75\n",
            "2399 0.4960 87.5\n",
            "2449 0.0227 100.0\n",
            "2499 0.3308 93.75\n",
            "2549 0.1565 93.75\n",
            "2599 0.0421 100.0\n",
            "2649 0.0601 100.0\n",
            "2699 0.1517 93.75\n",
            "2749 0.0740 100.0\n",
            "2799 0.3141 81.25\n",
            "2849 0.3429 93.75\n",
            "2899 0.0524 100.0\n",
            "2949 0.0667 93.75\n",
            "2999 0.0380 100.0\n",
            "3049 0.0159 100.0\n",
            "3099 0.0772 100.0\n",
            "3149 0.0131 100.0\n",
            "3199 0.4683 87.5\n",
            "3249 0.0383 100.0\n",
            "3299 0.4091 87.5\n",
            "3349 0.0248 100.0\n",
            "3399 0.3103 87.5\n",
            "3449 0.0180 100.0\n",
            "3499 0.0047 100.0\n",
            "3549 0.0645 100.0\n",
            "3599 0.2214 93.75\n",
            "3649 0.0054 100.0\n",
            "3699 0.1929 93.75\n",
            "3749 0.0203 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0231, Accuracy: 8968/10000 (89.68%)\n",
            "\n",
            "--- 17.17199444770813 seconds ---\n",
            "Epoch: 3\n",
            "49 0.0262 100.0\n",
            "99 0.0512 100.0\n",
            "149 0.0275 100.0\n",
            "199 0.0355 100.0\n",
            "249 0.0139 100.0\n",
            "299 0.0012 100.0\n",
            "349 0.0303 100.0\n",
            "399 0.0036 100.0\n",
            "449 0.0002 100.0\n",
            "499 0.0101 100.0\n",
            "549 0.0048 100.0\n",
            "599 0.0799 93.75\n",
            "649 0.0557 100.0\n",
            "699 0.0010 100.0\n",
            "749 0.1217 93.75\n",
            "799 0.0012 100.0\n",
            "849 0.0523 100.0\n",
            "899 0.0067 100.0\n",
            "949 0.0003 100.0\n",
            "999 0.0005 100.0\n",
            "1049 0.0278 100.0\n",
            "1099 0.2797 93.75\n",
            "1149 0.0073 100.0\n",
            "1199 0.1113 93.75\n",
            "1249 0.0408 100.0\n",
            "1299 0.3119 93.75\n",
            "1349 0.2443 87.5\n",
            "1399 0.0532 93.75\n",
            "1449 0.0028 100.0\n",
            "1499 0.0004 100.0\n",
            "1549 0.0410 100.0\n",
            "1599 0.0593 100.0\n",
            "1649 0.0394 100.0\n",
            "1699 0.3830 93.75\n",
            "1749 0.1285 93.75\n",
            "1799 0.0495 93.75\n",
            "1849 0.2555 93.75\n",
            "1899 0.1109 93.75\n",
            "1949 0.0071 100.0\n",
            "1999 0.0764 93.75\n",
            "2049 0.0052 100.0\n",
            "2099 0.3610 87.5\n",
            "2149 0.3142 93.75\n",
            "2199 0.0311 100.0\n",
            "2249 0.0007 100.0\n",
            "2299 0.0020 100.0\n",
            "2349 0.1900 93.75\n",
            "2399 0.0279 100.0\n",
            "2449 0.0155 100.0\n",
            "2499 0.0004 100.0\n",
            "2549 0.1315 93.75\n",
            "2599 0.0346 100.0\n",
            "2649 0.0671 100.0\n",
            "2699 0.1568 93.75\n",
            "2749 0.0092 100.0\n",
            "2799 0.0048 100.0\n",
            "2849 0.0154 100.0\n",
            "2899 0.0674 93.75\n",
            "2949 0.0030 100.0\n",
            "2999 0.3407 87.5\n",
            "3049 0.1653 93.75\n",
            "3099 0.0475 100.0\n",
            "3149 0.0058 100.0\n",
            "3199 0.0657 100.0\n",
            "3249 0.1446 93.75\n",
            "3299 0.2799 93.75\n",
            "3349 0.0025 100.0\n",
            "3399 0.0653 100.0\n",
            "3449 0.0067 100.0\n",
            "3499 0.0131 100.0\n",
            "3549 0.0151 100.0\n",
            "3599 0.0121 100.0\n",
            "3649 0.0022 100.0\n",
            "3699 0.1049 93.75\n",
            "3749 0.0023 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0229, Accuracy: 9035/10000 (90.35%)\n",
            "\n",
            "--- 17.991681814193726 seconds ---\n",
            "Epoch: 4\n",
            "49 0.0031 100.0\n",
            "99 0.0003 100.0\n",
            "149 0.0076 100.0\n",
            "199 0.0149 100.0\n",
            "249 0.0067 100.0\n",
            "299 0.0070 100.0\n",
            "349 0.0125 100.0\n",
            "399 0.1332 93.75\n",
            "449 0.0087 100.0\n",
            "499 0.0178 100.0\n",
            "549 0.0255 100.0\n",
            "599 0.0272 100.0\n",
            "649 0.0097 100.0\n",
            "699 0.0065 100.0\n",
            "749 0.0136 100.0\n",
            "799 0.3273 87.5\n",
            "849 0.0391 100.0\n",
            "899 0.0035 100.0\n",
            "949 0.0019 100.0\n",
            "999 0.0234 100.0\n",
            "1049 0.0024 100.0\n",
            "1099 0.0164 100.0\n",
            "1149 0.0068 100.0\n",
            "1199 0.0518 93.75\n",
            "1249 0.0059 100.0\n",
            "1299 0.0011 100.0\n",
            "1349 0.0043 100.0\n",
            "1399 0.0240 100.0\n",
            "1449 0.0010 100.0\n",
            "1499 0.0148 100.0\n",
            "1549 0.0172 100.0\n",
            "1599 0.0078 100.0\n",
            "1649 0.0001 100.0\n",
            "1699 0.0474 100.0\n",
            "1749 0.1382 93.75\n",
            "1799 0.0053 100.0\n",
            "1849 0.1176 93.75\n",
            "1899 0.0015 100.0\n",
            "1949 0.1460 93.75\n",
            "1999 0.0389 100.0\n",
            "2049 0.1002 93.75\n",
            "2099 0.5200 93.75\n",
            "2149 0.0042 100.0\n",
            "2199 0.0326 100.0\n",
            "2249 0.0452 100.0\n",
            "2299 0.0151 100.0\n",
            "2349 0.0060 100.0\n",
            "2399 0.1438 93.75\n",
            "2449 0.0281 100.0\n",
            "2499 0.0351 100.0\n",
            "2549 0.1115 93.75\n",
            "2599 0.0712 93.75\n",
            "2649 0.0122 100.0\n",
            "2699 0.0463 93.75\n",
            "2749 0.0327 100.0\n",
            "2799 0.0129 100.0\n",
            "2849 0.0458 100.0\n",
            "2899 0.0165 100.0\n",
            "2949 0.1812 93.75\n",
            "2999 0.0597 93.75\n",
            "3049 0.0610 100.0\n",
            "3099 0.0558 100.0\n",
            "3149 0.2881 93.75\n",
            "3199 0.0016 100.0\n",
            "3249 0.5738 93.75\n",
            "3299 0.0078 100.0\n",
            "3349 0.0874 93.75\n",
            "3399 0.0028 100.0\n",
            "3449 0.0152 100.0\n",
            "3499 0.2919 93.75\n",
            "3549 0.0862 93.75\n",
            "3599 0.0268 100.0\n",
            "3649 0.0665 93.75\n",
            "3699 0.0152 100.0\n",
            "3749 0.2348 87.5\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0220, Accuracy: 9071/10000 (90.71%)\n",
            "\n",
            "--- 16.997119665145874 seconds ---\n",
            "Epoch: 5\n",
            "49 0.0177 100.0\n",
            "99 0.0070 100.0\n",
            "149 0.0047 100.0\n",
            "199 0.0187 100.0\n",
            "249 0.0031 100.0\n",
            "299 0.0034 100.0\n",
            "349 0.0131 100.0\n",
            "399 0.0022 100.0\n",
            "449 0.1095 93.75\n",
            "499 0.0002 100.0\n",
            "549 0.0017 100.0\n",
            "599 0.1102 93.75\n",
            "649 0.0676 93.75\n",
            "699 0.0135 100.0\n",
            "749 0.0172 100.0\n",
            "799 0.1145 93.75\n",
            "849 0.0223 100.0\n",
            "899 0.0967 93.75\n",
            "949 0.0033 100.0\n",
            "999 0.0058 100.0\n",
            "1049 0.0021 100.0\n",
            "1099 0.0015 100.0\n",
            "1149 0.0009 100.0\n",
            "1199 0.0057 100.0\n",
            "1249 0.0011 100.0\n",
            "1299 0.0272 100.0\n",
            "1349 0.0013 100.0\n",
            "1399 0.0166 100.0\n",
            "1449 0.0033 100.0\n",
            "1499 0.0120 100.0\n",
            "1549 0.0428 100.0\n",
            "1599 0.0346 100.0\n",
            "1649 0.1237 93.75\n",
            "1699 0.0020 100.0\n",
            "1749 0.0046 100.0\n",
            "1799 0.0172 100.0\n",
            "1849 0.0451 100.0\n",
            "1899 0.0002 100.0\n",
            "1949 0.1266 93.75\n",
            "1999 0.2318 93.75\n",
            "2049 0.0091 100.0\n",
            "2099 0.0071 100.0\n",
            "2149 0.1030 93.75\n",
            "2199 0.0012 100.0\n",
            "2249 0.2728 93.75\n",
            "2299 0.0890 100.0\n",
            "2349 0.0506 100.0\n",
            "2399 0.0014 100.0\n",
            "2449 0.0491 100.0\n",
            "2499 0.0924 93.75\n",
            "2549 0.0267 100.0\n",
            "2599 0.0791 93.75\n",
            "2649 0.0252 100.0\n",
            "2699 0.1244 93.75\n",
            "2749 0.0165 100.0\n",
            "2799 0.0011 100.0\n",
            "2849 0.3483 93.75\n",
            "2899 0.1765 93.75\n",
            "2949 0.1569 93.75\n",
            "2999 0.0039 100.0\n",
            "3049 0.0343 100.0\n",
            "3099 0.0236 100.0\n",
            "3149 0.2634 87.5\n",
            "3199 0.0001 100.0\n",
            "3249 0.6488 81.25\n",
            "3299 0.0141 100.0\n",
            "3349 0.0163 100.0\n",
            "3399 0.0035 100.0\n",
            "3449 0.0050 100.0\n",
            "3499 0.0585 93.75\n",
            "3549 0.1268 93.75\n",
            "3599 0.0204 100.0\n",
            "3649 0.0226 100.0\n",
            "3699 0.1972 93.75\n",
            "3749 0.0669 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0240, Accuracy: 9001/10000 (90.01%)\n",
            "\n",
            "--- 16.99953579902649 seconds ---\n",
            "Epoch: 6\n",
            "49 0.0341 100.0\n",
            "99 0.0013 100.0\n",
            "149 0.0007 100.0\n",
            "199 0.0034 100.0\n",
            "249 0.0210 100.0\n",
            "299 0.0075 100.0\n",
            "349 0.0010 100.0\n",
            "399 0.0113 100.0\n",
            "449 0.0038 100.0\n",
            "499 0.0213 100.0\n",
            "549 0.0092 100.0\n",
            "599 0.0042 100.0\n",
            "649 0.0022 100.0\n",
            "699 0.0021 100.0\n",
            "749 0.0107 100.0\n",
            "799 0.0015 100.0\n",
            "849 0.0104 100.0\n",
            "899 0.0061 100.0\n",
            "949 0.0501 100.0\n",
            "999 0.0867 93.75\n",
            "1049 0.0001 100.0\n",
            "1099 0.2932 93.75\n",
            "1149 0.0001 100.0\n",
            "1199 0.0332 100.0\n",
            "1249 0.0037 100.0\n",
            "1299 0.0138 100.0\n",
            "1349 0.0040 100.0\n",
            "1399 0.0601 93.75\n",
            "1449 0.0012 100.0\n",
            "1499 0.0021 100.0\n",
            "1549 0.0040 100.0\n",
            "1599 0.0077 100.0\n",
            "1649 0.0113 100.0\n",
            "1699 0.2229 87.5\n",
            "1749 0.0064 100.0\n",
            "1799 0.0460 100.0\n",
            "1849 0.1826 93.75\n",
            "1899 0.0049 100.0\n",
            "1949 0.0038 100.0\n",
            "1999 0.0021 100.0\n",
            "2049 0.0527 100.0\n",
            "2099 0.0411 100.0\n",
            "2149 0.0033 100.0\n",
            "2199 0.0019 100.0\n",
            "2249 0.0026 100.0\n",
            "2299 0.0462 100.0\n",
            "2349 0.0015 100.0\n",
            "2399 0.0803 93.75\n",
            "2449 0.0217 100.0\n",
            "2499 0.0443 100.0\n",
            "2549 0.0649 100.0\n",
            "2599 0.0221 100.0\n",
            "2649 0.0687 100.0\n",
            "2699 0.0476 100.0\n",
            "2749 0.0134 100.0\n",
            "2799 0.0163 100.0\n",
            "2849 0.0136 100.0\n",
            "2899 0.0332 100.0\n",
            "2949 0.1663 93.75\n",
            "2999 0.0001 100.0\n",
            "3049 0.0028 100.0\n",
            "3099 0.0061 100.0\n",
            "3149 0.2183 93.75\n",
            "3199 0.0266 100.0\n",
            "3249 0.3151 87.5\n",
            "3299 0.0874 93.75\n",
            "3349 0.0330 100.0\n",
            "3399 0.0698 93.75\n",
            "3449 0.0246 100.0\n",
            "3499 0.0019 100.0\n",
            "3549 0.2340 87.5\n",
            "3599 0.1353 87.5\n",
            "3649 0.0628 100.0\n",
            "3699 0.0108 100.0\n",
            "3749 0.0055 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0242, Accuracy: 9008/10000 (90.08%)\n",
            "\n",
            "--- 18.018076419830322 seconds ---\n",
            "Epoch: 7\n",
            "49 0.0142 100.0\n",
            "99 0.0012 100.0\n",
            "149 0.0050 100.0\n",
            "199 0.0115 100.0\n",
            "249 0.0012 100.0\n",
            "299 0.0353 100.0\n",
            "349 0.0033 100.0\n",
            "399 0.0011 100.0\n",
            "449 0.3541 93.75\n",
            "499 0.0047 100.0\n",
            "549 0.0003 100.0\n",
            "599 0.0080 100.0\n",
            "649 0.0266 100.0\n",
            "699 0.0002 100.0\n",
            "749 0.0034 100.0\n",
            "799 0.0064 100.0\n",
            "849 0.0549 93.75\n",
            "899 0.0037 100.0\n",
            "949 0.0179 100.0\n",
            "999 0.1502 93.75\n",
            "1049 0.0205 100.0\n",
            "1099 0.0022 100.0\n",
            "1149 0.0173 100.0\n",
            "1199 0.1678 93.75\n",
            "1249 0.1406 93.75\n",
            "1299 0.0166 100.0\n",
            "1349 0.0156 100.0\n",
            "1399 0.4071 93.75\n",
            "1449 0.0049 100.0\n",
            "1499 0.0137 100.0\n",
            "1549 0.0033 100.0\n",
            "1599 0.0339 100.0\n",
            "1649 0.0028 100.0\n",
            "1699 0.0104 100.0\n",
            "1749 0.2353 93.75\n",
            "1799 0.0049 100.0\n",
            "1849 0.0457 93.75\n",
            "1899 0.1514 93.75\n",
            "1949 0.1420 93.75\n",
            "1999 0.0007 100.0\n",
            "2049 0.0045 100.0\n",
            "2099 0.0009 100.0\n",
            "2149 0.0490 93.75\n",
            "2199 0.1304 93.75\n",
            "2249 0.1652 93.75\n",
            "2299 0.0035 100.0\n",
            "2349 0.0181 100.0\n",
            "2399 0.0078 100.0\n",
            "2449 0.0323 100.0\n",
            "2499 0.0011 100.0\n",
            "2549 0.0012 100.0\n",
            "2599 0.0698 93.75\n",
            "2649 0.0547 100.0\n",
            "2699 0.0038 100.0\n",
            "2749 0.2950 93.75\n",
            "2799 0.3189 87.5\n",
            "2849 0.2638 87.5\n",
            "2899 0.0831 93.75\n",
            "2949 0.0001 100.0\n",
            "2999 0.0447 100.0\n",
            "3049 0.0705 93.75\n",
            "3099 0.4644 93.75\n",
            "3149 0.1539 93.75\n",
            "3199 0.0554 100.0\n",
            "3249 0.0091 100.0\n",
            "3299 0.2172 93.75\n",
            "3349 0.0239 100.0\n",
            "3399 0.0292 100.0\n",
            "3449 0.0243 100.0\n",
            "3499 0.1700 93.75\n",
            "3549 0.0324 100.0\n",
            "3599 0.0540 93.75\n",
            "3649 0.7815 87.5\n",
            "3699 0.0340 100.0\n",
            "3749 0.3087 93.75\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0245, Accuracy: 9019/10000 (90.19%)\n",
            "\n",
            "--- 16.992044925689697 seconds ---\n",
            "Epoch: 8\n",
            "49 0.0090 100.0\n",
            "99 0.0039 100.0\n",
            "149 0.0016 100.0\n",
            "199 0.3731 93.75\n",
            "249 0.1211 93.75\n",
            "299 0.0280 100.0\n",
            "349 0.0505 100.0\n",
            "399 0.0024 100.0\n",
            "449 0.0035 100.0\n",
            "499 0.0276 100.0\n",
            "549 0.0164 100.0\n",
            "599 0.0001 100.0\n",
            "649 0.0023 100.0\n",
            "699 0.0269 100.0\n",
            "749 0.0008 100.0\n",
            "799 0.0059 100.0\n",
            "849 0.0005 100.0\n",
            "899 0.0001 100.0\n",
            "949 0.0104 100.0\n",
            "999 0.0020 100.0\n",
            "1049 0.1603 93.75\n",
            "1099 0.0030 100.0\n",
            "1149 0.0010 100.0\n",
            "1199 0.0336 100.0\n",
            "1249 0.4656 93.75\n",
            "1299 0.2116 93.75\n",
            "1349 0.0065 100.0\n",
            "1399 0.0054 100.0\n",
            "1449 0.0517 100.0\n",
            "1499 0.0001 100.0\n",
            "1549 0.0051 100.0\n",
            "1599 0.0139 100.0\n",
            "1649 0.0152 100.0\n",
            "1699 0.0218 100.0\n",
            "1749 0.0009 100.0\n",
            "1799 0.3449 93.75\n",
            "1849 0.1404 93.75\n",
            "1899 0.0006 100.0\n",
            "1949 0.0070 100.0\n",
            "1999 0.0074 100.0\n",
            "2049 0.0115 100.0\n",
            "2099 0.0958 93.75\n",
            "2149 0.0860 93.75\n",
            "2199 0.0552 100.0\n",
            "2249 0.1400 93.75\n",
            "2299 0.0395 100.0\n",
            "2349 0.2025 93.75\n",
            "2399 0.0931 93.75\n",
            "2449 0.1307 87.5\n",
            "2499 0.0545 100.0\n",
            "2549 0.0111 100.0\n",
            "2599 0.1046 93.75\n",
            "2649 0.0218 100.0\n",
            "2699 0.0427 100.0\n",
            "2749 0.1196 93.75\n",
            "2799 0.0050 100.0\n",
            "2849 0.0640 93.75\n",
            "2899 0.0119 100.0\n",
            "2949 0.0206 100.0\n",
            "2999 0.2960 87.5\n",
            "3049 0.0772 93.75\n",
            "3099 0.0316 100.0\n",
            "3149 0.0926 100.0\n",
            "3199 0.0880 93.75\n",
            "3249 0.1296 93.75\n",
            "3299 0.0156 100.0\n",
            "3349 0.0102 100.0\n",
            "3399 0.0565 100.0\n",
            "3449 0.0202 100.0\n",
            "3499 0.0081 100.0\n",
            "3549 0.0284 100.0\n",
            "3599 0.0246 100.0\n",
            "3649 0.2587 93.75\n",
            "3699 0.0105 100.0\n",
            "3749 0.0791 93.75\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0246, Accuracy: 8994/10000 (89.94%)\n",
            "\n",
            "--- 17.211883783340454 seconds ---\n",
            "Epoch: 9\n",
            "49 0.0295 100.0\n",
            "99 0.0032 100.0\n",
            "149 0.0019 100.0\n",
            "199 0.0062 100.0\n",
            "249 0.0016 100.0\n",
            "299 0.0023 100.0\n",
            "349 0.0004 100.0\n",
            "399 0.1157 93.75\n",
            "449 0.0602 100.0\n",
            "499 0.0030 100.0\n",
            "549 0.0001 100.0\n",
            "599 0.0004 100.0\n",
            "649 0.0604 100.0\n",
            "699 0.0004 100.0\n",
            "749 0.0786 93.75\n",
            "799 0.0491 100.0\n",
            "849 0.0558 100.0\n",
            "899 0.0625 93.75\n",
            "949 0.0006 100.0\n",
            "999 0.0070 100.0\n",
            "1049 0.0202 100.0\n",
            "1099 0.0003 100.0\n",
            "1149 0.0019 100.0\n",
            "1199 0.0016 100.0\n",
            "1249 0.0018 100.0\n",
            "1299 0.0092 100.0\n",
            "1349 0.0416 100.0\n",
            "1399 0.0051 100.0\n",
            "1449 0.0014 100.0\n",
            "1499 0.0635 100.0\n",
            "1549 0.0538 100.0\n",
            "1599 0.0111 100.0\n",
            "1649 0.0388 100.0\n",
            "1699 0.0011 100.0\n",
            "1749 0.0121 100.0\n",
            "1799 0.0029 100.0\n",
            "1849 0.2628 93.75\n",
            "1899 0.2348 93.75\n",
            "1949 0.0525 93.75\n",
            "1999 0.0126 100.0\n",
            "2049 0.1016 93.75\n",
            "2099 0.2351 93.75\n",
            "2149 0.0003 100.0\n",
            "2199 0.0045 100.0\n",
            "2249 0.0134 100.0\n",
            "2299 0.0304 100.0\n",
            "2349 0.0284 100.0\n",
            "2399 0.4708 93.75\n",
            "2449 0.0136 100.0\n",
            "2499 0.0623 93.75\n",
            "2549 0.0073 100.0\n",
            "2599 0.0018 100.0\n",
            "2649 0.0029 100.0\n",
            "2699 0.0318 100.0\n",
            "2749 0.0060 100.0\n",
            "2799 0.0146 100.0\n",
            "2849 0.0048 100.0\n",
            "2899 0.0912 93.75\n",
            "2949 0.0005 100.0\n",
            "2999 0.0020 100.0\n",
            "3049 0.0010 100.0\n",
            "3099 0.0482 100.0\n",
            "3149 0.1756 93.75\n",
            "3199 0.2166 93.75\n",
            "3249 0.0907 93.75\n",
            "3299 0.0150 100.0\n",
            "3349 0.2843 93.75\n",
            "3399 0.2682 93.75\n",
            "3449 0.0056 100.0\n",
            "3499 0.0008 100.0\n",
            "3549 0.0052 100.0\n",
            "3599 0.0738 100.0\n",
            "3649 0.0216 100.0\n",
            "3699 0.0322 100.0\n",
            "3749 0.0661 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0240, Accuracy: 9012/10000 (90.12%)\n",
            "\n",
            "--- 17.921292543411255 seconds ---\n",
            "Epoch: 10\n",
            "49 0.0015 100.0\n",
            "99 0.0042 100.0\n",
            "149 0.0088 100.0\n",
            "199 0.0006 100.0\n",
            "249 0.0145 100.0\n",
            "299 0.0067 100.0\n",
            "349 0.0018 100.0\n",
            "399 0.0534 100.0\n",
            "449 0.0016 100.0\n",
            "499 0.0071 100.0\n",
            "549 0.0087 100.0\n",
            "599 0.0307 100.0\n",
            "649 0.0011 100.0\n",
            "699 0.0046 100.0\n",
            "749 0.0020 100.0\n",
            "799 0.0005 100.0\n",
            "849 0.1205 100.0\n",
            "899 0.0022 100.0\n",
            "949 0.0011 100.0\n",
            "999 0.0007 100.0\n",
            "1049 0.0044 100.0\n",
            "1099 0.0031 100.0\n",
            "1149 0.0405 100.0\n",
            "1199 0.0057 100.0\n",
            "1249 0.1381 93.75\n",
            "1299 0.0046 100.0\n",
            "1349 0.0027 100.0\n",
            "1399 0.0001 100.0\n",
            "1449 0.0016 100.0\n",
            "1499 0.0032 100.0\n",
            "1549 0.5485 87.5\n",
            "1599 0.0148 100.0\n",
            "1649 0.1134 93.75\n",
            "1699 0.1138 93.75\n",
            "1749 0.0261 100.0\n",
            "1799 0.0134 100.0\n",
            "1849 0.0298 100.0\n",
            "1899 0.3768 93.75\n",
            "1949 0.0136 100.0\n",
            "1999 0.0029 100.0\n",
            "2049 0.0007 100.0\n",
            "2099 0.0021 100.0\n",
            "2149 0.1776 93.75\n",
            "2199 0.0281 100.0\n",
            "2249 0.2302 93.75\n",
            "2299 0.0786 93.75\n",
            "2349 0.0643 93.75\n",
            "2399 0.0147 100.0\n",
            "2449 0.0192 100.0\n",
            "2499 0.0045 100.0\n",
            "2549 0.0009 100.0\n",
            "2599 0.0000 100.0\n",
            "2649 0.0060 100.0\n",
            "2699 0.1421 87.5\n",
            "2749 0.0192 100.0\n",
            "2799 0.2044 93.75\n",
            "2849 0.1781 87.5\n",
            "2899 0.0384 100.0\n",
            "2949 0.0876 93.75\n",
            "2999 0.0712 100.0\n",
            "3049 0.0172 100.0\n",
            "3099 0.0332 100.0\n",
            "3149 0.0103 100.0\n",
            "3199 0.0156 100.0\n",
            "3249 0.0009 100.0\n",
            "3299 0.0473 100.0\n",
            "3349 0.0169 100.0\n",
            "3399 0.1816 87.5\n",
            "3449 0.0101 100.0\n",
            "3499 0.4272 87.5\n",
            "3549 0.1167 100.0\n",
            "3599 0.0406 100.0\n",
            "3649 0.0011 100.0\n",
            "3699 0.0024 100.0\n",
            "3749 0.0064 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0240, Accuracy: 9035/10000 (90.35%)\n",
            "\n",
            "--- 17.086456060409546 seconds ---\n"
          ]
        }
      ],
      "source": [
        "#mnist epoch 10 rffg4vsa\n",
        "class ArgumentParser:\n",
        "    def __init__(self):\n",
        "        self.args = {\n",
        "            'lr': 0.01,            # Learning rate for optimizing class representative\n",
        "            'gamma': 0.3,          # Kernel parameter for computing covariance\n",
        "            'epoch': 10,            # Epochs of training\n",
        "            'gorder': 4,           # Order of the cyclic group required for G-VSA (8 ou 16)\n",
        "            'dim': 10000,          # Dimension of hypervectors c le grand D\n",
        "            'seed': 43,            # Random seed for reproducing results\n",
        "            'resume': True,       # Resume from existing encoded hypervectors\n",
        "            'data_dir': './encoded_data', # Directory to save encoded data (hypervectors)\n",
        "            'dataset': 'mnist',    # Dataset choice choices=['mnist', 'fmnist', 'cifar', 'isolet', 'ucihar']\n",
        "            'raw_data_dir': './dataset',   # Raw data directory\n",
        "            'model': 'rff-gvsa'    # Model type choices=['rff-hdc', 'linear-hdc', 'rff-gvsa']\n",
        "        }\n",
        "        self.seed = self.args['seed']\n",
        "        self.lr = self.args['lr']\n",
        "        self.gamma = self.args['gamma']\n",
        "        self.epoch = self.args['epoch']\n",
        "        self.gorder = self.args['gorder']\n",
        "        self.dim = self.args['dim']\n",
        "        self.resume = self.args['resume']\n",
        "        self.data_dir = self.args['data_dir']\n",
        "        self.dataset = self.args['dataset']\n",
        "        self.raw_data_dir = self.args['raw_data_dir']\n",
        "        self.model = self.args['model']\n",
        "\n",
        "\n",
        "    def get_args(self):\n",
        "        return self.args\n",
        "args = ArgumentParser()\n",
        "torch.manual_seed(args.seed)\n",
        "np.random.seed(args.seed)\n",
        "if 'hdc' in args.model:\n",
        "    args.gorder = 2\n",
        "    print(\"Use binary HDC with random fourier features, ignoring gorder, set to 2.\")\n",
        "args.data_dir = f'{args.data_dir}/{args.dataset}_{args.model}_order{args.gorder}_gamma{args.gamma}_dim{args.dim}'\n",
        "try:\n",
        "    os.makedirs(args.data_dir)\n",
        "except FileExistsError:\n",
        "    print('Encoded data folder already exists')\n",
        "if not args.resume:\n",
        "    print('Encode the dataset into hypervectors and save')\n",
        "    encode_and_save(args)\n",
        "    print('Finish encoding and saving')\n",
        "print(f'Optimizing class representatives for {args.epoch} epochs')\n",
        "train(args)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FMNIST"
      ],
      "metadata": {
        "id": "YmaJ8ipFVn0I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#fminst epoch 1 rffg4vsa\n",
        "class ArgumentParser:\n",
        "    def __init__(self):\n",
        "        self.args = {\n",
        "            'lr': 0.01,            # Learning rate for optimizing class representative\n",
        "            'gamma': 0.3,          # Kernel parameter for computing covariance\n",
        "            'epoch': 1,            # Epochs of training\n",
        "            'gorder': 4,           # Order of the cyclic group required for G-VSA (8 ou 16)\n",
        "            'dim': 10000,          # Dimension of hypervectors c le grand D\n",
        "            'seed': 43,            # Random seed for reproducing results\n",
        "            'resume': False,       # Resume from existing encoded hypervectors\n",
        "            'data_dir': './encoded_data', # Directory to save encoded data (hypervectors)\n",
        "            'dataset': 'fmnist',    # Dataset choice choices=['mnist', 'fmnist', 'cifar', 'isolet', 'ucihar']\n",
        "            'raw_data_dir': './dataset',   # Raw data directory\n",
        "            'model': 'rff-gvsa'    # Model type choices=['rff-hdc', 'linear-hdc', 'rff-gvsa']\n",
        "        }\n",
        "        self.seed = self.args['seed']\n",
        "        self.lr = self.args['lr']\n",
        "        self.gamma = self.args['gamma']\n",
        "        self.epoch = self.args['epoch']\n",
        "        self.gorder = self.args['gorder']\n",
        "        self.dim = self.args['dim']\n",
        "        self.resume = self.args['resume']\n",
        "        self.data_dir = self.args['data_dir']\n",
        "        self.dataset = self.args['dataset']\n",
        "        self.raw_data_dir = self.args['raw_data_dir']\n",
        "        self.model = self.args['model']\n",
        "\n",
        "\n",
        "    def get_args(self):\n",
        "        return self.args\n",
        "\n",
        "args = ArgumentParser()\n",
        "torch.manual_seed(args.seed)\n",
        "np.random.seed(args.seed)\n",
        "if 'hdc' in args.model:\n",
        "    args.gorder = 2\n",
        "    print(\"Use binary HDC with random fourier features, ignoring gorder, set to 2.\")\n",
        "args.data_dir = f'{args.data_dir}/{args.dataset}_{args.model}_order{args.gorder}_gamma{args.gamma}_dim{args.dim}'\n",
        "try:\n",
        "    os.makedirs(args.data_dir)\n",
        "except FileExistsError:\n",
        "    print('Encoded data folder already exists')\n",
        "if not args.resume:\n",
        "    print('Encode the dataset into hypervectors and save')\n",
        "    encode_and_save(args)\n",
        "    print('Finish encoding and saving')\n",
        "print(f'Optimizing class representatives for {args.epoch} epochs')\n",
        "train(args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97faa660-afd8-4d89-a88d-937bfcef5787",
        "id": "QQJBmIqEVn0J"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encode the dataset into hypervectors and save\n",
            "# of channels of data 1\n",
            "# of training samples and test samples 60000 10000\n",
            "Encoding with random fourier features encoder.\n",
            "the threshold to discretize fourier features to group elements tensor([-0.6746,  0.0000,  0.6746])\n",
            "Encoded pixels to hypervectors with size:  torch.Size([256, 10000])\n",
            "Encoding training data...\n",
            "Start encoding data\n",
            "12800 images encoded. Total time elapse = 26.797804832458496\n",
            "25600 images encoded. Total time elapse = 54.070013761520386\n",
            "38400 images encoded. Total time elapse = 80.93772220611572\n",
            "51200 images encoded. Total time elapse = 107.74367713928223\n",
            "Finish encoding data\n",
            "Encoding test data...\n",
            "Start encoding data\n",
            "Finish encoding data\n",
            "Finish encoding and saving\n",
            "Optimizing class representatives for 1 epochs\n",
            "Loading encoded training data...\n",
            "Loading encoded test data...\n",
            "Size of encoded training data torch.Size([60000, 10000]) and test data torch.Size([10000, 10000])\n",
            "Epoch: 1\n",
            "49 1.4405 68.75\n",
            "99 1.4364 43.75\n",
            "149 1.2066 56.25\n",
            "199 0.5389 93.75\n",
            "249 0.2863 93.75\n",
            "299 0.9092 68.75\n",
            "349 0.9574 68.75\n",
            "399 0.4768 81.25\n",
            "449 0.7444 75.0\n",
            "499 0.9149 68.75\n",
            "549 0.5823 81.25\n",
            "599 0.2922 87.5\n",
            "649 0.4412 81.25\n",
            "699 0.3877 87.5\n",
            "749 0.1507 100.0\n",
            "799 0.6087 75.0\n",
            "849 0.3756 93.75\n",
            "899 0.4727 81.25\n",
            "949 0.4660 87.5\n",
            "999 0.6603 81.25\n",
            "1049 0.9936 50.0\n",
            "1099 0.5514 81.25\n",
            "1149 0.5787 68.75\n",
            "1199 0.7631 87.5\n",
            "1249 0.4120 87.5\n",
            "1299 0.3574 87.5\n",
            "1349 0.4634 87.5\n",
            "1399 0.8512 75.0\n",
            "1449 0.5715 81.25\n",
            "1499 0.3272 87.5\n",
            "1549 0.9381 75.0\n",
            "1599 0.7377 75.0\n",
            "1649 0.5488 81.25\n",
            "1699 0.2702 93.75\n",
            "1749 0.8891 56.25\n",
            "1799 0.5473 81.25\n",
            "1849 0.3593 87.5\n",
            "1899 0.4259 81.25\n",
            "1949 0.5048 81.25\n",
            "1999 0.4986 87.5\n",
            "2049 0.3997 87.5\n",
            "2099 0.8451 68.75\n",
            "2149 0.4474 81.25\n",
            "2199 0.8933 81.25\n",
            "2249 0.4468 75.0\n",
            "2299 0.3585 87.5\n",
            "2349 0.3332 87.5\n",
            "2399 0.8517 75.0\n",
            "2449 0.6928 75.0\n",
            "2499 0.1974 100.0\n",
            "2549 0.5842 81.25\n",
            "2599 0.5368 81.25\n",
            "2649 0.6716 87.5\n",
            "2699 0.5045 81.25\n",
            "2749 0.7285 75.0\n",
            "2799 0.3886 93.75\n",
            "2849 0.4432 87.5\n",
            "2899 0.6958 87.5\n",
            "2949 0.2162 93.75\n",
            "2999 0.7530 75.0\n",
            "3049 0.8135 62.5\n",
            "3099 0.7387 68.75\n",
            "3149 0.4435 87.5\n",
            "3199 0.2121 100.0\n",
            "3249 0.2632 87.5\n",
            "3299 0.7666 75.0\n",
            "3349 0.6366 68.75\n",
            "3399 0.7295 81.25\n",
            "3449 0.0950 100.0\n",
            "3499 0.7175 68.75\n",
            "3549 0.9678 75.0\n",
            "3599 0.1476 93.75\n",
            "3649 1.1022 62.5\n",
            "3699 0.5458 81.25\n",
            "3749 0.6020 87.5\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0392, Accuracy: 7904/10000 (79.04%)\n",
            "\n",
            "--- 17.324076890945435 seconds ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f0d2cd0-15fc-4501-df94-13c0f3addf97",
        "id": "B7z05CeXVn0J"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoded data folder already exists\n",
            "Optimizing class representatives for 10 epochs\n",
            "Loading encoded training data...\n",
            "Loading encoded test data...\n",
            "Size of encoded training data torch.Size([60000, 10000]) and test data torch.Size([10000, 10000])\n",
            "Epoch: 1\n",
            "49 1.2674 87.5\n",
            "99 1.0163 75.0\n",
            "149 1.0957 50.0\n",
            "199 0.7523 68.75\n",
            "249 0.4957 87.5\n",
            "299 0.5855 81.25\n",
            "349 0.5954 81.25\n",
            "399 0.7489 75.0\n",
            "449 0.6624 75.0\n",
            "499 0.4638 81.25\n",
            "549 0.3337 93.75\n",
            "599 0.5963 68.75\n",
            "649 0.1547 100.0\n",
            "699 0.7419 75.0\n",
            "749 1.1166 68.75\n",
            "799 0.6064 75.0\n",
            "849 0.2643 93.75\n",
            "899 0.5212 87.5\n",
            "949 0.0644 100.0\n",
            "999 0.3830 87.5\n",
            "1049 0.6343 75.0\n",
            "1099 0.5966 87.5\n",
            "1149 0.1737 100.0\n",
            "1199 0.4365 81.25\n",
            "1249 0.3015 93.75\n",
            "1299 0.3126 87.5\n",
            "1349 0.2497 93.75\n",
            "1399 0.2784 93.75\n",
            "1449 0.3525 87.5\n",
            "1499 0.8569 75.0\n",
            "1549 0.6186 81.25\n",
            "1599 0.6763 81.25\n",
            "1649 0.6395 81.25\n",
            "1699 0.2543 87.5\n",
            "1749 0.4843 81.25\n",
            "1799 0.5559 81.25\n",
            "1849 0.3420 87.5\n",
            "1899 0.6890 87.5\n",
            "1949 0.5187 87.5\n",
            "1999 0.5140 81.25\n",
            "2049 0.3469 93.75\n",
            "2099 0.6222 75.0\n",
            "2149 0.5069 81.25\n",
            "2199 0.6446 87.5\n",
            "2249 0.6962 75.0\n",
            "2299 0.3413 87.5\n",
            "2349 0.6961 75.0\n",
            "2399 0.3370 87.5\n",
            "2449 0.3325 87.5\n",
            "2499 0.9625 56.25\n",
            "2549 0.2408 81.25\n",
            "2599 0.5883 68.75\n",
            "2649 0.2216 93.75\n",
            "2699 0.2874 87.5\n",
            "2749 0.4374 87.5\n",
            "2799 0.1317 93.75\n",
            "2849 0.6444 81.25\n",
            "2899 0.4851 81.25\n",
            "2949 0.7926 75.0\n",
            "2999 1.1518 56.25\n",
            "3049 0.6864 75.0\n",
            "3099 0.7157 68.75\n",
            "3149 0.7933 75.0\n",
            "3199 0.5640 87.5\n",
            "3249 0.1961 93.75\n",
            "3299 0.4388 87.5\n",
            "3349 0.8229 75.0\n",
            "3399 0.6625 68.75\n",
            "3449 0.9084 81.25\n",
            "3499 0.5306 75.0\n",
            "3549 0.6919 81.25\n",
            "3599 0.2616 93.75\n",
            "3649 0.7775 75.0\n",
            "3699 0.2855 93.75\n",
            "3749 0.3780 81.25\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0395, Accuracy: 7941/10000 (79.41%)\n",
            "\n",
            "--- 17.00673770904541 seconds ---\n",
            "Epoch: 2\n",
            "49 0.1882 93.75\n",
            "99 0.1410 93.75\n",
            "149 0.0968 100.0\n",
            "199 0.0552 100.0\n",
            "249 0.1891 87.5\n",
            "299 0.0166 100.0\n",
            "349 0.3215 93.75\n",
            "399 0.1173 93.75\n",
            "449 0.0508 100.0\n",
            "499 0.3392 87.5\n",
            "549 0.3573 87.5\n",
            "599 0.4969 87.5\n",
            "649 0.4693 87.5\n",
            "699 0.1641 93.75\n",
            "749 0.0502 100.0\n",
            "799 0.6480 81.25\n",
            "849 0.1929 87.5\n",
            "899 0.1809 87.5\n",
            "949 0.4468 81.25\n",
            "999 0.1438 93.75\n",
            "1049 0.1075 100.0\n",
            "1099 0.2758 87.5\n",
            "1149 0.3190 81.25\n",
            "1199 0.1355 93.75\n",
            "1249 0.3652 87.5\n",
            "1299 0.2753 87.5\n",
            "1349 0.2101 87.5\n",
            "1399 0.0531 100.0\n",
            "1449 0.1753 93.75\n",
            "1499 0.3752 87.5\n",
            "1549 0.6212 81.25\n",
            "1599 0.8708 81.25\n",
            "1649 0.0673 100.0\n",
            "1699 0.0505 100.0\n",
            "1749 0.1571 93.75\n",
            "1799 0.3147 87.5\n",
            "1849 0.5026 87.5\n",
            "1899 0.6535 81.25\n",
            "1949 0.3674 93.75\n",
            "1999 0.5373 87.5\n",
            "2049 0.1457 93.75\n",
            "2099 0.2101 93.75\n",
            "2149 0.4718 81.25\n",
            "2199 0.3792 87.5\n",
            "2249 0.1934 93.75\n",
            "2299 0.1502 93.75\n",
            "2349 0.5020 81.25\n",
            "2399 0.0783 100.0\n",
            "2449 0.2030 93.75\n",
            "2499 0.7856 68.75\n",
            "2549 0.2155 93.75\n",
            "2599 0.2819 81.25\n",
            "2649 0.2372 93.75\n",
            "2699 0.4259 87.5\n",
            "2749 0.1139 93.75\n",
            "2799 0.1186 100.0\n",
            "2849 0.4911 81.25\n",
            "2899 0.4055 81.25\n",
            "2949 0.3542 87.5\n",
            "2999 0.7351 68.75\n",
            "3049 0.7677 81.25\n",
            "3099 0.2766 87.5\n",
            "3149 0.8877 81.25\n",
            "3199 0.2312 93.75\n",
            "3249 0.4440 87.5\n",
            "3299 0.1557 100.0\n",
            "3349 0.3732 93.75\n",
            "3399 0.2029 93.75\n",
            "3449 0.0427 100.0\n",
            "3499 0.7837 81.25\n",
            "3549 0.2288 93.75\n",
            "3599 0.3053 87.5\n",
            "3649 0.2416 93.75\n",
            "3699 0.3128 81.25\n",
            "3749 0.7809 75.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0435, Accuracy: 7788/10000 (77.88%)\n",
            "\n",
            "--- 16.793656826019287 seconds ---\n",
            "Epoch: 3\n",
            "49 0.1689 87.5\n",
            "99 0.0627 100.0\n",
            "149 0.0574 100.0\n",
            "199 0.1132 100.0\n",
            "249 0.1502 93.75\n",
            "299 0.0743 100.0\n",
            "349 0.0356 100.0\n",
            "399 0.0157 100.0\n",
            "449 0.1361 93.75\n",
            "499 0.0040 100.0\n",
            "549 0.0433 100.0\n",
            "599 0.1500 93.75\n",
            "649 0.0032 100.0\n",
            "699 0.0056 100.0\n",
            "749 0.0198 100.0\n",
            "799 0.0697 100.0\n",
            "849 0.3645 93.75\n",
            "899 0.2249 93.75\n",
            "949 0.0102 100.0\n",
            "999 0.2276 93.75\n",
            "1049 0.2147 87.5\n",
            "1099 0.0073 100.0\n",
            "1149 0.2961 93.75\n",
            "1199 0.2383 93.75\n",
            "1249 0.2836 93.75\n",
            "1299 0.1619 93.75\n",
            "1349 0.0167 100.0\n",
            "1399 0.0793 93.75\n",
            "1449 0.1809 93.75\n",
            "1499 0.5421 87.5\n",
            "1549 0.3776 87.5\n",
            "1599 0.0867 93.75\n",
            "1649 0.2900 93.75\n",
            "1699 0.0517 100.0\n",
            "1749 0.1740 93.75\n",
            "1799 0.1872 87.5\n",
            "1849 0.1359 100.0\n",
            "1899 0.0132 100.0\n",
            "1949 0.0854 100.0\n",
            "1999 0.1996 93.75\n",
            "2049 0.1805 87.5\n",
            "2099 0.0225 100.0\n",
            "2149 0.0551 100.0\n",
            "2199 0.3196 93.75\n",
            "2249 0.1637 93.75\n",
            "2299 0.2405 93.75\n",
            "2349 0.2925 87.5\n",
            "2399 0.0684 100.0\n",
            "2449 0.1536 93.75\n",
            "2499 0.2368 93.75\n",
            "2549 0.0888 100.0\n",
            "2599 0.4017 81.25\n",
            "2649 0.0880 100.0\n",
            "2699 0.3630 87.5\n",
            "2749 0.1398 93.75\n",
            "2799 0.1645 87.5\n",
            "2849 0.0183 100.0\n",
            "2899 0.3825 81.25\n",
            "2949 0.3848 87.5\n",
            "2999 0.5610 87.5\n",
            "3049 0.2076 87.5\n",
            "3099 0.0620 100.0\n",
            "3149 0.4348 81.25\n",
            "3199 0.2502 87.5\n",
            "3249 0.4674 75.0\n",
            "3299 0.1306 100.0\n",
            "3349 0.0303 100.0\n",
            "3399 0.1293 87.5\n",
            "3449 0.1703 93.75\n",
            "3499 0.2857 93.75\n",
            "3549 0.3892 81.25\n",
            "3599 0.2027 87.5\n",
            "3649 0.1763 93.75\n",
            "3699 0.3277 87.5\n",
            "3749 0.0344 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0492, Accuracy: 7675/10000 (76.75%)\n",
            "\n",
            "--- 17.587969303131104 seconds ---\n",
            "Epoch: 4\n",
            "49 0.1666 93.75\n",
            "99 0.0184 100.0\n",
            "149 0.5466 87.5\n",
            "199 0.0271 100.0\n",
            "249 0.2405 87.5\n",
            "299 0.0188 100.0\n",
            "349 0.0291 100.0\n",
            "399 0.0375 100.0\n",
            "449 0.0095 100.0\n",
            "499 0.0892 93.75\n",
            "549 0.0110 100.0\n",
            "599 0.1739 93.75\n",
            "649 0.0336 100.0\n",
            "699 0.1773 93.75\n",
            "749 0.0281 100.0\n",
            "799 0.1951 93.75\n",
            "849 0.0026 100.0\n",
            "899 0.0580 93.75\n",
            "949 0.1534 100.0\n",
            "999 0.0279 100.0\n",
            "1049 0.0178 100.0\n",
            "1099 0.2617 93.75\n",
            "1149 0.0065 100.0\n",
            "1199 0.1317 93.75\n",
            "1249 0.0795 100.0\n",
            "1299 0.0178 100.0\n",
            "1349 0.1059 93.75\n",
            "1399 0.2036 87.5\n",
            "1449 0.1676 87.5\n",
            "1499 0.0388 100.0\n",
            "1549 0.0309 100.0\n",
            "1599 0.3206 87.5\n",
            "1649 0.3426 81.25\n",
            "1699 0.0458 100.0\n",
            "1749 0.1309 93.75\n",
            "1799 0.1266 93.75\n",
            "1849 0.0568 100.0\n",
            "1899 0.0829 93.75\n",
            "1949 0.2250 93.75\n",
            "1999 0.1891 93.75\n",
            "2049 0.0580 100.0\n",
            "2099 0.1390 93.75\n",
            "2149 0.1723 93.75\n",
            "2199 0.0269 100.0\n",
            "2249 0.1189 100.0\n",
            "2299 0.0900 100.0\n",
            "2349 0.3189 93.75\n",
            "2399 0.1174 93.75\n",
            "2449 0.0828 93.75\n",
            "2499 0.2296 93.75\n",
            "2549 0.3552 87.5\n",
            "2599 0.3865 87.5\n",
            "2649 0.0294 100.0\n",
            "2699 0.0842 93.75\n",
            "2749 0.0956 100.0\n",
            "2799 0.0593 100.0\n",
            "2849 0.0470 100.0\n",
            "2899 0.0959 100.0\n",
            "2949 0.0805 93.75\n",
            "2999 0.0937 93.75\n",
            "3049 0.5449 75.0\n",
            "3099 0.4022 93.75\n",
            "3149 0.1950 87.5\n",
            "3199 0.1689 87.5\n",
            "3249 0.1727 93.75\n",
            "3299 0.0086 100.0\n",
            "3349 0.1608 93.75\n",
            "3399 0.8269 62.5\n",
            "3449 0.0503 100.0\n",
            "3499 0.5124 81.25\n",
            "3549 0.5668 93.75\n",
            "3599 0.0611 100.0\n",
            "3649 0.4900 81.25\n",
            "3699 0.0659 93.75\n",
            "3749 0.4522 75.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0551, Accuracy: 7677/10000 (76.77%)\n",
            "\n",
            "--- 17.06074047088623 seconds ---\n",
            "Epoch: 5\n",
            "49 0.0384 100.0\n",
            "99 0.0395 100.0\n",
            "149 0.1044 93.75\n",
            "199 0.0122 100.0\n",
            "249 0.1554 93.75\n",
            "299 0.1574 93.75\n",
            "349 0.0242 100.0\n",
            "399 0.0079 100.0\n",
            "449 0.1348 93.75\n",
            "499 0.0181 100.0\n",
            "549 0.0816 93.75\n",
            "599 0.4794 87.5\n",
            "649 0.3930 93.75\n",
            "699 0.0058 100.0\n",
            "749 0.2801 87.5\n",
            "799 0.0990 93.75\n",
            "849 0.2018 93.75\n",
            "899 0.0401 100.0\n",
            "949 0.1503 93.75\n",
            "999 0.6147 75.0\n",
            "1049 0.1397 87.5\n",
            "1099 0.2038 87.5\n",
            "1149 0.0787 93.75\n",
            "1199 0.2238 87.5\n",
            "1249 0.0786 93.75\n",
            "1299 0.0729 93.75\n",
            "1349 0.0367 100.0\n",
            "1399 0.5061 75.0\n",
            "1449 0.0959 100.0\n",
            "1499 0.1541 93.75\n",
            "1549 0.0553 100.0\n",
            "1599 0.0797 100.0\n",
            "1649 0.1342 93.75\n",
            "1699 0.0571 93.75\n",
            "1749 0.0914 93.75\n",
            "1799 0.1808 93.75\n",
            "1849 0.0380 100.0\n",
            "1899 0.5004 87.5\n",
            "1949 0.0258 100.0\n",
            "1999 0.6207 87.5\n",
            "2049 0.2454 87.5\n",
            "2099 0.6627 87.5\n",
            "2149 0.0156 100.0\n",
            "2199 0.3146 87.5\n",
            "2249 0.0769 100.0\n",
            "2299 0.0774 93.75\n",
            "2349 0.0134 100.0\n",
            "2399 0.3582 87.5\n",
            "2449 0.2514 87.5\n",
            "2499 0.0372 100.0\n",
            "2549 0.2132 93.75\n",
            "2599 0.0410 100.0\n",
            "2649 0.5641 81.25\n",
            "2699 0.2357 87.5\n",
            "2749 0.0878 100.0\n",
            "2799 0.1024 100.0\n",
            "2849 0.6017 75.0\n",
            "2899 0.2519 93.75\n",
            "2949 0.1126 93.75\n",
            "2999 0.1768 93.75\n",
            "3049 0.1843 93.75\n",
            "3099 0.2199 87.5\n",
            "3149 0.0973 93.75\n",
            "3199 0.3508 93.75\n",
            "3249 0.1795 93.75\n",
            "3299 0.0696 100.0\n",
            "3349 0.1770 93.75\n",
            "3399 0.1418 93.75\n",
            "3449 0.2177 93.75\n",
            "3499 0.0756 100.0\n",
            "3549 0.2190 93.75\n",
            "3599 0.5860 75.0\n",
            "3649 0.4736 93.75\n",
            "3699 0.2257 93.75\n",
            "3749 0.2675 87.5\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0561, Accuracy: 7665/10000 (76.65%)\n",
            "\n",
            "--- 17.104129552841187 seconds ---\n",
            "Epoch: 6\n",
            "49 0.1269 100.0\n",
            "99 0.1256 100.0\n",
            "149 0.0909 100.0\n",
            "199 0.0021 100.0\n",
            "249 0.0372 100.0\n",
            "299 0.1313 93.75\n",
            "349 0.0254 100.0\n",
            "399 0.1163 93.75\n",
            "449 0.0523 100.0\n",
            "499 0.5036 81.25\n",
            "549 0.0765 100.0\n",
            "599 0.0718 100.0\n",
            "649 0.0737 100.0\n",
            "699 0.3228 93.75\n",
            "749 0.0032 100.0\n",
            "799 0.0233 100.0\n",
            "849 0.0275 100.0\n",
            "899 0.1518 93.75\n",
            "949 0.0431 100.0\n",
            "999 0.2431 87.5\n",
            "1049 0.0472 100.0\n",
            "1099 0.1210 93.75\n",
            "1149 0.1277 93.75\n",
            "1199 0.0190 100.0\n",
            "1249 0.3006 93.75\n",
            "1299 0.0928 93.75\n",
            "1349 0.0745 100.0\n",
            "1399 0.0805 93.75\n",
            "1449 0.0846 93.75\n",
            "1499 0.0032 100.0\n",
            "1549 0.1631 93.75\n",
            "1599 0.0155 100.0\n",
            "1649 0.2992 87.5\n",
            "1699 0.1240 93.75\n",
            "1749 0.1289 93.75\n",
            "1799 0.0337 100.0\n",
            "1849 0.2895 87.5\n",
            "1899 0.0170 100.0\n",
            "1949 0.1794 93.75\n",
            "1999 0.1724 93.75\n",
            "2049 0.3711 87.5\n",
            "2099 0.2252 93.75\n",
            "2149 0.0102 100.0\n",
            "2199 0.5776 81.25\n",
            "2249 0.2064 87.5\n",
            "2299 0.1523 93.75\n",
            "2349 0.1339 100.0\n",
            "2399 0.3388 81.25\n",
            "2449 0.1679 93.75\n",
            "2499 0.0992 100.0\n",
            "2549 0.4041 87.5\n",
            "2599 0.2179 87.5\n",
            "2649 0.2583 87.5\n",
            "2699 0.0072 100.0\n",
            "2749 0.3279 93.75\n",
            "2799 0.0638 100.0\n",
            "2849 0.1610 87.5\n",
            "2899 0.4250 81.25\n",
            "2949 0.2083 93.75\n",
            "2999 0.4162 81.25\n",
            "3049 0.4338 75.0\n",
            "3099 0.3826 87.5\n",
            "3149 0.3007 87.5\n",
            "3199 0.0802 93.75\n",
            "3249 0.0635 100.0\n",
            "3299 0.0429 100.0\n",
            "3349 0.0822 93.75\n",
            "3399 0.0403 100.0\n",
            "3449 0.0675 100.0\n",
            "3499 0.0722 100.0\n",
            "3549 0.3004 87.5\n",
            "3599 0.2553 93.75\n",
            "3649 0.3076 87.5\n",
            "3699 0.1197 100.0\n",
            "3749 0.2609 93.75\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0579, Accuracy: 7626/10000 (76.26%)\n",
            "\n",
            "--- 17.927233695983887 seconds ---\n",
            "Epoch: 7\n",
            "49 0.0211 100.0\n",
            "99 0.0305 100.0\n",
            "149 0.1456 93.75\n",
            "199 0.0192 100.0\n",
            "249 0.0639 100.0\n",
            "299 0.0139 100.0\n",
            "349 0.1625 93.75\n",
            "399 0.0126 100.0\n",
            "449 0.0029 100.0\n",
            "499 0.1472 93.75\n",
            "549 0.2191 93.75\n",
            "599 0.0939 100.0\n",
            "649 0.0715 100.0\n",
            "699 0.0142 100.0\n",
            "749 0.1723 93.75\n",
            "799 0.2960 87.5\n",
            "849 0.0055 100.0\n",
            "899 0.0570 93.75\n",
            "949 0.0866 93.75\n",
            "999 0.0031 100.0\n",
            "1049 0.0503 100.0\n",
            "1099 0.0165 100.0\n",
            "1149 0.0252 100.0\n",
            "1199 0.0131 100.0\n",
            "1249 0.2570 87.5\n",
            "1299 0.0263 100.0\n",
            "1349 0.0540 93.75\n",
            "1399 0.1803 93.75\n",
            "1449 0.0450 100.0\n",
            "1499 0.0052 100.0\n",
            "1549 0.4699 75.0\n",
            "1599 0.0932 100.0\n",
            "1649 0.0875 100.0\n",
            "1699 0.3516 87.5\n",
            "1749 0.1831 93.75\n",
            "1799 0.4170 93.75\n",
            "1849 0.0234 100.0\n",
            "1899 0.3629 87.5\n",
            "1949 0.0260 100.0\n",
            "1999 0.1411 93.75\n",
            "2049 0.0939 93.75\n",
            "2099 0.0451 100.0\n",
            "2149 0.0625 100.0\n",
            "2199 0.2010 93.75\n",
            "2249 0.1809 93.75\n",
            "2299 0.1005 93.75\n",
            "2349 0.0401 100.0\n",
            "2399 0.0566 100.0\n",
            "2449 0.0048 100.0\n",
            "2499 0.1666 87.5\n",
            "2549 0.1079 93.75\n",
            "2599 0.4419 81.25\n",
            "2649 0.1028 100.0\n",
            "2699 0.4590 87.5\n",
            "2749 0.1236 93.75\n",
            "2799 0.7052 81.25\n",
            "2849 0.0238 100.0\n",
            "2899 0.6929 75.0\n",
            "2949 0.4164 75.0\n",
            "2999 0.1068 93.75\n",
            "3049 0.0761 93.75\n",
            "3099 0.3473 81.25\n",
            "3149 0.1704 87.5\n",
            "3199 0.1460 93.75\n",
            "3249 0.2451 87.5\n",
            "3299 0.4033 81.25\n",
            "3349 0.2087 93.75\n",
            "3399 0.1833 87.5\n",
            "3449 0.0198 100.0\n",
            "3499 0.3665 87.5\n",
            "3549 0.2450 93.75\n",
            "3599 0.8802 81.25\n",
            "3649 0.0970 100.0\n",
            "3699 0.0670 100.0\n",
            "3749 0.3127 87.5\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0571, Accuracy: 7604/10000 (76.04%)\n",
            "\n",
            "--- 17.019386053085327 seconds ---\n",
            "Epoch: 8\n",
            "49 0.0267 100.0\n",
            "99 0.2011 93.75\n",
            "149 0.0085 100.0\n",
            "199 0.0024 100.0\n",
            "249 0.0234 100.0\n",
            "299 0.2908 93.75\n",
            "349 0.0353 100.0\n",
            "399 0.0677 93.75\n",
            "449 0.0540 100.0\n",
            "499 0.0734 93.75\n",
            "549 0.0110 100.0\n",
            "599 0.2265 93.75\n",
            "649 0.0129 100.0\n",
            "699 0.0635 100.0\n",
            "749 0.0154 100.0\n",
            "799 0.1686 87.5\n",
            "849 0.1232 87.5\n",
            "899 0.1605 93.75\n",
            "949 0.0477 93.75\n",
            "999 0.2622 93.75\n",
            "1049 0.3308 81.25\n",
            "1099 0.0164 100.0\n",
            "1149 0.0166 100.0\n",
            "1199 0.0059 100.0\n",
            "1249 0.0071 100.0\n",
            "1299 0.1212 93.75\n",
            "1349 0.3199 93.75\n",
            "1399 0.0838 100.0\n",
            "1449 0.0777 93.75\n",
            "1499 0.1310 93.75\n",
            "1549 0.4751 87.5\n",
            "1599 0.0344 100.0\n",
            "1649 0.1782 93.75\n",
            "1699 0.3405 87.5\n",
            "1749 0.2702 93.75\n",
            "1799 0.2836 93.75\n",
            "1849 0.3280 93.75\n",
            "1899 0.1579 93.75\n",
            "1949 0.1185 93.75\n",
            "1999 0.0677 93.75\n",
            "2049 0.1358 93.75\n",
            "2099 0.1378 93.75\n",
            "2149 0.0704 100.0\n",
            "2199 0.0175 100.0\n",
            "2249 0.4163 87.5\n",
            "2299 0.4397 87.5\n",
            "2349 0.4327 87.5\n",
            "2399 0.0395 100.0\n",
            "2449 0.0865 100.0\n",
            "2499 0.0649 100.0\n",
            "2549 0.1928 93.75\n",
            "2599 0.6723 81.25\n",
            "2649 0.2778 87.5\n",
            "2699 0.2324 93.75\n",
            "2749 0.2062 93.75\n",
            "2799 0.3618 87.5\n",
            "2849 0.1148 100.0\n",
            "2899 0.1012 93.75\n",
            "2949 0.2886 87.5\n",
            "2999 0.1875 93.75\n",
            "3049 0.1396 100.0\n",
            "3099 0.0345 100.0\n",
            "3149 0.1481 93.75\n",
            "3199 0.0598 100.0\n",
            "3249 0.1514 100.0\n",
            "3299 0.3289 87.5\n",
            "3349 0.0583 100.0\n",
            "3399 0.1444 93.75\n",
            "3449 0.1370 93.75\n",
            "3499 0.1320 100.0\n",
            "3549 0.0857 93.75\n",
            "3599 0.2566 87.5\n",
            "3649 0.6529 87.5\n",
            "3699 0.1710 93.75\n",
            "3749 0.2269 93.75\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0576, Accuracy: 7644/10000 (76.44%)\n",
            "\n",
            "--- 17.078638076782227 seconds ---\n",
            "Epoch: 9\n",
            "49 0.0074 100.0\n",
            "99 0.0251 100.0\n",
            "149 0.0057 100.0\n",
            "199 0.0870 93.75\n",
            "249 0.0581 93.75\n",
            "299 0.1525 93.75\n",
            "349 0.1422 93.75\n",
            "399 0.0179 100.0\n",
            "449 0.0298 100.0\n",
            "499 0.0715 93.75\n",
            "549 0.0277 100.0\n",
            "599 0.5164 87.5\n",
            "649 0.0695 93.75\n",
            "699 0.0068 100.0\n",
            "749 0.1331 93.75\n",
            "799 0.0446 100.0\n",
            "849 0.1232 93.75\n",
            "899 0.2078 87.5\n",
            "949 0.2822 87.5\n",
            "999 0.0176 100.0\n",
            "1049 0.0624 100.0\n",
            "1099 0.0019 100.0\n",
            "1149 0.0603 100.0\n",
            "1199 0.0013 100.0\n",
            "1249 0.0948 93.75\n",
            "1299 0.0455 100.0\n",
            "1349 0.0490 100.0\n",
            "1399 0.0516 100.0\n",
            "1449 0.1934 93.75\n",
            "1499 0.0117 100.0\n",
            "1549 0.0247 100.0\n",
            "1599 0.6776 87.5\n",
            "1649 0.2498 93.75\n",
            "1699 0.0231 100.0\n",
            "1749 0.2342 93.75\n",
            "1799 0.2153 93.75\n",
            "1849 0.0345 100.0\n",
            "1899 0.0050 100.0\n",
            "1949 0.0785 100.0\n",
            "1999 0.3027 87.5\n",
            "2049 0.1334 93.75\n",
            "2099 0.2687 93.75\n",
            "2149 0.0473 100.0\n",
            "2199 0.0134 100.0\n",
            "2249 0.1811 93.75\n",
            "2299 0.4047 81.25\n",
            "2349 0.2245 87.5\n",
            "2399 0.1553 100.0\n",
            "2449 0.3846 81.25\n",
            "2499 0.2218 87.5\n",
            "2549 0.2466 87.5\n",
            "2599 0.0549 100.0\n",
            "2649 0.1779 93.75\n",
            "2699 0.3977 87.5\n",
            "2749 0.1977 93.75\n",
            "2799 0.0269 100.0\n",
            "2849 0.1428 93.75\n",
            "2899 0.2315 81.25\n",
            "2949 0.2014 81.25\n",
            "2999 0.0331 100.0\n",
            "3049 0.2461 87.5\n",
            "3099 0.1654 93.75\n",
            "3149 0.0507 100.0\n",
            "3199 0.0859 100.0\n",
            "3249 0.1382 93.75\n",
            "3299 0.5244 87.5\n",
            "3349 0.0286 100.0\n",
            "3399 0.1146 93.75\n",
            "3449 0.0101 100.0\n",
            "3499 0.6008 81.25\n",
            "3549 0.1172 93.75\n",
            "3599 0.1300 100.0\n",
            "3649 0.0942 100.0\n",
            "3699 0.3614 87.5\n",
            "3749 0.5425 81.25\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0580, Accuracy: 7576/10000 (75.76%)\n",
            "\n",
            "--- 17.987043380737305 seconds ---\n",
            "Epoch: 10\n",
            "49 0.0930 93.75\n",
            "99 0.0070 100.0\n",
            "149 0.0780 100.0\n",
            "199 0.0117 100.0\n",
            "249 0.0350 100.0\n",
            "299 0.0775 93.75\n",
            "349 0.0672 100.0\n",
            "399 0.0428 100.0\n",
            "449 0.0136 100.0\n",
            "499 0.0758 100.0\n",
            "549 0.0361 100.0\n",
            "599 0.0107 100.0\n",
            "649 0.4413 87.5\n",
            "699 0.0469 100.0\n",
            "749 0.0720 93.75\n",
            "799 0.1154 93.75\n",
            "849 0.2337 93.75\n",
            "899 0.0018 100.0\n",
            "949 0.2369 93.75\n",
            "999 0.1185 93.75\n",
            "1049 0.1623 93.75\n",
            "1099 0.2360 93.75\n",
            "1149 0.0936 93.75\n",
            "1199 0.0234 100.0\n",
            "1249 0.0365 100.0\n",
            "1299 0.0813 93.75\n",
            "1349 0.0460 100.0\n",
            "1399 0.1816 93.75\n",
            "1449 0.0997 93.75\n",
            "1499 0.0772 100.0\n",
            "1549 0.0532 100.0\n",
            "1599 0.2416 93.75\n",
            "1649 0.0630 93.75\n",
            "1699 0.1163 93.75\n",
            "1749 0.0834 93.75\n",
            "1799 0.2576 87.5\n",
            "1849 0.7675 81.25\n",
            "1899 0.4108 81.25\n",
            "1949 0.1368 93.75\n",
            "1999 0.1863 87.5\n",
            "2049 0.1748 93.75\n",
            "2099 0.0940 93.75\n",
            "2149 0.1804 93.75\n",
            "2199 0.0613 93.75\n",
            "2249 0.2311 93.75\n",
            "2299 0.1333 87.5\n",
            "2349 0.0152 100.0\n",
            "2399 0.0862 100.0\n",
            "2449 0.0106 100.0\n",
            "2499 0.5625 68.75\n",
            "2549 0.0610 100.0\n",
            "2599 0.1141 100.0\n",
            "2649 0.1488 100.0\n",
            "2699 0.3233 87.5\n",
            "2749 0.1108 93.75\n",
            "2799 0.0598 100.0\n",
            "2849 0.1709 93.75\n",
            "2899 0.0721 100.0\n",
            "2949 0.4026 81.25\n",
            "2999 0.4657 81.25\n",
            "3049 0.0946 100.0\n",
            "3099 0.2990 81.25\n",
            "3149 0.3093 87.5\n",
            "3199 0.1864 87.5\n",
            "3249 0.0624 100.0\n",
            "3299 0.2116 87.5\n",
            "3349 0.1858 93.75\n",
            "3399 0.1134 100.0\n",
            "3449 0.3103 87.5\n",
            "3499 0.4592 81.25\n",
            "3549 0.1297 100.0\n",
            "3599 0.0359 100.0\n",
            "3649 0.1080 93.75\n",
            "3699 0.2178 87.5\n",
            "3749 0.2397 87.5\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0598, Accuracy: 7603/10000 (76.03%)\n",
            "\n",
            "--- 17.007697582244873 seconds ---\n"
          ]
        }
      ],
      "source": [
        "#fminst epoch 10 rffg4vsa\n",
        "class ArgumentParser:\n",
        "    def __init__(self):\n",
        "        self.args = {\n",
        "            'lr': 0.01,            # Learning rate for optimizing class representative\n",
        "            'gamma': 0.3,          # Kernel parameter for computing covariance\n",
        "            'epoch': 10,            # Epochs of training\n",
        "            'gorder': 4,           # Order of the cyclic group required for G-VSA (8 ou 16)\n",
        "            'dim': 10000,          # Dimension of hypervectors c le grand D\n",
        "            'seed': 43,            # Random seed for reproducing results\n",
        "            'resume': True,       # Resume from existing encoded hypervectors\n",
        "            'data_dir': './encoded_data', # Directory to save encoded data (hypervectors)\n",
        "            'dataset': 'fmnist',    # Dataset choice choices=['mnist', 'fmnist', 'cifar', 'isolet', 'ucihar']\n",
        "            'raw_data_dir': './dataset',   # Raw data directory\n",
        "            'model': 'rff-gvsa'    # Model type choices=['rff-hdc', 'linear-hdc', 'rff-gvsa']\n",
        "        }\n",
        "        self.seed = self.args['seed']\n",
        "        self.lr = self.args['lr']\n",
        "        self.gamma = self.args['gamma']\n",
        "        self.epoch = self.args['epoch']\n",
        "        self.gorder = self.args['gorder']\n",
        "        self.dim = self.args['dim']\n",
        "        self.resume = self.args['resume']\n",
        "        self.data_dir = self.args['data_dir']\n",
        "        self.dataset = self.args['dataset']\n",
        "        self.raw_data_dir = self.args['raw_data_dir']\n",
        "        self.model = self.args['model']\n",
        "\n",
        "\n",
        "    def get_args(self):\n",
        "        return self.args\n",
        "args = ArgumentParser()\n",
        "torch.manual_seed(args.seed)\n",
        "np.random.seed(args.seed)\n",
        "if 'hdc' in args.model:\n",
        "    args.gorder = 2\n",
        "    print(\"Use binary HDC with random fourier features, ignoring gorder, set to 2.\")\n",
        "args.data_dir = f'{args.data_dir}/{args.dataset}_{args.model}_order{args.gorder}_gamma{args.gamma}_dim{args.dim}'\n",
        "try:\n",
        "    os.makedirs(args.data_dir)\n",
        "except FileExistsError:\n",
        "    print('Encoded data folder already exists')\n",
        "if not args.resume:\n",
        "    print('Encode the dataset into hypervectors and save')\n",
        "    encode_and_save(args)\n",
        "    print('Finish encoding and saving')\n",
        "print(f'Optimizing class representatives for {args.epoch} epochs')\n",
        "train(args)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RFF G($2^3$)-VSA"
      ],
      "metadata": {
        "id": "aoV7eabgxeL4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ISOLET"
      ],
      "metadata": {
        "id": "ZE35cQ6exo0D"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20ee4ddc-64c4-4af0-c13a-560ecf776d46",
        "id": "aQmYJ8Axxo0D"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encode the dataset into hypervectors and save\n",
            "# of channels of data 1\n",
            "# of training samples and test samples 6238 1559\n",
            "Encoding with random fourier features encoder.\n",
            "the threshold to discretize fourier features to group elements tensor([-1.1505, -0.6746, -0.3187,  0.0000,  0.3187,  0.6746,  1.1505])\n",
            "Encoded pixels to hypervectors with size:  torch.Size([256, 10000])\n",
            "Encoding training data...\n",
            "Start encoding data\n",
            "Finish encoding data\n",
            "Encoding test data...\n",
            "Start encoding data\n",
            "Finish encoding data\n",
            "Finish encoding and saving\n",
            "Optimizing class representatives for 1 epochs\n",
            "Loading encoded training data...\n",
            "Loading encoded test data...\n",
            "Size of encoded training data torch.Size([6238, 10000]) and test data torch.Size([1559, 10000])\n",
            "Epoch: 1\n",
            "49 1.8414 75.0\n",
            "99 0.8240 81.25\n",
            "149 0.4975 93.75\n",
            "199 0.3391 87.5\n",
            "249 0.3624 87.5\n",
            "299 0.3582 87.5\n",
            "349 0.0888 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0297, Accuracy: 1446/1559 (92.75%)\n",
            "\n",
            "--- 2.0775809288024902 seconds ---\n"
          ]
        }
      ],
      "source": [
        "#isolet epoch 1 RFF G8VSA\n",
        "class ArgumentParser:\n",
        "    def __init__(self):\n",
        "        self.args = {\n",
        "            'lr': 0.01,            # Learning rate for optimizing class representative\n",
        "            'gamma': 0.3,          # Kernel parameter for computing covariance\n",
        "            'epoch': 1,            # Epochs of training\n",
        "            'gorder': 8,           # Order of the cyclic group required for G-VSA (8 ou 16)\n",
        "            'dim': 10000,          # Dimension of hypervectors c le grand D\n",
        "            'seed': 43,            # Random seed for reproducing results\n",
        "            'resume': False,       # Resume from existing encoded hypervectors\n",
        "            'data_dir': './encoded_data', # Directory to save encoded data (hypervectors)\n",
        "            'dataset': 'isolet',    # Dataset choice choices=['mnist', 'fmnist', 'cifar', 'isolet', 'ucihar']\n",
        "            'raw_data_dir': './dataset',   # Raw data directory\n",
        "            'model': 'rff-gvsa'    # Model type choices=['rff-hdc', 'linear-hdc', 'rff-gvsa']\n",
        "        }\n",
        "        self.seed = self.args['seed']\n",
        "        self.lr = self.args['lr']\n",
        "        self.gamma = self.args['gamma']\n",
        "        self.epoch = self.args['epoch']\n",
        "        self.gorder = self.args['gorder']\n",
        "        self.dim = self.args['dim']\n",
        "        self.resume = self.args['resume']\n",
        "        self.data_dir = self.args['data_dir']\n",
        "        self.dataset = self.args['dataset']\n",
        "        self.raw_data_dir = self.args['raw_data_dir']\n",
        "        self.model = self.args['model']\n",
        "\n",
        "\n",
        "    def get_args(self):\n",
        "        return self.args\n",
        "args = ArgumentParser()\n",
        "torch.manual_seed(args.seed)\n",
        "np.random.seed(args.seed)\n",
        "if 'hdc' in args.model:\n",
        "    args.gorder = 2\n",
        "    print(\"Use binary HDC with random fourier features, ignoring gorder, set to 2.\")\n",
        "args.data_dir = f'{args.data_dir}/{args.dataset}_{args.model}_order{args.gorder}_gamma{args.gamma}_dim{args.dim}'\n",
        "try:\n",
        "    os.makedirs(args.data_dir)\n",
        "except FileExistsError:\n",
        "    print('Encoded data folder already exists')\n",
        "if not args.resume:\n",
        "    print('Encode the dataset into hypervectors and save')\n",
        "    encode_and_save(args)\n",
        "    print('Finish encoding and saving')\n",
        "print(f'Optimizing class representatives for {args.epoch} epochs')\n",
        "train(args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a470239-c0d1-4cb8-c0b7-29db987df70f",
        "id": "BjRUwh6hxo0D"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoded data folder already exists\n",
            "Optimizing class representatives for 10 epochs\n",
            "Loading encoded training data...\n",
            "Loading encoded test data...\n",
            "Size of encoded training data torch.Size([6238, 10000]) and test data torch.Size([1559, 10000])\n",
            "Epoch: 1\n",
            "49 1.8972 62.5\n",
            "99 1.0855 68.75\n",
            "149 0.1954 100.0\n",
            "199 0.7464 75.0\n",
            "249 0.3968 81.25\n",
            "299 0.3751 87.5\n",
            "349 0.1076 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0286, Accuracy: 1444/1559 (92.62%)\n",
            "\n",
            "--- 2.0307114124298096 seconds ---\n",
            "Epoch: 2\n",
            "49 0.0133 100.0\n",
            "99 0.0271 100.0\n",
            "149 0.0087 100.0\n",
            "199 0.0097 100.0\n",
            "249 0.0233 100.0\n",
            "299 0.0175 100.0\n",
            "349 0.0047 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0176, Accuracy: 1464/1559 (93.91%)\n",
            "\n",
            "--- 1.9907522201538086 seconds ---\n",
            "Epoch: 3\n",
            "49 0.0073 100.0\n",
            "99 0.0052 100.0\n",
            "149 0.0054 100.0\n",
            "199 0.0100 100.0\n",
            "249 0.0032 100.0\n",
            "299 0.0034 100.0\n",
            "349 0.0043 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0147, Accuracy: 1470/1559 (94.29%)\n",
            "\n",
            "--- 2.6011674404144287 seconds ---\n",
            "Epoch: 4\n",
            "49 0.0010 100.0\n",
            "99 0.0037 100.0\n",
            "149 0.0042 100.0\n",
            "199 0.0021 100.0\n",
            "249 0.0023 100.0\n",
            "299 0.0023 100.0\n",
            "349 0.0022 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0134, Accuracy: 1475/1559 (94.61%)\n",
            "\n",
            "--- 2.078479051589966 seconds ---\n",
            "Epoch: 5\n",
            "49 0.0035 100.0\n",
            "99 0.0017 100.0\n",
            "149 0.0014 100.0\n",
            "199 0.0023 100.0\n",
            "249 0.0023 100.0\n",
            "299 0.0023 100.0\n",
            "349 0.0024 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0127, Accuracy: 1478/1559 (94.80%)\n",
            "\n",
            "--- 1.9831788539886475 seconds ---\n",
            "Epoch: 6\n",
            "49 0.0007 100.0\n",
            "99 0.0009 100.0\n",
            "149 0.0016 100.0\n",
            "199 0.0015 100.0\n",
            "249 0.0011 100.0\n",
            "299 0.0012 100.0\n",
            "349 0.0009 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0122, Accuracy: 1477/1559 (94.74%)\n",
            "\n",
            "--- 1.992659568786621 seconds ---\n",
            "Epoch: 7\n",
            "49 0.0007 100.0\n",
            "99 0.0006 100.0\n",
            "149 0.0010 100.0\n",
            "199 0.0007 100.0\n",
            "249 0.0009 100.0\n",
            "299 0.0009 100.0\n",
            "349 0.0004 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0119, Accuracy: 1474/1559 (94.55%)\n",
            "\n",
            "--- 1.978245496749878 seconds ---\n",
            "Epoch: 8\n",
            "49 0.0009 100.0\n",
            "99 0.0007 100.0\n",
            "149 0.0003 100.0\n",
            "199 0.0005 100.0\n",
            "249 0.0006 100.0\n",
            "299 0.0005 100.0\n",
            "349 0.0005 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0115, Accuracy: 1479/1559 (94.87%)\n",
            "\n",
            "--- 1.9701764583587646 seconds ---\n",
            "Epoch: 9\n",
            "49 0.0006 100.0\n",
            "99 0.0003 100.0\n",
            "149 0.0005 100.0\n",
            "199 0.0003 100.0\n",
            "249 0.0004 100.0\n",
            "299 0.0003 100.0\n",
            "349 0.0003 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0113, Accuracy: 1480/1559 (94.93%)\n",
            "\n",
            "--- 2.769726514816284 seconds ---\n",
            "Epoch: 10\n",
            "49 0.0003 100.0\n",
            "99 0.0004 100.0\n",
            "149 0.0003 100.0\n",
            "199 0.0002 100.0\n",
            "249 0.0003 100.0\n",
            "299 0.0003 100.0\n",
            "349 0.0003 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0112, Accuracy: 1477/1559 (94.74%)\n",
            "\n",
            "--- 2.013824701309204 seconds ---\n"
          ]
        }
      ],
      "source": [
        "#isolet epoch 10 rff8gvsa\n",
        "class ArgumentParser:\n",
        "    def __init__(self):\n",
        "        self.args = {\n",
        "            'lr': 0.01,            # Learning rate for optimizing class representative\n",
        "            'gamma': 0.3,          # Kernel parameter for computing covariance\n",
        "            'epoch': 10,            # Epochs of training\n",
        "            'gorder': 8,           # Order of the cyclic group required for G-VSA (8 ou 16)\n",
        "            'dim': 10000,          # Dimension of hypervectors c le grand D\n",
        "            'seed': 43,            # Random seed for reproducing results\n",
        "            'resume': True,       # Resume from existing encoded hypervectors\n",
        "            'data_dir': './encoded_data', # Directory to save encoded data (hypervectors)\n",
        "            'dataset': 'isolet',    # Dataset choice choices=['mnist', 'fmnist', 'cifar', 'isolet', 'ucihar']\n",
        "            'raw_data_dir': './dataset',   # Raw data directory\n",
        "            'model': 'rff-gvsa'    # Model type choices=['rff-hdc', 'linear-hdc', 'rff-gvsa']\n",
        "        }\n",
        "        self.seed = self.args['seed']\n",
        "        self.lr = self.args['lr']\n",
        "        self.gamma = self.args['gamma']\n",
        "        self.epoch = self.args['epoch']\n",
        "        self.gorder = self.args['gorder']\n",
        "        self.dim = self.args['dim']\n",
        "        self.resume = self.args['resume']\n",
        "        self.data_dir = self.args['data_dir']\n",
        "        self.dataset = self.args['dataset']\n",
        "        self.raw_data_dir = self.args['raw_data_dir']\n",
        "        self.model = self.args['model']\n",
        "\n",
        "\n",
        "    def get_args(self):\n",
        "        return self.args\n",
        "args = ArgumentParser()\n",
        "torch.manual_seed(args.seed)\n",
        "np.random.seed(args.seed)\n",
        "if 'hdc' in args.model:\n",
        "    args.gorder = 2\n",
        "    print(\"Use binary HDC with random fourier features, ignoring gorder, set to 2.\")\n",
        "args.data_dir = f'{args.data_dir}/{args.dataset}_{args.model}_order{args.gorder}_gamma{args.gamma}_dim{args.dim}'\n",
        "try:\n",
        "    os.makedirs(args.data_dir)\n",
        "except FileExistsError:\n",
        "    print('Encoded data folder already exists')\n",
        "if not args.resume:\n",
        "    print('Encode the dataset into hypervectors and save')\n",
        "    encode_and_save(args)\n",
        "    print('Finish encoding and saving')\n",
        "print(f'Optimizing class representatives for {args.epoch} epochs')\n",
        "train(args)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## UCIHAR"
      ],
      "metadata": {
        "id": "yeuZT_FCxo0E"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a9f843c-5e85-4974-8f51-87f4a5f6a196",
        "id": "mXLLkidgxo0E"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encode the dataset into hypervectors and save\n",
            "# of channels of data 1\n",
            "# of training samples and test samples 7352 2947\n",
            "Encoding with random fourier features encoder.\n",
            "the threshold to discretize fourier features to group elements tensor([-1.1505, -0.6746, -0.3187,  0.0000,  0.3187,  0.6746,  1.1505])\n",
            "Encoded pixels to hypervectors with size:  torch.Size([256, 10000])\n",
            "Encoding training data...\n",
            "Start encoding data\n",
            "Finish encoding data\n",
            "Encoding test data...\n",
            "Start encoding data\n",
            "Finish encoding data\n",
            "Finish encoding and saving\n",
            "Optimizing class representatives for 1 epochs\n",
            "Loading encoded training data...\n",
            "Loading encoded test data...\n",
            "Size of encoded training data torch.Size([7352, 10000]) and test data torch.Size([2947, 10000])\n",
            "Epoch: 1\n",
            "49 0.6040 81.25\n",
            "99 0.1575 87.5\n",
            "149 0.1115 100.0\n",
            "199 0.0493 100.0\n",
            "249 0.1137 93.75\n",
            "299 0.0518 100.0\n",
            "349 0.0896 93.75\n",
            "399 0.0637 100.0\n",
            "449 0.1015 93.75\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0136, Accuracy: 2762/2947 (93.72%)\n",
            "\n",
            "--- 2.8710765838623047 seconds ---\n"
          ]
        }
      ],
      "source": [
        "#ucihar epoch 1 rffg8vsa\n",
        "class ArgumentParser:\n",
        "    def __init__(self):\n",
        "        self.args = {\n",
        "            'lr': 0.01,            # Learning rate for optimizing class representative\n",
        "            'gamma': 0.3,          # Kernel parameter for computing covariance\n",
        "            'epoch': 1,            # Epochs of training\n",
        "            'gorder': 8,           # Order of the cyclic group required for G-VSA (8 ou 16)\n",
        "            'dim': 10000,          # Dimension of hypervectors c le grand D\n",
        "            'seed': 43,            # Random seed for reproducing results\n",
        "            'resume': False,       # Resume from existing encoded hypervectors\n",
        "            'data_dir': './encoded_data', # Directory to save encoded data (hypervectors)\n",
        "            'dataset': 'ucihar',    # Dataset choice choices=['mnist', 'fmnist', 'cifar', 'isolet', 'ucihar']\n",
        "            'raw_data_dir': './dataset',   # Raw data directory\n",
        "            'model': 'rff-gvsa'    # Model type choices=['rff-hdc', 'linear-hdc', 'rff-gvsa']\n",
        "        }\n",
        "        self.seed = self.args['seed']\n",
        "        self.lr = self.args['lr']\n",
        "        self.gamma = self.args['gamma']\n",
        "        self.epoch = self.args['epoch']\n",
        "        self.gorder = self.args['gorder']\n",
        "        self.dim = self.args['dim']\n",
        "        self.resume = self.args['resume']\n",
        "        self.data_dir = self.args['data_dir']\n",
        "        self.dataset = self.args['dataset']\n",
        "        self.raw_data_dir = self.args['raw_data_dir']\n",
        "        self.model = self.args['model']\n",
        "\n",
        "\n",
        "    def get_args(self):\n",
        "        return self.args\n",
        "args = ArgumentParser()\n",
        "torch.manual_seed(args.seed)\n",
        "np.random.seed(args.seed)\n",
        "if 'hdc' in args.model:\n",
        "    args.gorder = 2\n",
        "    print(\"Use binary HDC with random fourier features, ignoring gorder, set to 2.\")\n",
        "args.data_dir = f'{args.data_dir}/{args.dataset}_{args.model}_order{args.gorder}_gamma{args.gamma}_dim{args.dim}'\n",
        "try:\n",
        "    os.makedirs(args.data_dir)\n",
        "except FileExistsError:\n",
        "    print('Encoded data folder already exists')\n",
        "if not args.resume:\n",
        "    print('Encode the dataset into hypervectors and save')\n",
        "    encode_and_save(args)\n",
        "    print('Finish encoding and saving')\n",
        "print(f'Optimizing class representatives for {args.epoch} epochs')\n",
        "train(args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6475c7d-99a8-49ec-dc07-46843801839a",
        "id": "S8v2A9KFxo0E"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoded data folder already exists\n",
            "Optimizing class representatives for 10 epochs\n",
            "Loading encoded training data...\n",
            "Loading encoded test data...\n",
            "Size of encoded training data torch.Size([7352, 10000]) and test data torch.Size([2947, 10000])\n",
            "Epoch: 1\n",
            "49 0.3539 93.75\n",
            "99 0.2014 100.0\n",
            "149 0.1689 100.0\n",
            "199 0.1881 93.75\n",
            "249 0.0837 93.75\n",
            "299 0.0740 100.0\n",
            "349 0.0995 93.75\n",
            "399 0.0474 100.0\n",
            "449 0.0732 93.75\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0130, Accuracy: 2768/2947 (93.93%)\n",
            "\n",
            "--- 2.4904892444610596 seconds ---\n",
            "Epoch: 2\n",
            "49 0.0066 100.0\n",
            "99 0.1260 93.75\n",
            "149 0.0055 100.0\n",
            "199 0.0907 93.75\n",
            "249 0.0157 100.0\n",
            "299 0.0167 100.0\n",
            "349 0.0130 100.0\n",
            "399 0.0025 100.0\n",
            "449 0.0046 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0090, Accuracy: 2789/2947 (94.64%)\n",
            "\n",
            "--- 2.4219865798950195 seconds ---\n",
            "Epoch: 3\n",
            "49 0.0052 100.0\n",
            "99 0.0016 100.0\n",
            "149 0.0032 100.0\n",
            "199 0.0104 100.0\n",
            "249 0.0010 100.0\n",
            "299 0.0031 100.0\n",
            "349 0.0222 100.0\n",
            "399 0.0034 100.0\n",
            "449 0.0062 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0091, Accuracy: 2785/2947 (94.50%)\n",
            "\n",
            "--- 2.4447195529937744 seconds ---\n",
            "Epoch: 4\n",
            "49 0.0023 100.0\n",
            "99 0.0027 100.0\n",
            "149 0.0023 100.0\n",
            "199 0.0021 100.0\n",
            "249 0.0005 100.0\n",
            "299 0.0135 100.0\n",
            "349 0.0013 100.0\n",
            "399 0.0004 100.0\n",
            "449 0.0113 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0081, Accuracy: 2801/2947 (95.05%)\n",
            "\n",
            "--- 3.156954050064087 seconds ---\n",
            "Epoch: 5\n",
            "49 0.0002 100.0\n",
            "99 0.0033 100.0\n",
            "149 0.0002 100.0\n",
            "199 0.0004 100.0\n",
            "249 0.0096 100.0\n",
            "299 0.0046 100.0\n",
            "349 0.0093 100.0\n",
            "399 0.0026 100.0\n",
            "449 0.0354 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0082, Accuracy: 2813/2947 (95.45%)\n",
            "\n",
            "--- 2.5038769245147705 seconds ---\n",
            "Epoch: 6\n",
            "49 0.0004 100.0\n",
            "99 0.0001 100.0\n",
            "149 0.0003 100.0\n",
            "199 0.0005 100.0\n",
            "249 0.0017 100.0\n",
            "299 0.0039 100.0\n",
            "349 0.0002 100.0\n",
            "399 0.0035 100.0\n",
            "449 0.0006 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0091, Accuracy: 2801/2947 (95.05%)\n",
            "\n",
            "--- 2.452476739883423 seconds ---\n",
            "Epoch: 7\n",
            "49 0.0004 100.0\n",
            "99 0.0010 100.0\n",
            "149 0.0013 100.0\n",
            "199 0.0012 100.0\n",
            "249 0.0004 100.0\n",
            "299 0.0007 100.0\n",
            "349 0.0005 100.0\n",
            "399 0.0001 100.0\n",
            "449 0.0001 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0086, Accuracy: 2804/2947 (95.15%)\n",
            "\n",
            "--- 2.4745585918426514 seconds ---\n",
            "Epoch: 8\n",
            "49 0.0005 100.0\n",
            "99 0.0001 100.0\n",
            "149 0.0004 100.0\n",
            "199 0.0004 100.0\n",
            "249 0.0003 100.0\n",
            "299 0.0000 100.0\n",
            "349 0.0000 100.0\n",
            "399 0.0015 100.0\n",
            "449 0.0001 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0081, Accuracy: 2820/2947 (95.69%)\n",
            "\n",
            "--- 2.4348013401031494 seconds ---\n",
            "Epoch: 9\n",
            "49 0.0010 100.0\n",
            "99 0.0005 100.0\n",
            "149 0.0005 100.0\n",
            "199 0.0010 100.0\n",
            "249 0.0005 100.0\n",
            "299 0.0003 100.0\n",
            "349 0.0003 100.0\n",
            "399 0.0008 100.0\n",
            "449 0.0002 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0087, Accuracy: 2813/2947 (95.45%)\n",
            "\n",
            "--- 3.189577102661133 seconds ---\n",
            "Epoch: 10\n",
            "49 0.0000 100.0\n",
            "99 0.0000 100.0\n",
            "149 0.0000 100.0\n",
            "199 0.0001 100.0\n",
            "249 0.0000 100.0\n",
            "299 0.0004 100.0\n",
            "349 0.0001 100.0\n",
            "399 0.0002 100.0\n",
            "449 0.0001 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0087, Accuracy: 2815/2947 (95.52%)\n",
            "\n",
            "--- 2.4504191875457764 seconds ---\n"
          ]
        }
      ],
      "source": [
        "#ucihar epoch 10 rffg8vsa\n",
        "class ArgumentParser:\n",
        "    def __init__(self):\n",
        "        self.args = {\n",
        "            'lr': 0.01,            # Learning rate for optimizing class representative\n",
        "            'gamma': 0.3,          # Kernel parameter for computing covariance\n",
        "            'epoch': 10,            # Epochs of training\n",
        "            'gorder': 8,           # Order of the cyclic group required for G-VSA (8 ou 16)\n",
        "            'dim': 10000,          # Dimension of hypervectors c le grand D\n",
        "            'seed': 43,            # Random seed for reproducing results\n",
        "            'resume': True,       # Resume from existing encoded hypervectors\n",
        "            'data_dir': './encoded_data', # Directory to save encoded data (hypervectors)\n",
        "            'dataset': 'ucihar',    # Dataset choice choices=['mnist', 'fmnist', 'cifar', 'isolet', 'ucihar']\n",
        "            'raw_data_dir': './dataset',   # Raw data directory\n",
        "            'model': 'rff-gvsa'    # Model type choices=['rff-hdc', 'linear-hdc', 'rff-gvsa']\n",
        "        }\n",
        "        self.seed = self.args['seed']\n",
        "        self.lr = self.args['lr']\n",
        "        self.gamma = self.args['gamma']\n",
        "        self.epoch = self.args['epoch']\n",
        "        self.gorder = self.args['gorder']\n",
        "        self.dim = self.args['dim']\n",
        "        self.resume = self.args['resume']\n",
        "        self.data_dir = self.args['data_dir']\n",
        "        self.dataset = self.args['dataset']\n",
        "        self.raw_data_dir = self.args['raw_data_dir']\n",
        "        self.model = self.args['model']\n",
        "\n",
        "\n",
        "    def get_args(self):\n",
        "        return self.args\n",
        "args = ArgumentParser()\n",
        "torch.manual_seed(args.seed)\n",
        "np.random.seed(args.seed)\n",
        "if 'hdc' in args.model:\n",
        "    args.gorder = 2\n",
        "    print(\"Use binary HDC with random fourier features, ignoring gorder, set to 2.\")\n",
        "args.data_dir = f'{args.data_dir}/{args.dataset}_{args.model}_order{args.gorder}_gamma{args.gamma}_dim{args.dim}'\n",
        "try:\n",
        "    os.makedirs(args.data_dir)\n",
        "except FileExistsError:\n",
        "    print('Encoded data folder already exists')\n",
        "if not args.resume:\n",
        "    print('Encode the dataset into hypervectors and save')\n",
        "    encode_and_save(args)\n",
        "    print('Finish encoding and saving')\n",
        "print(f'Optimizing class representatives for {args.epoch} epochs')\n",
        "train(args)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MNIST"
      ],
      "metadata": {
        "id": "6EMCIwaQxo0E"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "920eefe4-936d-48a4-e1c1-a8533e4db169",
        "id": "vU1vlH5jxo0E"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encode the dataset into hypervectors and save\n",
            "# of channels of data 1\n",
            "# of training samples and test samples 60000 10000\n",
            "Encoding with random fourier features encoder.\n",
            "the threshold to discretize fourier features to group elements tensor([-1.1505, -0.6746, -0.3187,  0.0000,  0.3187,  0.6746,  1.1505])\n",
            "Encoded pixels to hypervectors with size:  torch.Size([256, 10000])\n",
            "Encoding training data...\n",
            "Start encoding data\n",
            "12800 images encoded. Total time elapse = 23.032389163970947\n",
            "25600 images encoded. Total time elapse = 46.168821811676025\n",
            "38400 images encoded. Total time elapse = 69.22472286224365\n",
            "51200 images encoded. Total time elapse = 92.15534234046936\n",
            "Finish encoding data\n",
            "Encoding test data...\n",
            "Start encoding data\n",
            "Finish encoding data\n",
            "Finish encoding and saving\n",
            "Optimizing class representatives for 1 epochs\n",
            "Loading encoded training data...\n",
            "Loading encoded test data...\n",
            "Size of encoded training data torch.Size([60000, 10000]) and test data torch.Size([10000, 10000])\n",
            "Epoch: 1\n",
            "49 0.9797 100.0\n",
            "99 0.6518 75.0\n",
            "149 0.2406 93.75\n",
            "199 0.3080 81.25\n",
            "249 0.2574 93.75\n",
            "299 0.2459 87.5\n",
            "349 0.7164 75.0\n",
            "399 0.4385 75.0\n",
            "449 0.3001 93.75\n",
            "499 0.3635 87.5\n",
            "549 0.1867 93.75\n",
            "599 0.2267 87.5\n",
            "649 0.4737 81.25\n",
            "699 0.1046 93.75\n",
            "749 0.1301 100.0\n",
            "799 0.1817 100.0\n",
            "849 0.1715 93.75\n",
            "899 0.1191 93.75\n",
            "949 0.1988 93.75\n",
            "999 0.0364 100.0\n",
            "1049 0.3911 87.5\n",
            "1099 0.0204 100.0\n",
            "1149 0.1474 93.75\n",
            "1199 0.3047 93.75\n",
            "1249 0.0962 93.75\n",
            "1299 0.0963 93.75\n",
            "1349 0.0734 100.0\n",
            "1399 0.0135 100.0\n",
            "1449 0.1812 93.75\n",
            "1499 0.1000 93.75\n",
            "1549 0.0260 100.0\n",
            "1599 0.3636 87.5\n",
            "1649 0.0947 93.75\n",
            "1699 0.5046 93.75\n",
            "1749 0.0834 100.0\n",
            "1799 0.1715 87.5\n",
            "1849 0.1338 100.0\n",
            "1899 0.0365 100.0\n",
            "1949 0.2832 87.5\n",
            "1999 0.2300 93.75\n",
            "2049 0.1325 93.75\n",
            "2099 0.0700 100.0\n",
            "2149 0.0039 100.0\n",
            "2199 0.0796 93.75\n",
            "2249 0.1023 93.75\n",
            "2299 0.0174 100.0\n",
            "2349 0.0098 100.0\n",
            "2399 0.2360 87.5\n",
            "2449 0.0630 100.0\n",
            "2499 0.0228 100.0\n",
            "2549 0.2218 93.75\n",
            "2599 0.0021 100.0\n",
            "2649 0.5029 87.5\n",
            "2699 0.0031 100.0\n",
            "2749 0.0308 100.0\n",
            "2799 0.0131 100.0\n",
            "2849 0.4529 93.75\n",
            "2899 0.0682 93.75\n",
            "2949 0.0150 100.0\n",
            "2999 0.2700 93.75\n",
            "3049 0.0657 100.0\n",
            "3099 0.0100 100.0\n",
            "3149 0.0017 100.0\n",
            "3199 0.2102 93.75\n",
            "3249 0.3016 81.25\n",
            "3299 0.1066 93.75\n",
            "3349 0.2871 87.5\n",
            "3399 0.1194 93.75\n",
            "3449 0.3093 93.75\n",
            "3499 0.0145 100.0\n",
            "3549 0.0065 100.0\n",
            "3599 0.0198 100.0\n",
            "3649 0.0326 100.0\n",
            "3699 0.0068 100.0\n",
            "3749 0.0089 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0080, Accuracy: 9589/10000 (95.89%)\n",
            "\n",
            "--- 17.058554649353027 seconds ---\n"
          ]
        }
      ],
      "source": [
        "#mnist epoch 1 rffg8vsa\n",
        "class ArgumentParser:\n",
        "    def __init__(self):\n",
        "        self.args = {\n",
        "            'lr': 0.01,            # Learning rate for optimizing class representative\n",
        "            'gamma': 0.3,          # Kernel parameter for computing covariance\n",
        "            'epoch': 1,            # Epochs of training\n",
        "            'gorder': 8,           # Order of the cyclic group required for G-VSA (8 ou 16)\n",
        "            'dim': 10000,          # Dimension of hypervectors c le grand D\n",
        "            'seed': 43,            # Random seed for reproducing results\n",
        "            'resume': False,       # Resume from existing encoded hypervectors\n",
        "            'data_dir': './encoded_data', # Directory to save encoded data (hypervectors)\n",
        "            'dataset': 'mnist',    # Dataset choice choices=['mnist', 'fmnist', 'cifar', 'isolet', 'ucihar']\n",
        "            'raw_data_dir': './dataset',   # Raw data directory\n",
        "            'model': 'rff-gvsa'    # Model type choices=['rff-hdc', 'linear-hdc', 'rff-gvsa']\n",
        "        }\n",
        "        self.seed = self.args['seed']\n",
        "        self.lr = self.args['lr']\n",
        "        self.gamma = self.args['gamma']\n",
        "        self.epoch = self.args['epoch']\n",
        "        self.gorder = self.args['gorder']\n",
        "        self.dim = self.args['dim']\n",
        "        self.resume = self.args['resume']\n",
        "        self.data_dir = self.args['data_dir']\n",
        "        self.dataset = self.args['dataset']\n",
        "        self.raw_data_dir = self.args['raw_data_dir']\n",
        "        self.model = self.args['model']\n",
        "\n",
        "\n",
        "    def get_args(self):\n",
        "        return self.args\n",
        "args = ArgumentParser()\n",
        "torch.manual_seed(args.seed)\n",
        "np.random.seed(args.seed)\n",
        "if 'hdc' in args.model:\n",
        "    args.gorder = 2\n",
        "    print(\"Use binary HDC with random fourier features, ignoring gorder, set to 2.\")\n",
        "args.data_dir = f'{args.data_dir}/{args.dataset}_{args.model}_order{args.gorder}_gamma{args.gamma}_dim{args.dim}'\n",
        "try:\n",
        "    os.makedirs(args.data_dir)\n",
        "except FileExistsError:\n",
        "    print('Encoded data folder already exists')\n",
        "if not args.resume:\n",
        "    print('Encode the dataset into hypervectors and save')\n",
        "    encode_and_save(args)\n",
        "    print('Finish encoding and saving')\n",
        "print(f'Optimizing class representatives for {args.epoch} epochs')\n",
        "train(args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c6b8f6f-a8e4-4a86-9d08-782eb944cee1",
        "id": "hj8m-IoExo0F"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoded data folder already exists\n",
            "Optimizing class representatives for 10 epochs\n",
            "Loading encoded training data...\n",
            "Loading encoded test data...\n",
            "Size of encoded training data torch.Size([60000, 10000]) and test data torch.Size([10000, 10000])\n",
            "Epoch: 1\n",
            "49 1.5183 62.5\n",
            "99 0.5644 81.25\n",
            "149 0.2876 93.75\n",
            "199 0.2138 100.0\n",
            "249 0.1203 93.75\n",
            "299 0.3441 87.5\n",
            "349 0.4103 81.25\n",
            "399 0.2338 93.75\n",
            "449 0.1695 93.75\n",
            "499 0.2503 87.5\n",
            "549 0.2024 93.75\n",
            "599 0.5539 81.25\n",
            "649 0.0518 100.0\n",
            "699 0.1080 93.75\n",
            "749 0.1177 93.75\n",
            "799 0.0948 93.75\n",
            "849 0.4120 87.5\n",
            "899 0.1409 93.75\n",
            "949 0.0621 100.0\n",
            "999 0.0705 100.0\n",
            "1049 0.0661 100.0\n",
            "1099 0.1561 93.75\n",
            "1149 0.0307 100.0\n",
            "1199 0.1392 93.75\n",
            "1249 0.0372 100.0\n",
            "1299 0.1060 93.75\n",
            "1349 0.0049 100.0\n",
            "1399 0.1702 93.75\n",
            "1449 0.0126 100.0\n",
            "1499 0.0854 100.0\n",
            "1549 0.0029 100.0\n",
            "1599 0.0355 100.0\n",
            "1649 0.1042 93.75\n",
            "1699 0.0269 100.0\n",
            "1749 0.0570 100.0\n",
            "1799 0.1607 87.5\n",
            "1849 0.0530 100.0\n",
            "1899 0.0902 100.0\n",
            "1949 0.0568 100.0\n",
            "1999 0.0211 100.0\n",
            "2049 0.0168 100.0\n",
            "2099 0.3495 93.75\n",
            "2149 0.0634 100.0\n",
            "2199 0.0249 100.0\n",
            "2249 0.4560 87.5\n",
            "2299 0.0214 100.0\n",
            "2349 0.0135 100.0\n",
            "2399 0.0664 100.0\n",
            "2449 0.1423 93.75\n",
            "2499 0.0156 100.0\n",
            "2549 0.1169 100.0\n",
            "2599 0.2385 93.75\n",
            "2649 0.1755 93.75\n",
            "2699 0.0183 100.0\n",
            "2749 0.1142 100.0\n",
            "2799 0.0484 100.0\n",
            "2849 0.0373 100.0\n",
            "2899 0.0311 100.0\n",
            "2949 0.2030 93.75\n",
            "2999 0.0818 100.0\n",
            "3049 0.0062 100.0\n",
            "3099 0.0441 100.0\n",
            "3149 0.1898 87.5\n",
            "3199 0.3518 87.5\n",
            "3249 0.1188 93.75\n",
            "3299 0.1879 93.75\n",
            "3349 0.0095 100.0\n",
            "3399 0.0740 100.0\n",
            "3449 0.0681 100.0\n",
            "3499 0.1041 93.75\n",
            "3549 0.0107 100.0\n",
            "3599 0.0357 100.0\n",
            "3649 0.1017 100.0\n",
            "3699 0.0980 100.0\n",
            "3749 0.0526 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0083, Accuracy: 9585/10000 (95.85%)\n",
            "\n",
            "--- 17.593474864959717 seconds ---\n",
            "Epoch: 2\n",
            "49 0.0690 93.75\n",
            "99 0.0148 100.0\n",
            "149 0.0167 100.0\n",
            "199 0.0533 100.0\n",
            "249 0.0147 100.0\n",
            "299 0.0019 100.0\n",
            "349 0.0018 100.0\n",
            "399 0.0015 100.0\n",
            "449 0.0035 100.0\n",
            "499 0.0051 100.0\n",
            "549 0.0184 100.0\n",
            "599 0.1591 93.75\n",
            "649 0.0024 100.0\n",
            "699 0.0011 100.0\n",
            "749 0.0106 100.0\n",
            "799 0.0004 100.0\n",
            "849 0.0219 100.0\n",
            "899 0.1268 93.75\n",
            "949 0.0002 100.0\n",
            "999 0.0075 100.0\n",
            "1049 0.0039 100.0\n",
            "1099 0.0010 100.0\n",
            "1149 0.0490 93.75\n",
            "1199 0.0073 100.0\n",
            "1249 0.0086 100.0\n",
            "1299 0.0055 100.0\n",
            "1349 0.0002 100.0\n",
            "1399 0.0048 100.0\n",
            "1449 0.4286 93.75\n",
            "1499 0.0010 100.0\n",
            "1549 0.0322 100.0\n",
            "1599 0.1450 93.75\n",
            "1649 0.0006 100.0\n",
            "1699 0.0835 93.75\n",
            "1749 0.0130 100.0\n",
            "1799 0.0013 100.0\n",
            "1849 0.0043 100.0\n",
            "1899 0.0307 100.0\n",
            "1949 0.0251 100.0\n",
            "1999 0.0007 100.0\n",
            "2049 0.0006 100.0\n",
            "2099 0.0055 100.0\n",
            "2149 0.0022 100.0\n",
            "2199 0.0002 100.0\n",
            "2249 0.0008 100.0\n",
            "2299 0.0043 100.0\n",
            "2349 0.0011 100.0\n",
            "2399 0.0029 100.0\n",
            "2449 0.0014 100.0\n",
            "2499 0.0004 100.0\n",
            "2549 0.0009 100.0\n",
            "2599 0.0011 100.0\n",
            "2649 0.0001 100.0\n",
            "2699 0.0027 100.0\n",
            "2749 0.0068 100.0\n",
            "2799 0.0052 100.0\n",
            "2849 0.0424 100.0\n",
            "2899 0.0681 93.75\n",
            "2949 0.0860 93.75\n",
            "2999 0.0016 100.0\n",
            "3049 0.1708 93.75\n",
            "3099 0.0041 100.0\n",
            "3149 0.0134 100.0\n",
            "3199 0.4698 93.75\n",
            "3249 0.0222 100.0\n",
            "3299 0.0471 100.0\n",
            "3349 0.0006 100.0\n",
            "3399 0.0642 100.0\n",
            "3449 0.0029 100.0\n",
            "3499 0.0214 100.0\n",
            "3549 0.0027 100.0\n",
            "3599 0.0286 100.0\n",
            "3649 0.0000 100.0\n",
            "3699 0.0000 100.0\n",
            "3749 0.0524 93.75\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0103, Accuracy: 9555/10000 (95.55%)\n",
            "\n",
            "--- 17.170231103897095 seconds ---\n",
            "Epoch: 3\n",
            "49 0.2899 93.75\n",
            "99 0.0063 100.0\n",
            "149 0.0018 100.0\n",
            "199 0.1290 93.75\n",
            "249 0.0302 100.0\n",
            "299 0.0078 100.0\n",
            "349 0.0073 100.0\n",
            "399 0.0003 100.0\n",
            "449 0.0009 100.0\n",
            "499 0.0005 100.0\n",
            "549 0.0035 100.0\n",
            "599 0.0093 100.0\n",
            "649 0.0001 100.0\n",
            "699 0.0019 100.0\n",
            "749 0.0362 100.0\n",
            "799 0.0012 100.0\n",
            "849 0.0431 100.0\n",
            "899 0.0000 100.0\n",
            "949 0.0006 100.0\n",
            "999 0.1003 93.75\n",
            "1049 0.0002 100.0\n",
            "1099 0.0125 100.0\n",
            "1149 0.0001 100.0\n",
            "1199 0.0001 100.0\n",
            "1249 0.0001 100.0\n",
            "1299 0.0016 100.0\n",
            "1349 0.0322 100.0\n",
            "1399 0.3634 93.75\n",
            "1449 0.0091 100.0\n",
            "1499 0.0009 100.0\n",
            "1549 0.0038 100.0\n",
            "1599 0.0001 100.0\n",
            "1649 0.0001 100.0\n",
            "1699 0.0000 100.0\n",
            "1749 0.0002 100.0\n",
            "1799 0.0240 100.0\n",
            "1849 0.2289 93.75\n",
            "1899 0.0005 100.0\n",
            "1949 0.0004 100.0\n",
            "1999 0.0002 100.0\n",
            "2049 0.0002 100.0\n",
            "2099 0.0280 100.0\n",
            "2149 0.0966 93.75\n",
            "2199 0.0019 100.0\n",
            "2249 0.0000 100.0\n",
            "2299 0.0004 100.0\n",
            "2349 0.0005 100.0\n",
            "2399 0.0107 100.0\n",
            "2449 0.0142 100.0\n",
            "2499 0.0017 100.0\n",
            "2549 0.0026 100.0\n",
            "2599 0.0038 100.0\n",
            "2649 0.0042 100.0\n",
            "2699 0.0386 100.0\n",
            "2749 0.0445 100.0\n",
            "2799 0.0007 100.0\n",
            "2849 0.0223 100.0\n",
            "2899 0.0000 100.0\n",
            "2949 0.0003 100.0\n",
            "2999 0.0037 100.0\n",
            "3049 0.7665 93.75\n",
            "3099 0.0331 100.0\n",
            "3149 0.0002 100.0\n",
            "3199 0.1028 93.75\n",
            "3249 0.0005 100.0\n",
            "3299 0.0103 100.0\n",
            "3349 0.0431 100.0\n",
            "3399 0.0009 100.0\n",
            "3449 0.0673 93.75\n",
            "3499 0.0000 100.0\n",
            "3549 0.0096 100.0\n",
            "3599 0.0001 100.0\n",
            "3649 0.0545 93.75\n",
            "3699 0.0055 100.0\n",
            "3749 0.0082 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0120, Accuracy: 9562/10000 (95.62%)\n",
            "\n",
            "--- 16.942221879959106 seconds ---\n",
            "Epoch: 4\n",
            "49 0.0559 93.75\n",
            "99 0.0000 100.0\n",
            "149 0.0082 100.0\n",
            "199 0.0012 100.0\n",
            "249 0.0002 100.0\n",
            "299 0.1435 93.75\n",
            "349 0.0000 100.0\n",
            "399 0.0002 100.0\n",
            "449 0.0008 100.0\n",
            "499 0.0003 100.0\n",
            "549 0.0009 100.0\n",
            "599 0.0006 100.0\n",
            "649 0.0000 100.0\n",
            "699 0.0051 100.0\n",
            "749 0.0127 100.0\n",
            "799 0.0000 100.0\n",
            "849 0.0001 100.0\n",
            "899 0.0003 100.0\n",
            "949 0.0021 100.0\n",
            "999 0.0533 93.75\n",
            "1049 0.0229 100.0\n",
            "1099 0.0000 100.0\n",
            "1149 0.0001 100.0\n",
            "1199 0.0001 100.0\n",
            "1249 0.0000 100.0\n",
            "1299 0.0035 100.0\n",
            "1349 0.0001 100.0\n",
            "1399 0.0147 100.0\n",
            "1449 0.1256 93.75\n",
            "1499 0.0080 100.0\n",
            "1549 0.0041 100.0\n",
            "1599 0.0001 100.0\n",
            "1649 0.0001 100.0\n",
            "1699 0.0003 100.0\n",
            "1749 0.3358 93.75\n",
            "1799 0.0001 100.0\n",
            "1849 0.0000 100.0\n",
            "1899 0.0045 100.0\n",
            "1949 0.0059 100.0\n",
            "1999 0.0038 100.0\n",
            "2049 0.0020 100.0\n",
            "2099 0.0009 100.0\n",
            "2149 0.0092 100.0\n",
            "2199 0.0008 100.0\n",
            "2249 0.0211 100.0\n",
            "2299 0.0003 100.0\n",
            "2349 0.0004 100.0\n",
            "2399 0.0000 100.0\n",
            "2449 0.0007 100.0\n",
            "2499 0.0005 100.0\n",
            "2549 0.0001 100.0\n",
            "2599 0.0019 100.0\n",
            "2649 0.0136 100.0\n",
            "2699 0.0025 100.0\n",
            "2749 0.0024 100.0\n",
            "2799 0.0150 100.0\n",
            "2849 0.0020 100.0\n",
            "2899 0.0000 100.0\n",
            "2949 0.0575 93.75\n",
            "2999 0.0004 100.0\n",
            "3049 0.0157 100.0\n",
            "3099 0.0002 100.0\n",
            "3149 0.0014 100.0\n",
            "3199 0.0002 100.0\n",
            "3249 0.0008 100.0\n",
            "3299 0.2319 93.75\n",
            "3349 0.0300 100.0\n",
            "3399 0.0000 100.0\n",
            "3449 0.0005 100.0\n",
            "3499 0.4767 93.75\n",
            "3549 0.0010 100.0\n",
            "3599 0.0013 100.0\n",
            "3649 0.0007 100.0\n",
            "3699 0.0006 100.0\n",
            "3749 0.0111 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0115, Accuracy: 9589/10000 (95.89%)\n",
            "\n",
            "--- 17.82400345802307 seconds ---\n",
            "Epoch: 5\n",
            "49 0.0001 100.0\n",
            "99 0.0000 100.0\n",
            "149 0.0001 100.0\n",
            "199 0.0078 100.0\n",
            "249 0.0224 100.0\n",
            "299 0.0000 100.0\n",
            "349 0.0022 100.0\n",
            "399 0.0073 100.0\n",
            "449 0.0687 93.75\n",
            "499 0.0007 100.0\n",
            "549 0.0005 100.0\n",
            "599 0.0570 100.0\n",
            "649 0.0001 100.0\n",
            "699 0.0001 100.0\n",
            "749 0.0144 100.0\n",
            "799 0.0005 100.0\n",
            "849 0.0015 100.0\n",
            "899 0.0058 100.0\n",
            "949 0.0001 100.0\n",
            "999 0.0172 100.0\n",
            "1049 0.0001 100.0\n",
            "1099 0.0129 100.0\n",
            "1149 0.0021 100.0\n",
            "1199 0.0004 100.0\n",
            "1249 0.0008 100.0\n",
            "1299 0.0000 100.0\n",
            "1349 0.1252 93.75\n",
            "1399 0.0224 100.0\n",
            "1449 0.0000 100.0\n",
            "1499 0.0008 100.0\n",
            "1549 0.0004 100.0\n",
            "1599 0.0016 100.0\n",
            "1649 0.0002 100.0\n",
            "1699 0.0001 100.0\n",
            "1749 0.0057 100.0\n",
            "1799 0.0804 93.75\n",
            "1849 0.0234 100.0\n",
            "1899 0.0000 100.0\n",
            "1949 0.0001 100.0\n",
            "1999 0.0055 100.0\n",
            "2049 0.0003 100.0\n",
            "2099 0.0000 100.0\n",
            "2149 0.0003 100.0\n",
            "2199 0.0000 100.0\n",
            "2249 0.0006 100.0\n",
            "2299 0.0030 100.0\n",
            "2349 0.0004 100.0\n",
            "2399 0.0003 100.0\n",
            "2449 0.1590 93.75\n",
            "2499 0.0006 100.0\n",
            "2549 0.0420 100.0\n",
            "2599 0.0380 100.0\n",
            "2649 0.0349 100.0\n",
            "2699 0.0074 100.0\n",
            "2749 0.0009 100.0\n",
            "2799 0.0000 100.0\n",
            "2849 0.0898 93.75\n",
            "2899 0.0004 100.0\n",
            "2949 0.0001 100.0\n",
            "2999 0.0000 100.0\n",
            "3049 0.0001 100.0\n",
            "3099 0.0019 100.0\n",
            "3149 0.0994 93.75\n",
            "3199 0.0002 100.0\n",
            "3249 0.0036 100.0\n",
            "3299 0.0441 100.0\n",
            "3349 0.0000 100.0\n",
            "3399 0.0001 100.0\n",
            "3449 0.0000 100.0\n",
            "3499 0.0001 100.0\n",
            "3549 0.0052 100.0\n",
            "3599 0.0001 100.0\n",
            "3649 0.0476 93.75\n",
            "3699 0.0012 100.0\n",
            "3749 0.0011 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0131, Accuracy: 9546/10000 (95.46%)\n",
            "\n",
            "--- 16.852012634277344 seconds ---\n",
            "Epoch: 6\n",
            "49 0.0005 100.0\n",
            "99 0.0013 100.0\n",
            "149 0.0003 100.0\n",
            "199 0.0001 100.0\n",
            "249 0.0036 100.0\n",
            "299 0.0003 100.0\n",
            "349 0.0013 100.0\n",
            "399 0.0000 100.0\n",
            "449 0.0001 100.0\n",
            "499 0.0352 100.0\n",
            "549 0.0008 100.0\n",
            "599 0.0046 100.0\n",
            "649 0.0150 100.0\n",
            "699 0.0008 100.0\n",
            "749 0.0022 100.0\n",
            "799 0.0918 93.75\n",
            "849 0.0058 100.0\n",
            "899 0.0000 100.0\n",
            "949 0.0364 100.0\n",
            "999 0.0455 93.75\n",
            "1049 0.0001 100.0\n",
            "1099 0.0000 100.0\n",
            "1149 0.0006 100.0\n",
            "1199 0.0040 100.0\n",
            "1249 0.0001 100.0\n",
            "1299 0.0002 100.0\n",
            "1349 0.0003 100.0\n",
            "1399 0.0006 100.0\n",
            "1449 0.0001 100.0\n",
            "1499 0.0003 100.0\n",
            "1549 0.0013 100.0\n",
            "1599 0.0270 100.0\n",
            "1649 0.0000 100.0\n",
            "1699 0.0017 100.0\n",
            "1749 0.0032 100.0\n",
            "1799 0.0002 100.0\n",
            "1849 0.0000 100.0\n",
            "1899 0.0034 100.0\n",
            "1949 0.0000 100.0\n",
            "1999 0.0000 100.0\n",
            "2049 0.0044 100.0\n",
            "2099 0.0323 100.0\n",
            "2149 0.0000 100.0\n",
            "2199 0.0001 100.0\n",
            "2249 0.0007 100.0\n",
            "2299 0.0005 100.0\n",
            "2349 0.0000 100.0\n",
            "2399 0.0002 100.0\n",
            "2449 0.0003 100.0\n",
            "2499 0.0001 100.0\n",
            "2549 0.0001 100.0\n",
            "2599 0.0002 100.0\n",
            "2649 0.0077 100.0\n",
            "2699 0.0040 100.0\n",
            "2749 0.0053 100.0\n",
            "2799 0.0005 100.0\n",
            "2849 0.0006 100.0\n",
            "2899 0.0006 100.0\n",
            "2949 0.0065 100.0\n",
            "2999 0.0000 100.0\n",
            "3049 0.0005 100.0\n",
            "3099 0.0006 100.0\n",
            "3149 0.0027 100.0\n",
            "3199 0.0009 100.0\n",
            "3249 0.0063 100.0\n",
            "3299 0.0001 100.0\n",
            "3349 0.0008 100.0\n",
            "3399 0.0001 100.0\n",
            "3449 0.0002 100.0\n",
            "3499 0.0000 100.0\n",
            "3549 0.0002 100.0\n",
            "3599 0.0037 100.0\n",
            "3649 0.2120 93.75\n",
            "3699 0.0003 100.0\n",
            "3749 0.0001 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0137, Accuracy: 9568/10000 (95.68%)\n",
            "\n",
            "--- 17.305640697479248 seconds ---\n",
            "Epoch: 7\n",
            "49 0.0776 93.75\n",
            "99 0.0009 100.0\n",
            "149 0.0002 100.0\n",
            "199 0.0006 100.0\n",
            "249 0.0000 100.0\n",
            "299 0.0152 100.0\n",
            "349 0.0009 100.0\n",
            "399 0.0000 100.0\n",
            "449 0.0012 100.0\n",
            "499 0.0001 100.0\n",
            "549 0.0075 100.0\n",
            "599 0.0000 100.0\n",
            "649 0.0002 100.0\n",
            "699 0.0000 100.0\n",
            "749 0.0102 100.0\n",
            "799 0.0003 100.0\n",
            "849 0.0283 100.0\n",
            "899 0.0001 100.0\n",
            "949 0.0004 100.0\n",
            "999 0.0018 100.0\n",
            "1049 0.0000 100.0\n",
            "1099 0.0001 100.0\n",
            "1149 0.0002 100.0\n",
            "1199 0.0010 100.0\n",
            "1249 0.0614 93.75\n",
            "1299 0.0001 100.0\n",
            "1349 0.0000 100.0\n",
            "1399 0.0002 100.0\n",
            "1449 0.0001 100.0\n",
            "1499 0.0008 100.0\n",
            "1549 0.0029 100.0\n",
            "1599 0.0000 100.0\n",
            "1649 0.0019 100.0\n",
            "1699 0.0023 100.0\n",
            "1749 0.0012 100.0\n",
            "1799 0.0003 100.0\n",
            "1849 0.0593 93.75\n",
            "1899 0.0001 100.0\n",
            "1949 0.1590 93.75\n",
            "1999 0.0000 100.0\n",
            "2049 0.0023 100.0\n",
            "2099 0.0033 100.0\n",
            "2149 0.0000 100.0\n",
            "2199 0.0014 100.0\n",
            "2249 0.0001 100.0\n",
            "2299 0.0000 100.0\n",
            "2349 0.0000 100.0\n",
            "2399 0.0004 100.0\n",
            "2449 0.0002 100.0\n",
            "2499 0.0001 100.0\n",
            "2549 0.0023 100.0\n",
            "2599 0.0000 100.0\n",
            "2649 0.0000 100.0\n",
            "2699 0.0000 100.0\n",
            "2749 0.0020 100.0\n",
            "2799 0.0005 100.0\n",
            "2849 0.0179 100.0\n",
            "2899 0.0001 100.0\n",
            "2949 0.0001 100.0\n",
            "2999 0.0003 100.0\n",
            "3049 0.0109 100.0\n",
            "3099 0.1183 93.75\n",
            "3149 0.0002 100.0\n",
            "3199 0.0003 100.0\n",
            "3249 0.0462 93.75\n",
            "3299 0.0005 100.0\n",
            "3349 0.0006 100.0\n",
            "3399 0.0002 100.0\n",
            "3449 0.0046 100.0\n",
            "3499 0.0000 100.0\n",
            "3549 0.0378 100.0\n",
            "3599 0.0000 100.0\n",
            "3649 0.0000 100.0\n",
            "3699 0.0028 100.0\n",
            "3749 0.0010 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0138, Accuracy: 9566/10000 (95.66%)\n",
            "\n",
            "--- 17.77397847175598 seconds ---\n",
            "Epoch: 8\n",
            "49 0.0003 100.0\n",
            "99 0.0012 100.0\n",
            "149 0.0016 100.0\n",
            "199 0.0143 100.0\n",
            "249 0.0000 100.0\n",
            "299 0.0000 100.0\n",
            "349 0.0045 100.0\n",
            "399 0.0018 100.0\n",
            "449 0.0002 100.0\n",
            "499 0.0005 100.0\n",
            "549 0.0001 100.0\n",
            "599 0.0000 100.0\n",
            "649 0.0007 100.0\n",
            "699 0.0000 100.0\n",
            "749 0.0003 100.0\n",
            "799 0.0014 100.0\n",
            "849 0.0000 100.0\n",
            "899 0.0001 100.0\n",
            "949 0.0001 100.0\n",
            "999 0.0269 100.0\n",
            "1049 0.0027 100.0\n",
            "1099 0.0002 100.0\n",
            "1149 0.0000 100.0\n",
            "1199 0.0000 100.0\n",
            "1249 0.2937 93.75\n",
            "1299 0.0002 100.0\n",
            "1349 0.2262 93.75\n",
            "1399 0.0211 100.0\n",
            "1449 0.0755 93.75\n",
            "1499 0.0001 100.0\n",
            "1549 0.0022 100.0\n",
            "1599 0.0001 100.0\n",
            "1649 0.0000 100.0\n",
            "1699 0.0000 100.0\n",
            "1749 0.0003 100.0\n",
            "1799 0.0004 100.0\n",
            "1849 0.0017 100.0\n",
            "1899 0.0001 100.0\n",
            "1949 0.0000 100.0\n",
            "1999 0.0001 100.0\n",
            "2049 0.0000 100.0\n",
            "2099 0.0000 100.0\n",
            "2149 0.0000 100.0\n",
            "2199 0.0002 100.0\n",
            "2249 0.0008 100.0\n",
            "2299 0.0013 100.0\n",
            "2349 0.0000 100.0\n",
            "2399 0.0003 100.0\n",
            "2449 0.0003 100.0\n",
            "2499 0.0003 100.0\n",
            "2549 0.0001 100.0\n",
            "2599 0.0000 100.0\n",
            "2649 0.0000 100.0\n",
            "2699 0.0023 100.0\n",
            "2749 0.0016 100.0\n",
            "2799 0.0000 100.0\n",
            "2849 0.0000 100.0\n",
            "2899 0.1356 93.75\n",
            "2949 0.0036 100.0\n",
            "2999 0.0000 100.0\n",
            "3049 0.1284 93.75\n",
            "3099 0.0158 100.0\n",
            "3149 0.0567 93.75\n",
            "3199 0.0000 100.0\n",
            "3249 0.0000 100.0\n",
            "3299 0.0000 100.0\n",
            "3349 0.0004 100.0\n",
            "3399 0.0002 100.0\n",
            "3449 0.0033 100.0\n",
            "3499 0.0006 100.0\n",
            "3549 0.0014 100.0\n",
            "3599 0.0023 100.0\n",
            "3649 0.0008 100.0\n",
            "3699 0.0016 100.0\n",
            "3749 0.0013 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0158, Accuracy: 9550/10000 (95.50%)\n",
            "\n",
            "--- 16.88283109664917 seconds ---\n",
            "Epoch: 9\n",
            "49 0.0026 100.0\n",
            "99 0.0000 100.0\n",
            "149 0.0000 100.0\n",
            "199 0.0000 100.0\n",
            "249 0.0050 100.0\n",
            "299 0.0004 100.0\n",
            "349 0.0000 100.0\n",
            "399 0.0049 100.0\n",
            "449 0.0002 100.0\n",
            "499 0.0133 100.0\n",
            "549 0.0000 100.0\n",
            "599 0.0002 100.0\n",
            "649 0.0028 100.0\n",
            "699 0.0000 100.0\n",
            "749 0.0014 100.0\n",
            "799 0.0003 100.0\n",
            "849 0.0001 100.0\n",
            "899 0.0027 100.0\n",
            "949 0.0000 100.0\n",
            "999 0.0016 100.0\n",
            "1049 0.0000 100.0\n",
            "1099 0.0001 100.0\n",
            "1149 0.0000 100.0\n",
            "1199 0.0000 100.0\n",
            "1249 0.0000 100.0\n",
            "1299 0.0002 100.0\n",
            "1349 0.0017 100.0\n",
            "1399 0.0010 100.0\n",
            "1449 0.0080 100.0\n",
            "1499 0.0000 100.0\n",
            "1549 0.0003 100.0\n",
            "1599 0.0001 100.0\n",
            "1649 0.0007 100.0\n",
            "1699 0.0004 100.0\n",
            "1749 0.0479 93.75\n",
            "1799 0.0000 100.0\n",
            "1849 0.0000 100.0\n",
            "1899 0.0402 100.0\n",
            "1949 0.0002 100.0\n",
            "1999 0.0001 100.0\n",
            "2049 0.0000 100.0\n",
            "2099 0.0023 100.0\n",
            "2149 0.0000 100.0\n",
            "2199 0.0000 100.0\n",
            "2249 0.0593 93.75\n",
            "2299 0.0034 100.0\n",
            "2349 0.0000 100.0\n",
            "2399 0.0000 100.0\n",
            "2449 0.0000 100.0\n",
            "2499 0.0001 100.0\n",
            "2549 0.0003 100.0\n",
            "2599 0.0004 100.0\n",
            "2649 0.0002 100.0\n",
            "2699 0.0021 100.0\n",
            "2749 0.0021 100.0\n",
            "2799 0.0000 100.0\n",
            "2849 0.0091 100.0\n",
            "2899 0.0000 100.0\n",
            "2949 0.0001 100.0\n",
            "2999 0.0029 100.0\n",
            "3049 0.0002 100.0\n",
            "3099 0.0002 100.0\n",
            "3149 0.0001 100.0\n",
            "3199 0.0036 100.0\n",
            "3249 0.0022 100.0\n",
            "3299 0.0030 100.0\n",
            "3349 0.0000 100.0\n",
            "3399 0.0002 100.0\n",
            "3449 0.0003 100.0\n",
            "3499 0.0001 100.0\n",
            "3549 0.0017 100.0\n",
            "3599 0.0016 100.0\n",
            "3649 0.0004 100.0\n",
            "3699 0.0019 100.0\n",
            "3749 0.0000 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0139, Accuracy: 9573/10000 (95.73%)\n",
            "\n",
            "--- 16.966814756393433 seconds ---\n",
            "Epoch: 10\n",
            "49 0.0001 100.0\n",
            "99 0.0001 100.0\n",
            "149 0.0000 100.0\n",
            "199 0.0001 100.0\n",
            "249 0.0008 100.0\n",
            "299 0.0000 100.0\n",
            "349 0.0004 100.0\n",
            "399 0.0001 100.0\n",
            "449 0.0083 100.0\n",
            "499 0.0002 100.0\n",
            "549 0.0018 100.0\n",
            "599 0.0003 100.0\n",
            "649 0.0001 100.0\n",
            "699 0.0000 100.0\n",
            "749 0.0000 100.0\n",
            "799 0.0021 100.0\n",
            "849 0.0299 100.0\n",
            "899 0.0000 100.0\n",
            "949 0.0001 100.0\n",
            "999 0.0001 100.0\n",
            "1049 0.0010 100.0\n",
            "1099 0.0001 100.0\n",
            "1149 0.0001 100.0\n",
            "1199 0.0001 100.0\n",
            "1249 0.0115 100.0\n",
            "1299 0.0003 100.0\n",
            "1349 0.0000 100.0\n",
            "1399 0.0000 100.0\n",
            "1449 0.0000 100.0\n",
            "1499 0.0001 100.0\n",
            "1549 0.0001 100.0\n",
            "1599 0.0000 100.0\n",
            "1649 0.0000 100.0\n",
            "1699 0.0004 100.0\n",
            "1749 0.0015 100.0\n",
            "1799 0.0009 100.0\n",
            "1849 0.0617 93.75\n",
            "1899 0.0000 100.0\n",
            "1949 0.0022 100.0\n",
            "1999 0.0001 100.0\n",
            "2049 0.0001 100.0\n",
            "2099 0.0000 100.0\n",
            "2149 0.0000 100.0\n",
            "2199 0.0000 100.0\n",
            "2249 0.0001 100.0\n",
            "2299 0.0145 100.0\n",
            "2349 0.0000 100.0\n",
            "2399 0.0001 100.0\n",
            "2449 0.0011 100.0\n",
            "2499 0.0000 100.0\n",
            "2549 0.0088 100.0\n",
            "2599 0.1393 93.75\n",
            "2649 0.0042 100.0\n",
            "2699 0.0009 100.0\n",
            "2749 0.0000 100.0\n",
            "2799 0.0029 100.0\n",
            "2849 0.0045 100.0\n",
            "2899 0.0000 100.0\n",
            "2949 0.2567 93.75\n",
            "2999 0.0009 100.0\n",
            "3049 0.0005 100.0\n",
            "3099 0.0001 100.0\n",
            "3149 0.0000 100.0\n",
            "3199 0.0004 100.0\n",
            "3249 0.0021 100.0\n",
            "3299 0.0102 100.0\n",
            "3349 0.0000 100.0\n",
            "3399 0.0220 100.0\n",
            "3449 0.0050 100.0\n",
            "3499 0.0016 100.0\n",
            "3549 0.0030 100.0\n",
            "3599 0.0000 100.0\n",
            "3649 0.0042 100.0\n",
            "3699 0.0008 100.0\n",
            "3749 0.0003 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0145, Accuracy: 9566/10000 (95.66%)\n",
            "\n",
            "--- 17.874130249023438 seconds ---\n"
          ]
        }
      ],
      "source": [
        "#mnist epoch 10 rffg8vsa\n",
        "class ArgumentParser:\n",
        "    def __init__(self):\n",
        "        self.args = {\n",
        "            'lr': 0.01,            # Learning rate for optimizing class representative\n",
        "            'gamma': 0.3,          # Kernel parameter for computing covariance\n",
        "            'epoch': 10,            # Epochs of training\n",
        "            'gorder': 8,           # Order of the cyclic group required for G-VSA (8 ou 16)\n",
        "            'dim': 10000,          # Dimension of hypervectors c le grand D\n",
        "            'seed': 43,            # Random seed for reproducing results\n",
        "            'resume': True,       # Resume from existing encoded hypervectors\n",
        "            'data_dir': './encoded_data', # Directory to save encoded data (hypervectors)\n",
        "            'dataset': 'mnist',    # Dataset choice choices=['mnist', 'fmnist', 'cifar', 'isolet', 'ucihar']\n",
        "            'raw_data_dir': './dataset',   # Raw data directory\n",
        "            'model': 'rff-gvsa'    # Model type choices=['rff-hdc', 'linear-hdc', 'rff-gvsa']\n",
        "        }\n",
        "        self.seed = self.args['seed']\n",
        "        self.lr = self.args['lr']\n",
        "        self.gamma = self.args['gamma']\n",
        "        self.epoch = self.args['epoch']\n",
        "        self.gorder = self.args['gorder']\n",
        "        self.dim = self.args['dim']\n",
        "        self.resume = self.args['resume']\n",
        "        self.data_dir = self.args['data_dir']\n",
        "        self.dataset = self.args['dataset']\n",
        "        self.raw_data_dir = self.args['raw_data_dir']\n",
        "        self.model = self.args['model']\n",
        "\n",
        "\n",
        "    def get_args(self):\n",
        "        return self.args\n",
        "args = ArgumentParser()\n",
        "torch.manual_seed(args.seed)\n",
        "np.random.seed(args.seed)\n",
        "if 'hdc' in args.model:\n",
        "    args.gorder = 2\n",
        "    print(\"Use binary HDC with random fourier features, ignoring gorder, set to 2.\")\n",
        "args.data_dir = f'{args.data_dir}/{args.dataset}_{args.model}_order{args.gorder}_gamma{args.gamma}_dim{args.dim}'\n",
        "try:\n",
        "    os.makedirs(args.data_dir)\n",
        "except FileExistsError:\n",
        "    print('Encoded data folder already exists')\n",
        "if not args.resume:\n",
        "    print('Encode the dataset into hypervectors and save')\n",
        "    encode_and_save(args)\n",
        "    print('Finish encoding and saving')\n",
        "print(f'Optimizing class representatives for {args.epoch} epochs')\n",
        "train(args)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FMNIST"
      ],
      "metadata": {
        "id": "mFSppsT5xo0F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#fminst epoch 1 rffg8vsa\n",
        "class ArgumentParser:\n",
        "    def __init__(self):\n",
        "        self.args = {\n",
        "            'lr': 0.01,            # Learning rate for optimizing class representative\n",
        "            'gamma': 0.3,          # Kernel parameter for computing covariance\n",
        "            'epoch': 1,            # Epochs of training\n",
        "            'gorder': 8,           # Order of the cyclic group required for G-VSA (8 ou 16)\n",
        "            'dim': 10000,          # Dimension of hypervectors c le grand D\n",
        "            'seed': 43,            # Random seed for reproducing results\n",
        "            'resume': False,       # Resume from existing encoded hypervectors\n",
        "            'data_dir': './encoded_data', # Directory to save encoded data (hypervectors)\n",
        "            'dataset': 'fmnist',    # Dataset choice choices=['mnist', 'fmnist', 'cifar', 'isolet', 'ucihar']\n",
        "            'raw_data_dir': './dataset',   # Raw data directory\n",
        "            'model': 'rff-gvsa'    # Model type choices=['rff-hdc', 'linear-hdc', 'rff-gvsa']\n",
        "        }\n",
        "        self.seed = self.args['seed']\n",
        "        self.lr = self.args['lr']\n",
        "        self.gamma = self.args['gamma']\n",
        "        self.epoch = self.args['epoch']\n",
        "        self.gorder = self.args['gorder']\n",
        "        self.dim = self.args['dim']\n",
        "        self.resume = self.args['resume']\n",
        "        self.data_dir = self.args['data_dir']\n",
        "        self.dataset = self.args['dataset']\n",
        "        self.raw_data_dir = self.args['raw_data_dir']\n",
        "        self.model = self.args['model']\n",
        "\n",
        "\n",
        "    def get_args(self):\n",
        "        return self.args\n",
        "\n",
        "args = ArgumentParser()\n",
        "torch.manual_seed(args.seed)\n",
        "np.random.seed(args.seed)\n",
        "if 'hdc' in args.model:\n",
        "    args.gorder = 2\n",
        "    print(\"Use binary HDC with random fourier features, ignoring gorder, set to 2.\")\n",
        "args.data_dir = f'{args.data_dir}/{args.dataset}_{args.model}_order{args.gorder}_gamma{args.gamma}_dim{args.dim}'\n",
        "try:\n",
        "    os.makedirs(args.data_dir)\n",
        "except FileExistsError:\n",
        "    print('Encoded data folder already exists')\n",
        "if not args.resume:\n",
        "    print('Encode the dataset into hypervectors and save')\n",
        "    encode_and_save(args)\n",
        "    print('Finish encoding and saving')\n",
        "print(f'Optimizing class representatives for {args.epoch} epochs')\n",
        "train(args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Az-HrMJkxo0F",
        "outputId": "78a47bed-3af2-4827-c1f8-9c37309027f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encode the dataset into hypervectors and save\n",
            "# of channels of data 1\n",
            "# of training samples and test samples 60000 10000\n",
            "Encoding with random fourier features encoder.\n",
            "the threshold to discretize fourier features to group elements tensor([-1.1505, -0.6746, -0.3187,  0.0000,  0.3187,  0.6746,  1.1505])\n",
            "Encoded pixels to hypervectors with size:  torch.Size([256, 10000])\n",
            "Encoding training data...\n",
            "Start encoding data\n",
            "12800 images encoded. Total time elapse = 27.11051082611084\n",
            "25600 images encoded. Total time elapse = 54.34337568283081\n",
            "38400 images encoded. Total time elapse = 81.67120671272278\n",
            "51200 images encoded. Total time elapse = 108.89743971824646\n",
            "Finish encoding data\n",
            "Encoding test data...\n",
            "Start encoding data\n",
            "Finish encoding data\n",
            "Finish encoding and saving\n",
            "Optimizing class representatives for 1 epochs\n",
            "Loading encoded training data...\n",
            "Loading encoded test data...\n",
            "Size of encoded training data torch.Size([60000, 10000]) and test data torch.Size([10000, 10000])\n",
            "Epoch: 1\n",
            "49 1.1395 81.25\n",
            "99 1.3386 56.25\n",
            "149 1.1338 62.5\n",
            "199 0.2014 100.0\n",
            "249 0.2061 100.0\n",
            "299 0.8291 75.0\n",
            "349 0.9154 68.75\n",
            "399 0.5923 75.0\n",
            "449 0.4414 68.75\n",
            "499 0.5712 75.0\n",
            "549 0.3027 93.75\n",
            "599 0.2622 93.75\n",
            "649 0.4106 87.5\n",
            "699 0.2176 93.75\n",
            "749 0.0720 100.0\n",
            "799 0.2869 87.5\n",
            "849 0.2480 93.75\n",
            "899 0.2266 93.75\n",
            "949 0.4250 75.0\n",
            "999 0.4264 81.25\n",
            "1049 0.5425 75.0\n",
            "1099 0.3483 81.25\n",
            "1149 0.5728 81.25\n",
            "1199 0.8046 93.75\n",
            "1249 0.2324 93.75\n",
            "1299 0.2335 87.5\n",
            "1349 0.1439 93.75\n",
            "1399 0.5882 81.25\n",
            "1449 0.6807 68.75\n",
            "1499 0.3403 81.25\n",
            "1549 0.8167 56.25\n",
            "1599 0.1778 93.75\n",
            "1649 0.4695 87.5\n",
            "1699 0.2284 93.75\n",
            "1749 0.5252 81.25\n",
            "1799 0.2630 87.5\n",
            "1849 0.1624 93.75\n",
            "1899 0.3219 81.25\n",
            "1949 0.0959 100.0\n",
            "1999 0.5415 87.5\n",
            "2049 0.1660 100.0\n",
            "2099 0.7042 68.75\n",
            "2149 0.4312 81.25\n",
            "2199 0.5001 81.25\n",
            "2249 0.2593 87.5\n",
            "2299 0.3366 81.25\n",
            "2349 0.4408 75.0\n",
            "2399 0.4241 81.25\n",
            "2449 0.6410 87.5\n",
            "2499 0.2279 87.5\n",
            "2549 0.4244 87.5\n",
            "2599 0.5634 75.0\n",
            "2649 0.2173 93.75\n",
            "2699 0.9488 75.0\n",
            "2749 0.1886 93.75\n",
            "2799 0.0901 100.0\n",
            "2849 0.1795 100.0\n",
            "2899 0.4673 81.25\n",
            "2949 0.0729 100.0\n",
            "2999 0.5812 81.25\n",
            "3049 0.4161 87.5\n",
            "3099 0.3790 75.0\n",
            "3149 0.5703 87.5\n",
            "3199 0.1319 93.75\n",
            "3249 0.1777 93.75\n",
            "3299 0.4281 81.25\n",
            "3349 0.7200 75.0\n",
            "3399 0.3221 87.5\n",
            "3449 0.0411 100.0\n",
            "3499 0.2206 93.75\n",
            "3549 0.6939 75.0\n",
            "3599 0.1375 93.75\n",
            "3649 0.6377 75.0\n",
            "3699 0.5726 81.25\n",
            "3749 0.2952 93.75\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0264, Accuracy: 8498/10000 (84.98%)\n",
            "\n",
            "--- 17.726536750793457 seconds ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a18a84d-88f1-4d11-ad16-40158909b1dd",
        "id": "ujBoc5Ttxo0F"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoded data folder already exists\n",
            "Optimizing class representatives for 10 epochs\n",
            "Loading encoded training data...\n",
            "Loading encoded test data...\n",
            "Size of encoded training data torch.Size([60000, 10000]) and test data torch.Size([10000, 10000])\n",
            "Epoch: 1\n",
            "49 0.9050 87.5\n",
            "99 0.8003 68.75\n",
            "149 0.7982 68.75\n",
            "199 0.6604 81.25\n",
            "249 0.3492 93.75\n",
            "299 0.5100 87.5\n",
            "349 0.2224 93.75\n",
            "399 0.4002 87.5\n",
            "449 0.3760 93.75\n",
            "499 0.2147 93.75\n",
            "549 0.4227 87.5\n",
            "599 0.3542 81.25\n",
            "649 0.1335 100.0\n",
            "699 0.3603 87.5\n",
            "749 0.8115 75.0\n",
            "799 0.3730 87.5\n",
            "849 0.0820 100.0\n",
            "899 0.3445 87.5\n",
            "949 0.0761 93.75\n",
            "999 0.3287 87.5\n",
            "1049 0.9485 68.75\n",
            "1099 0.5422 81.25\n",
            "1149 0.2502 93.75\n",
            "1199 0.4544 81.25\n",
            "1249 0.1526 93.75\n",
            "1299 0.1383 100.0\n",
            "1349 0.1398 93.75\n",
            "1399 0.2118 93.75\n",
            "1449 0.2858 93.75\n",
            "1499 0.4725 75.0\n",
            "1549 0.6279 87.5\n",
            "1599 0.6112 87.5\n",
            "1649 0.2713 93.75\n",
            "1699 0.3447 81.25\n",
            "1749 0.2502 87.5\n",
            "1799 0.3153 93.75\n",
            "1849 0.3553 87.5\n",
            "1899 0.5517 81.25\n",
            "1949 0.5069 81.25\n",
            "1999 0.2369 93.75\n",
            "2049 0.3452 87.5\n",
            "2099 0.5783 68.75\n",
            "2149 0.2108 93.75\n",
            "2199 0.4360 93.75\n",
            "2249 0.4168 81.25\n",
            "2299 0.4777 81.25\n",
            "2349 0.6722 81.25\n",
            "2399 0.1760 100.0\n",
            "2449 0.5567 81.25\n",
            "2499 0.5006 81.25\n",
            "2549 0.5016 87.5\n",
            "2599 0.4384 81.25\n",
            "2649 0.1773 93.75\n",
            "2699 0.1278 100.0\n",
            "2749 0.1228 100.0\n",
            "2799 0.0839 100.0\n",
            "2849 0.3028 87.5\n",
            "2899 0.3910 81.25\n",
            "2949 0.5720 87.5\n",
            "2999 0.6822 68.75\n",
            "3049 0.9139 81.25\n",
            "3099 0.3568 81.25\n",
            "3149 0.3477 81.25\n",
            "3199 0.4410 68.75\n",
            "3249 0.0598 100.0\n",
            "3299 0.4754 87.5\n",
            "3349 0.4537 81.25\n",
            "3399 0.5501 81.25\n",
            "3449 0.5057 81.25\n",
            "3499 0.3954 87.5\n",
            "3549 0.5390 81.25\n",
            "3599 0.1269 100.0\n",
            "3649 0.6782 68.75\n",
            "3699 0.0685 100.0\n",
            "3749 0.1453 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0256, Accuracy: 8556/10000 (85.56%)\n",
            "\n",
            "--- 17.941567182540894 seconds ---\n",
            "Epoch: 2\n",
            "49 0.0476 100.0\n",
            "99 0.1232 100.0\n",
            "149 0.1187 93.75\n",
            "199 0.0835 100.0\n",
            "249 0.1457 93.75\n",
            "299 0.0109 100.0\n",
            "349 0.4891 87.5\n",
            "399 0.1630 93.75\n",
            "449 0.2468 87.5\n",
            "499 0.3503 87.5\n",
            "549 0.0572 100.0\n",
            "599 0.2541 93.75\n",
            "649 0.1765 93.75\n",
            "699 0.0804 100.0\n",
            "749 0.0258 100.0\n",
            "799 0.1658 93.75\n",
            "849 0.0304 100.0\n",
            "899 0.2490 93.75\n",
            "949 0.1899 87.5\n",
            "999 0.5987 87.5\n",
            "1049 0.1082 93.75\n",
            "1099 0.2282 93.75\n",
            "1149 0.2309 87.5\n",
            "1199 0.2971 87.5\n",
            "1249 0.1395 93.75\n",
            "1299 0.1710 93.75\n",
            "1349 0.1055 93.75\n",
            "1399 0.0348 100.0\n",
            "1449 0.0823 93.75\n",
            "1499 0.0732 93.75\n",
            "1549 0.1795 93.75\n",
            "1599 0.3649 87.5\n",
            "1649 0.0858 93.75\n",
            "1699 0.0555 100.0\n",
            "1749 0.1648 93.75\n",
            "1799 0.1647 93.75\n",
            "1849 0.2065 87.5\n",
            "1899 0.3046 87.5\n",
            "1949 0.2746 93.75\n",
            "1999 0.2486 93.75\n",
            "2049 0.2447 93.75\n",
            "2099 0.3839 93.75\n",
            "2149 0.1638 93.75\n",
            "2199 0.1469 93.75\n",
            "2249 0.3599 87.5\n",
            "2299 0.0357 100.0\n",
            "2349 0.2482 87.5\n",
            "2399 0.0492 100.0\n",
            "2449 0.0059 100.0\n",
            "2499 0.3498 93.75\n",
            "2549 0.0719 100.0\n",
            "2599 0.1307 93.75\n",
            "2649 0.0753 100.0\n",
            "2699 0.1013 93.75\n",
            "2749 0.0352 100.0\n",
            "2799 0.0530 100.0\n",
            "2849 0.4480 87.5\n",
            "2899 0.2489 93.75\n",
            "2949 0.6700 81.25\n",
            "2999 0.3704 81.25\n",
            "3049 0.1299 100.0\n",
            "3099 0.0589 100.0\n",
            "3149 0.4919 81.25\n",
            "3199 0.1396 93.75\n",
            "3249 0.2002 93.75\n",
            "3299 0.0710 100.0\n",
            "3349 0.2292 87.5\n",
            "3399 0.1570 93.75\n",
            "3449 0.0556 100.0\n",
            "3499 0.1634 100.0\n",
            "3549 0.1497 93.75\n",
            "3599 0.1030 93.75\n",
            "3649 0.1589 87.5\n",
            "3699 0.2483 87.5\n",
            "3749 0.3885 75.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0298, Accuracy: 8414/10000 (84.14%)\n",
            "\n",
            "--- 16.843251943588257 seconds ---\n",
            "Epoch: 3\n",
            "49 0.1525 87.5\n",
            "99 0.0175 100.0\n",
            "149 0.0849 93.75\n",
            "199 0.0616 100.0\n",
            "249 0.0151 100.0\n",
            "299 0.0724 100.0\n",
            "349 0.0058 100.0\n",
            "399 0.0634 100.0\n",
            "449 0.0617 100.0\n",
            "499 0.0011 100.0\n",
            "549 0.0361 100.0\n",
            "599 0.0521 100.0\n",
            "649 0.0347 100.0\n",
            "699 0.0158 100.0\n",
            "749 0.1565 93.75\n",
            "799 0.0582 100.0\n",
            "849 0.0251 100.0\n",
            "899 0.0463 100.0\n",
            "949 0.0093 100.0\n",
            "999 0.1989 93.75\n",
            "1049 0.4513 87.5\n",
            "1099 0.0026 100.0\n",
            "1149 0.3201 87.5\n",
            "1199 0.4313 93.75\n",
            "1249 0.0049 100.0\n",
            "1299 0.0543 100.0\n",
            "1349 0.0029 100.0\n",
            "1399 0.0363 100.0\n",
            "1449 0.1406 93.75\n",
            "1499 0.4802 93.75\n",
            "1549 0.1838 93.75\n",
            "1599 0.1568 93.75\n",
            "1649 0.0522 100.0\n",
            "1699 0.4106 93.75\n",
            "1749 0.1652 93.75\n",
            "1799 0.0143 100.0\n",
            "1849 0.0121 100.0\n",
            "1899 0.0079 100.0\n",
            "1949 0.0240 100.0\n",
            "1999 0.0471 100.0\n",
            "2049 0.0514 100.0\n",
            "2099 0.0641 100.0\n",
            "2149 0.0160 100.0\n",
            "2199 0.0335 100.0\n",
            "2249 0.0718 93.75\n",
            "2299 0.0412 100.0\n",
            "2349 0.2114 87.5\n",
            "2399 0.2406 87.5\n",
            "2449 0.1482 93.75\n",
            "2499 0.4372 93.75\n",
            "2549 0.2981 93.75\n",
            "2599 0.0247 100.0\n",
            "2649 0.0199 100.0\n",
            "2699 0.1323 93.75\n",
            "2749 0.0711 100.0\n",
            "2799 0.2130 93.75\n",
            "2849 0.0038 100.0\n",
            "2899 0.0914 100.0\n",
            "2949 0.0273 100.0\n",
            "2999 0.2603 93.75\n",
            "3049 0.0163 100.0\n",
            "3099 0.1044 93.75\n",
            "3149 0.0454 100.0\n",
            "3199 0.0854 93.75\n",
            "3249 0.1152 93.75\n",
            "3299 0.0303 100.0\n",
            "3349 0.1650 93.75\n",
            "3399 0.1759 93.75\n",
            "3449 0.0277 100.0\n",
            "3499 0.0119 100.0\n",
            "3549 0.1253 93.75\n",
            "3599 0.2283 93.75\n",
            "3649 0.0541 93.75\n",
            "3699 0.0983 93.75\n",
            "3749 0.0336 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0375, Accuracy: 8356/10000 (83.56%)\n",
            "\n",
            "--- 16.909953117370605 seconds ---\n",
            "Epoch: 4\n",
            "49 0.0046 100.0\n",
            "99 0.0142 100.0\n",
            "149 0.0649 93.75\n",
            "199 0.0485 100.0\n",
            "249 0.0175 100.0\n",
            "299 0.0048 100.0\n",
            "349 0.0207 100.0\n",
            "399 0.0065 100.0\n",
            "449 0.0115 100.0\n",
            "499 0.0051 100.0\n",
            "549 0.1057 93.75\n",
            "599 0.0417 100.0\n",
            "649 0.0395 100.0\n",
            "699 0.0053 100.0\n",
            "749 0.0023 100.0\n",
            "799 0.0089 100.0\n",
            "849 0.0111 100.0\n",
            "899 0.0070 100.0\n",
            "949 0.0930 93.75\n",
            "999 0.1627 87.5\n",
            "1049 0.0021 100.0\n",
            "1099 0.1419 93.75\n",
            "1149 0.0471 100.0\n",
            "1199 0.0019 100.0\n",
            "1249 0.0047 100.0\n",
            "1299 0.0108 100.0\n",
            "1349 0.0038 100.0\n",
            "1399 0.1995 93.75\n",
            "1449 0.0424 100.0\n",
            "1499 0.0089 100.0\n",
            "1549 0.0142 100.0\n",
            "1599 0.5858 87.5\n",
            "1649 0.0040 100.0\n",
            "1699 0.1132 93.75\n",
            "1749 0.3647 93.75\n",
            "1799 0.0015 100.0\n",
            "1849 0.0594 100.0\n",
            "1899 0.0559 100.0\n",
            "1949 0.2088 87.5\n",
            "1999 0.0906 93.75\n",
            "2049 0.0114 100.0\n",
            "2099 0.1331 93.75\n",
            "2149 0.0714 93.75\n",
            "2199 0.1155 93.75\n",
            "2249 0.2713 93.75\n",
            "2299 0.0101 100.0\n",
            "2349 0.0411 100.0\n",
            "2399 0.0120 100.0\n",
            "2449 0.0013 100.0\n",
            "2499 0.0244 100.0\n",
            "2549 0.0469 100.0\n",
            "2599 0.0125 100.0\n",
            "2649 0.1303 93.75\n",
            "2699 0.1323 93.75\n",
            "2749 0.3453 93.75\n",
            "2799 0.0052 100.0\n",
            "2849 0.0129 100.0\n",
            "2899 0.0010 100.0\n",
            "2949 0.1287 87.5\n",
            "2999 0.0861 93.75\n",
            "3049 0.0201 100.0\n",
            "3099 0.0830 93.75\n",
            "3149 0.0288 100.0\n",
            "3199 0.0133 100.0\n",
            "3249 0.0017 100.0\n",
            "3299 0.0080 100.0\n",
            "3349 0.0044 100.0\n",
            "3399 0.0163 100.0\n",
            "3449 0.0109 100.0\n",
            "3499 0.3280 87.5\n",
            "3549 0.0283 100.0\n",
            "3599 0.0527 100.0\n",
            "3649 0.0066 100.0\n",
            "3699 0.0113 100.0\n",
            "3749 0.0546 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0456, Accuracy: 8373/10000 (83.73%)\n",
            "\n",
            "--- 17.593424558639526 seconds ---\n",
            "Epoch: 5\n",
            "49 0.0095 100.0\n",
            "99 0.0053 100.0\n",
            "149 0.1005 93.75\n",
            "199 0.0236 100.0\n",
            "249 0.2629 93.75\n",
            "299 0.1258 93.75\n",
            "349 0.0692 100.0\n",
            "399 0.0704 93.75\n",
            "449 0.1382 93.75\n",
            "499 0.0012 100.0\n",
            "549 0.0031 100.0\n",
            "599 0.0625 93.75\n",
            "649 0.0060 100.0\n",
            "699 0.1459 93.75\n",
            "749 0.0050 100.0\n",
            "799 0.0122 100.0\n",
            "849 0.0004 100.0\n",
            "899 0.0184 100.0\n",
            "949 0.0214 100.0\n",
            "999 0.0049 100.0\n",
            "1049 0.0189 100.0\n",
            "1099 0.0065 100.0\n",
            "1149 0.0044 100.0\n",
            "1199 0.1027 93.75\n",
            "1249 0.0064 100.0\n",
            "1299 0.0394 100.0\n",
            "1349 0.0222 100.0\n",
            "1399 0.0750 93.75\n",
            "1449 0.1300 93.75\n",
            "1499 0.0079 100.0\n",
            "1549 0.1165 93.75\n",
            "1599 0.0158 100.0\n",
            "1649 0.0293 100.0\n",
            "1699 0.0299 100.0\n",
            "1749 0.0007 100.0\n",
            "1799 0.1298 93.75\n",
            "1849 0.0182 100.0\n",
            "1899 0.1218 93.75\n",
            "1949 0.0168 100.0\n",
            "1999 0.1380 93.75\n",
            "2049 0.0707 93.75\n",
            "2099 0.0018 100.0\n",
            "2149 0.3939 87.5\n",
            "2199 0.0087 100.0\n",
            "2249 0.0026 100.0\n",
            "2299 0.1776 93.75\n",
            "2349 0.0003 100.0\n",
            "2399 0.0014 100.0\n",
            "2449 0.4866 87.5\n",
            "2499 0.0064 100.0\n",
            "2549 0.1985 93.75\n",
            "2599 0.0019 100.0\n",
            "2649 0.3825 93.75\n",
            "2699 0.0082 100.0\n",
            "2749 0.0230 100.0\n",
            "2799 0.0511 100.0\n",
            "2849 0.1820 93.75\n",
            "2899 0.1427 93.75\n",
            "2949 0.0082 100.0\n",
            "2999 0.4280 93.75\n",
            "3049 0.0055 100.0\n",
            "3099 0.0109 100.0\n",
            "3149 0.0196 100.0\n",
            "3199 0.1084 93.75\n",
            "3249 0.0064 100.0\n",
            "3299 0.2721 87.5\n",
            "3349 0.2578 93.75\n",
            "3399 0.0039 100.0\n",
            "3449 0.0889 93.75\n",
            "3499 0.0138 100.0\n",
            "3549 0.0829 93.75\n",
            "3599 0.0169 100.0\n",
            "3649 0.2467 87.5\n",
            "3699 0.0542 100.0\n",
            "3749 0.0219 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0506, Accuracy: 8303/10000 (83.03%)\n",
            "\n",
            "--- 16.91473150253296 seconds ---\n",
            "Epoch: 6\n",
            "49 0.0137 100.0\n",
            "99 0.0024 100.0\n",
            "149 0.1317 87.5\n",
            "199 0.0715 93.75\n",
            "249 0.0444 100.0\n",
            "299 0.0060 100.0\n",
            "349 0.0373 100.0\n",
            "399 0.0553 93.75\n",
            "449 0.0384 100.0\n",
            "499 0.0920 93.75\n",
            "549 0.0240 100.0\n",
            "599 0.1720 93.75\n",
            "649 0.1253 93.75\n",
            "699 0.0198 100.0\n",
            "749 0.0711 100.0\n",
            "799 0.0021 100.0\n",
            "849 0.0023 100.0\n",
            "899 0.0081 100.0\n",
            "949 0.0704 100.0\n",
            "999 0.0072 100.0\n",
            "1049 0.0156 100.0\n",
            "1099 0.0008 100.0\n",
            "1149 0.0012 100.0\n",
            "1199 0.0051 100.0\n",
            "1249 0.1075 93.75\n",
            "1299 0.0859 93.75\n",
            "1349 0.0199 100.0\n",
            "1399 0.0017 100.0\n",
            "1449 0.1368 93.75\n",
            "1499 0.0601 93.75\n",
            "1549 0.0090 100.0\n",
            "1599 0.0499 100.0\n",
            "1649 0.0019 100.0\n",
            "1699 0.0500 100.0\n",
            "1749 0.1560 93.75\n",
            "1799 0.0758 100.0\n",
            "1849 0.0012 100.0\n",
            "1899 0.0089 100.0\n",
            "1949 0.0733 93.75\n",
            "1999 0.0262 100.0\n",
            "2049 0.1900 93.75\n",
            "2099 0.0009 100.0\n",
            "2149 0.0002 100.0\n",
            "2199 0.2422 93.75\n",
            "2249 0.0359 100.0\n",
            "2299 0.0239 100.0\n",
            "2349 0.0028 100.0\n",
            "2399 0.0679 93.75\n",
            "2449 0.0053 100.0\n",
            "2499 0.0266 100.0\n",
            "2549 0.0876 93.75\n",
            "2599 0.0038 100.0\n",
            "2649 0.0226 100.0\n",
            "2699 0.0248 100.0\n",
            "2749 0.0188 100.0\n",
            "2799 0.1117 87.5\n",
            "2849 0.0131 100.0\n",
            "2899 0.1095 93.75\n",
            "2949 0.0063 100.0\n",
            "2999 0.0022 100.0\n",
            "3049 0.0264 100.0\n",
            "3099 0.2165 93.75\n",
            "3149 0.3221 81.25\n",
            "3199 0.0537 100.0\n",
            "3249 0.0550 100.0\n",
            "3299 0.1155 93.75\n",
            "3349 0.1139 93.75\n",
            "3399 0.0196 100.0\n",
            "3449 0.1684 93.75\n",
            "3499 0.0277 100.0\n",
            "3549 0.0099 100.0\n",
            "3599 0.0267 100.0\n",
            "3649 0.0035 100.0\n",
            "3699 0.0113 100.0\n",
            "3749 0.1308 93.75\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0544, Accuracy: 8355/10000 (83.55%)\n",
            "\n",
            "--- 16.932949542999268 seconds ---\n",
            "Epoch: 7\n",
            "49 0.0596 93.75\n",
            "99 0.0032 100.0\n",
            "149 0.0136 100.0\n",
            "199 0.0015 100.0\n",
            "249 0.0040 100.0\n",
            "299 0.0373 100.0\n",
            "349 0.0095 100.0\n",
            "399 0.0060 100.0\n",
            "449 0.0276 100.0\n",
            "499 0.0060 100.0\n",
            "549 0.0781 100.0\n",
            "599 0.0132 100.0\n",
            "649 0.0011 100.0\n",
            "699 0.0051 100.0\n",
            "749 0.0094 100.0\n",
            "799 0.0104 100.0\n",
            "849 0.0038 100.0\n",
            "899 0.0011 100.0\n",
            "949 0.0173 100.0\n",
            "999 0.0048 100.0\n",
            "1049 0.0000 100.0\n",
            "1099 0.0018 100.0\n",
            "1149 0.0466 93.75\n",
            "1199 0.0194 100.0\n",
            "1249 0.0170 100.0\n",
            "1299 0.0510 93.75\n",
            "1349 0.1111 93.75\n",
            "1399 0.0010 100.0\n",
            "1449 0.0013 100.0\n",
            "1499 0.0311 100.0\n",
            "1549 0.0374 100.0\n",
            "1599 0.0354 100.0\n",
            "1649 0.0022 100.0\n",
            "1699 0.0387 100.0\n",
            "1749 0.1070 93.75\n",
            "1799 0.0105 100.0\n",
            "1849 0.0102 100.0\n",
            "1899 0.1345 93.75\n",
            "1949 0.2610 87.5\n",
            "1999 0.0275 100.0\n",
            "2049 0.0198 100.0\n",
            "2099 0.0777 93.75\n",
            "2149 0.0008 100.0\n",
            "2199 0.0009 100.0\n",
            "2249 0.0057 100.0\n",
            "2299 0.0166 100.0\n",
            "2349 0.0057 100.0\n",
            "2399 0.0081 100.0\n",
            "2449 0.0097 100.0\n",
            "2499 0.0031 100.0\n",
            "2549 0.0055 100.0\n",
            "2599 0.0855 93.75\n",
            "2649 0.0679 100.0\n",
            "2699 0.0320 100.0\n",
            "2749 0.0074 100.0\n",
            "2799 0.0079 100.0\n",
            "2849 0.0075 100.0\n",
            "2899 0.1529 93.75\n",
            "2949 0.0021 100.0\n",
            "2999 0.0318 100.0\n",
            "3049 0.0139 100.0\n",
            "3099 0.1188 93.75\n",
            "3149 0.0082 100.0\n",
            "3199 0.0071 100.0\n",
            "3249 0.0529 100.0\n",
            "3299 0.0026 100.0\n",
            "3349 0.0270 100.0\n",
            "3399 0.0284 100.0\n",
            "3449 0.0094 100.0\n",
            "3499 0.2422 93.75\n",
            "3549 0.0305 100.0\n",
            "3599 0.0317 100.0\n",
            "3649 0.0066 100.0\n",
            "3699 0.1776 87.5\n",
            "3749 0.0112 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0547, Accuracy: 8313/10000 (83.13%)\n",
            "\n",
            "--- 17.537802696228027 seconds ---\n",
            "Epoch: 8\n",
            "49 0.0067 100.0\n",
            "99 0.0024 100.0\n",
            "149 0.0007 100.0\n",
            "199 0.0004 100.0\n",
            "249 0.0246 100.0\n",
            "299 0.0069 100.0\n",
            "349 0.0949 93.75\n",
            "399 0.0057 100.0\n",
            "449 0.0248 100.0\n",
            "499 0.0010 100.0\n",
            "549 0.0015 100.0\n",
            "599 0.0059 100.0\n",
            "649 0.0550 93.75\n",
            "699 0.1134 93.75\n",
            "749 0.0022 100.0\n",
            "799 0.0422 100.0\n",
            "849 0.0553 93.75\n",
            "899 0.0031 100.0\n",
            "949 0.0056 100.0\n",
            "999 0.0038 100.0\n",
            "1049 0.2263 93.75\n",
            "1099 0.0009 100.0\n",
            "1149 0.0089 100.0\n",
            "1199 0.0000 100.0\n",
            "1249 0.0114 100.0\n",
            "1299 0.1423 93.75\n",
            "1349 0.0039 100.0\n",
            "1399 0.0135 100.0\n",
            "1449 0.0197 100.0\n",
            "1499 0.0228 100.0\n",
            "1549 0.0084 100.0\n",
            "1599 0.0108 100.0\n",
            "1649 0.0044 100.0\n",
            "1699 0.1850 93.75\n",
            "1749 0.0202 100.0\n",
            "1799 0.3558 93.75\n",
            "1849 0.0041 100.0\n",
            "1899 0.0036 100.0\n",
            "1949 0.0012 100.0\n",
            "1999 0.0021 100.0\n",
            "2049 0.0288 100.0\n",
            "2099 0.0668 93.75\n",
            "2149 0.0002 100.0\n",
            "2199 0.0011 100.0\n",
            "2249 0.0000 100.0\n",
            "2299 0.0156 100.0\n",
            "2349 0.0965 87.5\n",
            "2399 0.0004 100.0\n",
            "2449 0.0746 93.75\n",
            "2499 0.0240 100.0\n",
            "2549 0.1020 93.75\n",
            "2599 0.0776 93.75\n",
            "2649 0.0580 100.0\n",
            "2699 0.0000 100.0\n",
            "2749 0.0404 100.0\n",
            "2799 0.0751 93.75\n",
            "2849 0.0598 93.75\n",
            "2899 0.0913 93.75\n",
            "2949 0.0576 93.75\n",
            "2999 0.0911 93.75\n",
            "3049 0.0060 100.0\n",
            "3099 0.0267 100.0\n",
            "3149 0.1170 93.75\n",
            "3199 0.0127 100.0\n",
            "3249 0.0003 100.0\n",
            "3299 0.0327 100.0\n",
            "3349 0.0260 100.0\n",
            "3399 0.1289 93.75\n",
            "3449 0.0108 100.0\n",
            "3499 0.0073 100.0\n",
            "3549 0.0452 100.0\n",
            "3599 0.0029 100.0\n",
            "3649 0.0143 100.0\n",
            "3699 0.0137 100.0\n",
            "3749 0.0532 93.75\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0587, Accuracy: 8289/10000 (82.89%)\n",
            "\n",
            "--- 16.846860885620117 seconds ---\n",
            "Epoch: 9\n",
            "49 0.0034 100.0\n",
            "99 0.0097 100.0\n",
            "149 0.0047 100.0\n",
            "199 0.0254 100.0\n",
            "249 0.0018 100.0\n",
            "299 0.0060 100.0\n",
            "349 0.0408 100.0\n",
            "399 0.0287 100.0\n",
            "449 0.0039 100.0\n",
            "499 0.0000 100.0\n",
            "549 0.0118 100.0\n",
            "599 0.0018 100.0\n",
            "649 0.0001 100.0\n",
            "699 0.0017 100.0\n",
            "749 0.0002 100.0\n",
            "799 0.0006 100.0\n",
            "849 0.0007 100.0\n",
            "899 0.0243 100.0\n",
            "949 0.0071 100.0\n",
            "999 0.0113 100.0\n",
            "1049 0.0008 100.0\n",
            "1099 0.0008 100.0\n",
            "1149 0.0878 93.75\n",
            "1199 0.0006 100.0\n",
            "1249 0.0549 93.75\n",
            "1299 0.0008 100.0\n",
            "1349 0.0002 100.0\n",
            "1399 0.0248 100.0\n",
            "1449 0.0010 100.0\n",
            "1499 0.0001 100.0\n",
            "1549 0.0058 100.0\n",
            "1599 0.5264 87.5\n",
            "1649 0.0241 100.0\n",
            "1699 0.1339 93.75\n",
            "1749 0.0040 100.0\n",
            "1799 0.0074 100.0\n",
            "1849 0.6241 87.5\n",
            "1899 0.0005 100.0\n",
            "1949 0.0178 100.0\n",
            "1999 0.0352 100.0\n",
            "2049 0.0292 100.0\n",
            "2099 0.0023 100.0\n",
            "2149 0.0520 93.75\n",
            "2199 0.3400 93.75\n",
            "2249 0.0830 100.0\n",
            "2299 0.0256 100.0\n",
            "2349 0.0167 100.0\n",
            "2399 0.0496 93.75\n",
            "2449 0.0118 100.0\n",
            "2499 0.0096 100.0\n",
            "2549 0.1833 93.75\n",
            "2599 0.0062 100.0\n",
            "2649 0.1783 93.75\n",
            "2699 0.0220 100.0\n",
            "2749 0.0006 100.0\n",
            "2799 0.0379 100.0\n",
            "2849 0.0068 100.0\n",
            "2899 0.1186 93.75\n",
            "2949 0.0211 100.0\n",
            "2999 0.0011 100.0\n",
            "3049 0.1530 93.75\n",
            "3099 0.0038 100.0\n",
            "3149 0.0016 100.0\n",
            "3199 0.2223 93.75\n",
            "3249 0.0480 100.0\n",
            "3299 0.0016 100.0\n",
            "3349 0.0187 100.0\n",
            "3399 0.0065 100.0\n",
            "3449 0.0159 100.0\n",
            "3499 0.4201 93.75\n",
            "3549 0.0005 100.0\n",
            "3599 0.0500 93.75\n",
            "3649 0.0248 100.0\n",
            "3699 0.0032 100.0\n",
            "3749 0.0074 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0615, Accuracy: 8307/10000 (83.07%)\n",
            "\n",
            "--- 16.974304914474487 seconds ---\n",
            "Epoch: 10\n",
            "49 0.0029 100.0\n",
            "99 0.0088 100.0\n",
            "149 0.0013 100.0\n",
            "199 0.0006 100.0\n",
            "249 0.0007 100.0\n",
            "299 0.0198 100.0\n",
            "349 0.0091 100.0\n",
            "399 0.0111 100.0\n",
            "449 0.0003 100.0\n",
            "499 0.0013 100.0\n",
            "549 0.0090 100.0\n",
            "599 0.0034 100.0\n",
            "649 0.0033 100.0\n",
            "699 0.0150 100.0\n",
            "749 0.0100 100.0\n",
            "799 0.0476 93.75\n",
            "849 0.0451 100.0\n",
            "899 0.0003 100.0\n",
            "949 0.0055 100.0\n",
            "999 0.0023 100.0\n",
            "1049 0.0150 100.0\n",
            "1099 0.0748 93.75\n",
            "1149 0.0004 100.0\n",
            "1199 0.0000 100.0\n",
            "1249 0.0317 100.0\n",
            "1299 0.0532 100.0\n",
            "1349 0.0190 100.0\n",
            "1399 0.0119 100.0\n",
            "1449 0.1356 87.5\n",
            "1499 0.0213 100.0\n",
            "1549 0.0273 100.0\n",
            "1599 0.8687 87.5\n",
            "1649 0.0000 100.0\n",
            "1699 0.0145 100.0\n",
            "1749 0.0011 100.0\n",
            "1799 0.0724 93.75\n",
            "1849 0.0239 100.0\n",
            "1899 0.0092 100.0\n",
            "1949 0.0091 100.0\n",
            "1999 0.0119 100.0\n",
            "2049 0.0112 100.0\n",
            "2099 0.0206 100.0\n",
            "2149 0.0033 100.0\n",
            "2199 0.0000 100.0\n",
            "2249 0.0191 100.0\n",
            "2299 0.0082 100.0\n",
            "2349 0.0610 93.75\n",
            "2399 0.1006 93.75\n",
            "2449 0.0004 100.0\n",
            "2499 0.0349 100.0\n",
            "2549 0.0079 100.0\n",
            "2599 0.0009 100.0\n",
            "2649 0.0193 100.0\n",
            "2699 0.0305 100.0\n",
            "2749 0.0049 100.0\n",
            "2799 0.0219 100.0\n",
            "2849 0.0036 100.0\n",
            "2899 0.0115 100.0\n",
            "2949 0.0060 100.0\n",
            "2999 0.0288 100.0\n",
            "3049 0.0011 100.0\n",
            "3099 0.0497 100.0\n",
            "3149 0.0531 100.0\n",
            "3199 0.0198 100.0\n",
            "3249 0.0035 100.0\n",
            "3299 0.1868 93.75\n",
            "3349 0.0082 100.0\n",
            "3399 0.0048 100.0\n",
            "3449 0.0859 93.75\n",
            "3499 0.2559 93.75\n",
            "3549 0.0028 100.0\n",
            "3599 0.0071 100.0\n",
            "3649 0.0131 100.0\n",
            "3699 0.0782 93.75\n",
            "3749 0.0109 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0606, Accuracy: 8346/10000 (83.46%)\n",
            "\n",
            "--- 17.487548351287842 seconds ---\n"
          ]
        }
      ],
      "source": [
        "#fminst epoch 10 rffg8vsa\n",
        "class ArgumentParser:\n",
        "    def __init__(self):\n",
        "        self.args = {\n",
        "            'lr': 0.01,            # Learning rate for optimizing class representative\n",
        "            'gamma': 0.3,          # Kernel parameter for computing covariance\n",
        "            'epoch': 10,            # Epochs of training\n",
        "            'gorder': 8,           # Order of the cyclic group required for G-VSA (8 ou 16)\n",
        "            'dim': 10000,          # Dimension of hypervectors c le grand D\n",
        "            'seed': 43,            # Random seed for reproducing results\n",
        "            'resume': True,       # Resume from existing encoded hypervectors\n",
        "            'data_dir': './encoded_data', # Directory to save encoded data (hypervectors)\n",
        "            'dataset': 'fmnist',    # Dataset choice choices=['mnist', 'fmnist', 'cifar', 'isolet', 'ucihar']\n",
        "            'raw_data_dir': './dataset',   # Raw data directory\n",
        "            'model': 'rff-gvsa'    # Model type choices=['rff-hdc', 'linear-hdc', 'rff-gvsa']\n",
        "        }\n",
        "        self.seed = self.args['seed']\n",
        "        self.lr = self.args['lr']\n",
        "        self.gamma = self.args['gamma']\n",
        "        self.epoch = self.args['epoch']\n",
        "        self.gorder = self.args['gorder']\n",
        "        self.dim = self.args['dim']\n",
        "        self.resume = self.args['resume']\n",
        "        self.data_dir = self.args['data_dir']\n",
        "        self.dataset = self.args['dataset']\n",
        "        self.raw_data_dir = self.args['raw_data_dir']\n",
        "        self.model = self.args['model']\n",
        "\n",
        "\n",
        "    def get_args(self):\n",
        "        return self.args\n",
        "args = ArgumentParser()\n",
        "torch.manual_seed(args.seed)\n",
        "np.random.seed(args.seed)\n",
        "if 'hdc' in args.model:\n",
        "    args.gorder = 2\n",
        "    print(\"Use binary HDC with random fourier features, ignoring gorder, set to 2.\")\n",
        "args.data_dir = f'{args.data_dir}/{args.dataset}_{args.model}_order{args.gorder}_gamma{args.gamma}_dim{args.dim}'\n",
        "try:\n",
        "    os.makedirs(args.data_dir)\n",
        "except FileExistsError:\n",
        "    print('Encoded data folder already exists')\n",
        "if not args.resume:\n",
        "    print('Encode the dataset into hypervectors and save')\n",
        "    encode_and_save(args)\n",
        "    print('Finish encoding and saving')\n",
        "print(f'Optimizing class representatives for {args.epoch} epochs')\n",
        "train(args)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RFF G($2^4$)-VSA"
      ],
      "metadata": {
        "id": "m3GT1yIZyWxJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ISOLET"
      ],
      "metadata": {
        "id": "GwQT2xfeyb1k"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb301a0a-0810-4180-c71d-e5b0fe0fe6e5",
        "id": "ktUot05cyb1l"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encode the dataset into hypervectors and save\n",
            "# of channels of data 1\n",
            "# of training samples and test samples 6238 1559\n",
            "Encoding with random fourier features encoder.\n",
            "the threshold to discretize fourier features to group elements tensor([-1.5343, -1.1505, -0.8872, -0.6746, -0.4888, -0.3187, -0.1573,  0.0000,\n",
            "         0.1573,  0.3187,  0.4888,  0.6746,  0.8872,  1.1505,  1.5343])\n",
            "Encoded pixels to hypervectors with size:  torch.Size([256, 10000])\n",
            "Encoding training data...\n",
            "Start encoding data\n",
            "Finish encoding data\n",
            "Encoding test data...\n",
            "Start encoding data\n",
            "Finish encoding data\n",
            "Finish encoding and saving\n",
            "Optimizing class representatives for 1 epochs\n",
            "Loading encoded training data...\n",
            "Loading encoded test data...\n",
            "Size of encoded training data torch.Size([6238, 10000]) and test data torch.Size([1559, 10000])\n",
            "Epoch: 1\n",
            "49 2.0995 68.75\n",
            "99 0.9491 81.25\n",
            "149 0.6568 81.25\n",
            "199 0.3804 87.5\n",
            "249 0.4022 81.25\n",
            "299 0.4028 87.5\n",
            "349 0.1018 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0169, Accuracy: 1468/1559 (94.16%)\n",
            "\n",
            "--- 2.3579518795013428 seconds ---\n"
          ]
        }
      ],
      "source": [
        "#isolet epoch 1 rffg16vsa\n",
        "class ArgumentParser:\n",
        "    def __init__(self):\n",
        "        self.args = {\n",
        "            'lr': 0.01,            # Learning rate for optimizing class representative\n",
        "            'gamma': 0.3,          # Kernel parameter for computing covariance\n",
        "            'epoch': 1,            # Epochs of training\n",
        "            'gorder': 16,           # Order of the cyclic group required for G-VSA (8 ou 16)\n",
        "            'dim': 10000,          # Dimension of hypervectors c le grand D\n",
        "            'seed': 43,            # Random seed for reproducing results\n",
        "            'resume': False,       # Resume from existing encoded hypervectors\n",
        "            'data_dir': './encoded_data', # Directory to save encoded data (hypervectors)\n",
        "            'dataset': 'isolet',    # Dataset choice choices=['mnist', 'fmnist', 'cifar', 'isolet', 'ucihar']\n",
        "            'raw_data_dir': './dataset',   # Raw data directory\n",
        "            'model': 'rff-gvsa'    # Model type choices=['rff-hdc', 'linear-hdc', 'rff-gvsa']\n",
        "        }\n",
        "        self.seed = self.args['seed']\n",
        "        self.lr = self.args['lr']\n",
        "        self.gamma = self.args['gamma']\n",
        "        self.epoch = self.args['epoch']\n",
        "        self.gorder = self.args['gorder']\n",
        "        self.dim = self.args['dim']\n",
        "        self.resume = self.args['resume']\n",
        "        self.data_dir = self.args['data_dir']\n",
        "        self.dataset = self.args['dataset']\n",
        "        self.raw_data_dir = self.args['raw_data_dir']\n",
        "        self.model = self.args['model']\n",
        "\n",
        "\n",
        "    def get_args(self):\n",
        "        return self.args\n",
        "args = ArgumentParser()\n",
        "torch.manual_seed(args.seed)\n",
        "np.random.seed(args.seed)\n",
        "if 'hdc' in args.model:\n",
        "    args.gorder = 2\n",
        "    print(\"Use binary HDC with random fourier features, ignoring gorder, set to 2.\")\n",
        "args.data_dir = f'{args.data_dir}/{args.dataset}_{args.model}_order{args.gorder}_gamma{args.gamma}_dim{args.dim}'\n",
        "try:\n",
        "    os.makedirs(args.data_dir)\n",
        "except FileExistsError:\n",
        "    print('Encoded data folder already exists')\n",
        "if not args.resume:\n",
        "    print('Encode the dataset into hypervectors and save')\n",
        "    encode_and_save(args)\n",
        "    print('Finish encoding and saving')\n",
        "print(f'Optimizing class representatives for {args.epoch} epochs')\n",
        "train(args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae60f129-a05d-4b4a-e653-30e1ca1cf1df",
        "id": "JwsmYtbryb1l"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoded data folder already exists\n",
            "Optimizing class representatives for 10 epochs\n",
            "Loading encoded training data...\n",
            "Loading encoded test data...\n",
            "Size of encoded training data torch.Size([6238, 10000]) and test data torch.Size([1559, 10000])\n",
            "Epoch: 1\n",
            "49 2.1540 50.0\n",
            "99 1.1984 75.0\n",
            "149 0.2346 93.75\n",
            "199 0.8011 75.0\n",
            "249 0.3979 81.25\n",
            "299 0.4434 87.5\n",
            "349 0.1179 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0172, Accuracy: 1468/1559 (94.16%)\n",
            "\n",
            "--- 2.463609218597412 seconds ---\n",
            "Epoch: 2\n",
            "49 0.0426 100.0\n",
            "99 0.0932 100.0\n",
            "149 0.0388 100.0\n",
            "199 0.0176 100.0\n",
            "249 0.0797 100.0\n",
            "299 0.0440 100.0\n",
            "349 0.0234 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0111, Accuracy: 1479/1559 (94.87%)\n",
            "\n",
            "--- 1.9738655090332031 seconds ---\n",
            "Epoch: 3\n",
            "49 0.0171 100.0\n",
            "99 0.0166 100.0\n",
            "149 0.0161 100.0\n",
            "199 0.0541 100.0\n",
            "249 0.0084 100.0\n",
            "299 0.0108 100.0\n",
            "349 0.0121 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0097, Accuracy: 1482/1559 (95.06%)\n",
            "\n",
            "--- 2.017972230911255 seconds ---\n",
            "Epoch: 4\n",
            "49 0.0033 100.0\n",
            "99 0.0076 100.0\n",
            "149 0.0089 100.0\n",
            "199 0.0054 100.0\n",
            "249 0.0042 100.0\n",
            "299 0.0046 100.0\n",
            "349 0.0036 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0091, Accuracy: 1486/1559 (95.32%)\n",
            "\n",
            "--- 1.9951601028442383 seconds ---\n",
            "Epoch: 5\n",
            "49 0.0105 100.0\n",
            "99 0.0035 100.0\n",
            "149 0.0022 100.0\n",
            "199 0.0042 100.0\n",
            "249 0.0028 100.0\n",
            "299 0.0044 100.0\n",
            "349 0.0059 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0088, Accuracy: 1492/1559 (95.70%)\n",
            "\n",
            "--- 1.977837085723877 seconds ---\n",
            "Epoch: 6\n",
            "49 0.0008 100.0\n",
            "99 0.0024 100.0\n",
            "149 0.0048 100.0\n",
            "199 0.0030 100.0\n",
            "249 0.0015 100.0\n",
            "299 0.0038 100.0\n",
            "349 0.0014 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0086, Accuracy: 1496/1559 (95.96%)\n",
            "\n",
            "--- 2.4751670360565186 seconds ---\n",
            "Epoch: 7\n",
            "49 0.0018 100.0\n",
            "99 0.0018 100.0\n",
            "149 0.0024 100.0\n",
            "199 0.0014 100.0\n",
            "249 0.0022 100.0\n",
            "299 0.0021 100.0\n",
            "349 0.0008 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0086, Accuracy: 1493/1559 (95.77%)\n",
            "\n",
            "--- 2.4641616344451904 seconds ---\n",
            "Epoch: 8\n",
            "49 0.0015 100.0\n",
            "99 0.0015 100.0\n",
            "149 0.0006 100.0\n",
            "199 0.0009 100.0\n",
            "249 0.0012 100.0\n",
            "299 0.0010 100.0\n",
            "349 0.0009 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0086, Accuracy: 1499/1559 (96.15%)\n",
            "\n",
            "--- 1.968679666519165 seconds ---\n",
            "Epoch: 9\n",
            "49 0.0010 100.0\n",
            "99 0.0007 100.0\n",
            "149 0.0007 100.0\n",
            "199 0.0004 100.0\n",
            "249 0.0011 100.0\n",
            "299 0.0009 100.0\n",
            "349 0.0006 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0086, Accuracy: 1499/1559 (96.15%)\n",
            "\n",
            "--- 1.9669408798217773 seconds ---\n",
            "Epoch: 10\n",
            "49 0.0009 100.0\n",
            "99 0.0007 100.0\n",
            "149 0.0006 100.0\n",
            "199 0.0003 100.0\n",
            "249 0.0008 100.0\n",
            "299 0.0008 100.0\n",
            "349 0.0009 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0085, Accuracy: 1500/1559 (96.22%)\n",
            "\n",
            "--- 1.987260103225708 seconds ---\n"
          ]
        }
      ],
      "source": [
        "#isolet epoch 10 rffg16vsa\n",
        "class ArgumentParser:\n",
        "    def __init__(self):\n",
        "        self.args = {\n",
        "            'lr': 0.01,            # Learning rate for optimizing class representative\n",
        "            'gamma': 0.3,          # Kernel parameter for computing covariance\n",
        "            'epoch': 10,            # Epochs of training\n",
        "            'gorder': 16,           # Order of the cyclic group required for G-VSA (8 ou 16)\n",
        "            'dim': 10000,          # Dimension of hypervectors c le grand D\n",
        "            'seed': 43,            # Random seed for reproducing results\n",
        "            'resume': True,       # Resume from existing encoded hypervectors\n",
        "            'data_dir': './encoded_data', # Directory to save encoded data (hypervectors)\n",
        "            'dataset': 'isolet',    # Dataset choice choices=['mnist', 'fmnist', 'cifar', 'isolet', 'ucihar']\n",
        "            'raw_data_dir': './dataset',   # Raw data directory\n",
        "            'model': 'rff-gvsa'    # Model type choices=['rff-hdc', 'linear-hdc', 'rff-gvsa']\n",
        "        }\n",
        "        self.seed = self.args['seed']\n",
        "        self.lr = self.args['lr']\n",
        "        self.gamma = self.args['gamma']\n",
        "        self.epoch = self.args['epoch']\n",
        "        self.gorder = self.args['gorder']\n",
        "        self.dim = self.args['dim']\n",
        "        self.resume = self.args['resume']\n",
        "        self.data_dir = self.args['data_dir']\n",
        "        self.dataset = self.args['dataset']\n",
        "        self.raw_data_dir = self.args['raw_data_dir']\n",
        "        self.model = self.args['model']\n",
        "\n",
        "\n",
        "    def get_args(self):\n",
        "        return self.args\n",
        "args = ArgumentParser()\n",
        "torch.manual_seed(args.seed)\n",
        "np.random.seed(args.seed)\n",
        "if 'hdc' in args.model:\n",
        "    args.gorder = 2\n",
        "    print(\"Use binary HDC with random fourier features, ignoring gorder, set to 2.\")\n",
        "args.data_dir = f'{args.data_dir}/{args.dataset}_{args.model}_order{args.gorder}_gamma{args.gamma}_dim{args.dim}'\n",
        "try:\n",
        "    os.makedirs(args.data_dir)\n",
        "except FileExistsError:\n",
        "    print('Encoded data folder already exists')\n",
        "if not args.resume:\n",
        "    print('Encode the dataset into hypervectors and save')\n",
        "    encode_and_save(args)\n",
        "    print('Finish encoding and saving')\n",
        "print(f'Optimizing class representatives for {args.epoch} epochs')\n",
        "train(args)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## UCIHAR"
      ],
      "metadata": {
        "id": "4kaMC9GNyb1l"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c83d76ef-2139-46ec-c79e-b7299293a9e6",
        "id": "lGFUq6V6yb1l"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encode the dataset into hypervectors and save\n",
            "# of channels of data 1\n",
            "# of training samples and test samples 7352 2947\n",
            "Encoding with random fourier features encoder.\n",
            "the threshold to discretize fourier features to group elements tensor([-1.5343, -1.1505, -0.8872, -0.6746, -0.4888, -0.3187, -0.1573,  0.0000,\n",
            "         0.1573,  0.3187,  0.4888,  0.6746,  0.8872,  1.1505,  1.5343])\n",
            "Encoded pixels to hypervectors with size:  torch.Size([256, 10000])\n",
            "Encoding training data...\n",
            "Start encoding data\n",
            "Finish encoding data\n",
            "Encoding test data...\n",
            "Start encoding data\n",
            "Finish encoding data\n",
            "Finish encoding and saving\n",
            "Optimizing class representatives for 1 epochs\n",
            "Loading encoded training data...\n",
            "Loading encoded test data...\n",
            "Size of encoded training data torch.Size([7352, 10000]) and test data torch.Size([2947, 10000])\n",
            "Epoch: 1\n",
            "49 0.8452 62.5\n",
            "99 0.2788 87.5\n",
            "149 0.1851 100.0\n",
            "199 0.1250 100.0\n",
            "249 0.1513 93.75\n",
            "299 0.0681 100.0\n",
            "349 0.1164 93.75\n",
            "399 0.1283 100.0\n",
            "449 0.1121 93.75\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0104, Accuracy: 2800/2947 (95.01%)\n",
            "\n",
            "--- 2.444155693054199 seconds ---\n"
          ]
        }
      ],
      "source": [
        "#ucihar epoch 1 rffg16vsa\n",
        "class ArgumentParser:\n",
        "    def __init__(self):\n",
        "        self.args = {\n",
        "            'lr': 0.01,            # Learning rate for optimizing class representative\n",
        "            'gamma': 0.3,          # Kernel parameter for computing covariance\n",
        "            'epoch': 1,            # Epochs of training\n",
        "            'gorder': 16,           # Order of the cyclic group required for G-VSA (8 ou 16)\n",
        "            'dim': 10000,          # Dimension of hypervectors c le grand D\n",
        "            'seed': 43,            # Random seed for reproducing results\n",
        "            'resume': False,       # Resume from existing encoded hypervectors\n",
        "            'data_dir': './encoded_data', # Directory to save encoded data (hypervectors)\n",
        "            'dataset': 'ucihar',    # Dataset choice choices=['mnist', 'fmnist', 'cifar', 'isolet', 'ucihar']\n",
        "            'raw_data_dir': './dataset',   # Raw data directory\n",
        "            'model': 'rff-gvsa'    # Model type choices=['rff-hdc', 'linear-hdc', 'rff-gvsa']\n",
        "        }\n",
        "        self.seed = self.args['seed']\n",
        "        self.lr = self.args['lr']\n",
        "        self.gamma = self.args['gamma']\n",
        "        self.epoch = self.args['epoch']\n",
        "        self.gorder = self.args['gorder']\n",
        "        self.dim = self.args['dim']\n",
        "        self.resume = self.args['resume']\n",
        "        self.data_dir = self.args['data_dir']\n",
        "        self.dataset = self.args['dataset']\n",
        "        self.raw_data_dir = self.args['raw_data_dir']\n",
        "        self.model = self.args['model']\n",
        "\n",
        "\n",
        "    def get_args(self):\n",
        "        return self.args\n",
        "args = ArgumentParser()\n",
        "torch.manual_seed(args.seed)\n",
        "np.random.seed(args.seed)\n",
        "if 'hdc' in args.model:\n",
        "    args.gorder = 2\n",
        "    print(\"Use binary HDC with random fourier features, ignoring gorder, set to 2.\")\n",
        "args.data_dir = f'{args.data_dir}/{args.dataset}_{args.model}_order{args.gorder}_gamma{args.gamma}_dim{args.dim}'\n",
        "try:\n",
        "    os.makedirs(args.data_dir)\n",
        "except FileExistsError:\n",
        "    print('Encoded data folder already exists')\n",
        "if not args.resume:\n",
        "    print('Encode the dataset into hypervectors and save')\n",
        "    encode_and_save(args)\n",
        "    print('Finish encoding and saving')\n",
        "print(f'Optimizing class representatives for {args.epoch} epochs')\n",
        "train(args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a0a823a-78f4-49df-843b-1555ec0114b8",
        "id": "t8xfOsE8yb1m"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoded data folder already exists\n",
            "Optimizing class representatives for 10 epochs\n",
            "Loading encoded training data...\n",
            "Loading encoded test data...\n",
            "Size of encoded training data torch.Size([7352, 10000]) and test data torch.Size([2947, 10000])\n",
            "Epoch: 1\n",
            "49 0.5797 87.5\n",
            "99 0.4076 93.75\n",
            "149 0.2845 87.5\n",
            "199 0.2783 87.5\n",
            "249 0.1712 93.75\n",
            "299 0.1226 93.75\n",
            "349 0.1966 87.5\n",
            "399 0.0676 100.0\n",
            "449 0.0829 93.75\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0105, Accuracy: 2787/2947 (94.57%)\n",
            "\n",
            "--- 2.4394707679748535 seconds ---\n",
            "Epoch: 2\n",
            "49 0.0159 100.0\n",
            "99 0.1159 93.75\n",
            "149 0.0172 100.0\n",
            "199 0.1342 100.0\n",
            "249 0.0511 100.0\n",
            "299 0.0260 100.0\n",
            "349 0.0154 100.0\n",
            "399 0.0097 100.0\n",
            "449 0.0115 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0078, Accuracy: 2818/2947 (95.62%)\n",
            "\n",
            "--- 3.181349277496338 seconds ---\n",
            "Epoch: 3\n",
            "49 0.0121 100.0\n",
            "99 0.0102 100.0\n",
            "149 0.0032 100.0\n",
            "199 0.0256 100.0\n",
            "249 0.0035 100.0\n",
            "299 0.0095 100.0\n",
            "349 0.1119 93.75\n",
            "399 0.0098 100.0\n",
            "449 0.0257 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0115, Accuracy: 2730/2947 (92.64%)\n",
            "\n",
            "--- 2.4737963676452637 seconds ---\n",
            "Epoch: 4\n",
            "49 0.1822 81.25\n",
            "99 0.0198 100.0\n",
            "149 0.0058 100.0\n",
            "199 0.0062 100.0\n",
            "249 0.0029 100.0\n",
            "299 0.0633 100.0\n",
            "349 0.0052 100.0\n",
            "399 0.0021 100.0\n",
            "449 0.0159 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0062, Accuracy: 2834/2947 (96.17%)\n",
            "\n",
            "--- 2.4255528450012207 seconds ---\n",
            "Epoch: 5\n",
            "49 0.0030 100.0\n",
            "99 0.0057 100.0\n",
            "149 0.0003 100.0\n",
            "199 0.0012 100.0\n",
            "249 0.0258 100.0\n",
            "299 0.0662 93.75\n",
            "349 0.1243 93.75\n",
            "399 0.0186 100.0\n",
            "449 0.0451 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0073, Accuracy: 2815/2947 (95.52%)\n",
            "\n",
            "--- 2.4167592525482178 seconds ---\n",
            "Epoch: 6\n",
            "49 0.0067 100.0\n",
            "99 0.0004 100.0\n",
            "149 0.0005 100.0\n",
            "199 0.0020 100.0\n",
            "249 0.0060 100.0\n",
            "299 0.0136 100.0\n",
            "349 0.0009 100.0\n",
            "399 0.0054 100.0\n",
            "449 0.0093 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0087, Accuracy: 2794/2947 (94.81%)\n",
            "\n",
            "--- 2.412952423095703 seconds ---\n",
            "Epoch: 7\n",
            "49 0.0010 100.0\n",
            "99 0.0009 100.0\n",
            "149 0.0817 93.75\n",
            "199 0.0032 100.0\n",
            "249 0.0109 100.0\n",
            "299 0.0046 100.0\n",
            "349 0.0004 100.0\n",
            "399 0.0001 100.0\n",
            "449 0.0001 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0081, Accuracy: 2802/2947 (95.08%)\n",
            "\n",
            "--- 3.1172165870666504 seconds ---\n",
            "Epoch: 8\n",
            "49 0.0097 100.0\n",
            "99 0.0003 100.0\n",
            "149 0.0011 100.0\n",
            "199 0.0043 100.0\n",
            "249 0.0008 100.0\n",
            "299 0.0002 100.0\n",
            "349 0.0000 100.0\n",
            "399 0.0023 100.0\n",
            "449 0.0002 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0066, Accuracy: 2841/2947 (96.40%)\n",
            "\n",
            "--- 2.4097607135772705 seconds ---\n",
            "Epoch: 9\n",
            "49 0.0028 100.0\n",
            "99 0.0041 100.0\n",
            "149 0.0041 100.0\n",
            "199 0.0155 100.0\n",
            "249 0.0006 100.0\n",
            "299 0.0002 100.0\n",
            "349 0.0004 100.0\n",
            "399 0.0052 100.0\n",
            "449 0.0014 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0070, Accuracy: 2833/2947 (96.13%)\n",
            "\n",
            "--- 2.4241909980773926 seconds ---\n",
            "Epoch: 10\n",
            "49 0.0000 100.0\n",
            "99 0.0001 100.0\n",
            "149 0.0001 100.0\n",
            "199 0.0001 100.0\n",
            "249 0.0000 100.0\n",
            "299 0.0015 100.0\n",
            "349 0.0003 100.0\n",
            "399 0.0005 100.0\n",
            "449 0.0002 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0073, Accuracy: 2832/2947 (96.10%)\n",
            "\n",
            "--- 2.5264644622802734 seconds ---\n"
          ]
        }
      ],
      "source": [
        "#ucihar epoch 10 rffg16vsa\n",
        "class ArgumentParser:\n",
        "    def __init__(self):\n",
        "        self.args = {\n",
        "            'lr': 0.01,            # Learning rate for optimizing class representative\n",
        "            'gamma': 0.3,          # Kernel parameter for computing covariance\n",
        "            'epoch': 10,            # Epochs of training\n",
        "            'gorder': 16,           # Order of the cyclic group required for G-VSA (8 ou 16)\n",
        "            'dim': 10000,          # Dimension of hypervectors c le grand D\n",
        "            'seed': 43,            # Random seed for reproducing results\n",
        "            'resume': True,       # Resume from existing encoded hypervectors\n",
        "            'data_dir': './encoded_data', # Directory to save encoded data (hypervectors)\n",
        "            'dataset': 'ucihar',    # Dataset choice choices=['mnist', 'fmnist', 'cifar', 'isolet', 'ucihar']\n",
        "            'raw_data_dir': './dataset',   # Raw data directory\n",
        "            'model': 'rff-gvsa'    # Model type choices=['rff-hdc', 'linear-hdc', 'rff-gvsa']\n",
        "        }\n",
        "        self.seed = self.args['seed']\n",
        "        self.lr = self.args['lr']\n",
        "        self.gamma = self.args['gamma']\n",
        "        self.epoch = self.args['epoch']\n",
        "        self.gorder = self.args['gorder']\n",
        "        self.dim = self.args['dim']\n",
        "        self.resume = self.args['resume']\n",
        "        self.data_dir = self.args['data_dir']\n",
        "        self.dataset = self.args['dataset']\n",
        "        self.raw_data_dir = self.args['raw_data_dir']\n",
        "        self.model = self.args['model']\n",
        "\n",
        "\n",
        "    def get_args(self):\n",
        "        return self.args\n",
        "args = ArgumentParser()\n",
        "torch.manual_seed(args.seed)\n",
        "np.random.seed(args.seed)\n",
        "if 'hdc' in args.model:\n",
        "    args.gorder = 2\n",
        "    print(\"Use binary HDC with random fourier features, ignoring gorder, set to 2.\")\n",
        "args.data_dir = f'{args.data_dir}/{args.dataset}_{args.model}_order{args.gorder}_gamma{args.gamma}_dim{args.dim}'\n",
        "try:\n",
        "    os.makedirs(args.data_dir)\n",
        "except FileExistsError:\n",
        "    print('Encoded data folder already exists')\n",
        "if not args.resume:\n",
        "    print('Encode the dataset into hypervectors and save')\n",
        "    encode_and_save(args)\n",
        "    print('Finish encoding and saving')\n",
        "print(f'Optimizing class representatives for {args.epoch} epochs')\n",
        "train(args)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MNIST"
      ],
      "metadata": {
        "id": "OpJEHRZKyb1m"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0bbeb4dd-de05-4952-fab4-90b7b3073de9",
        "id": "FT2xPzjiyb1m"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encode the dataset into hypervectors and save\n",
            "# of channels of data 1\n",
            "# of training samples and test samples 60000 10000\n",
            "Encoding with random fourier features encoder.\n",
            "the threshold to discretize fourier features to group elements tensor([-1.5343, -1.1505, -0.8872, -0.6746, -0.4888, -0.3187, -0.1573,  0.0000,\n",
            "         0.1573,  0.3187,  0.4888,  0.6746,  0.8872,  1.1505,  1.5343])\n",
            "Encoded pixels to hypervectors with size:  torch.Size([256, 10000])\n",
            "Encoding training data...\n",
            "Start encoding data\n",
            "12800 images encoded. Total time elapse = 23.111278772354126\n",
            "25600 images encoded. Total time elapse = 46.25865411758423\n",
            "38400 images encoded. Total time elapse = 69.27807807922363\n",
            "51200 images encoded. Total time elapse = 92.28337907791138\n",
            "Finish encoding data\n",
            "Encoding test data...\n",
            "Start encoding data\n",
            "Finish encoding data\n",
            "Finish encoding and saving\n",
            "Optimizing class representatives for 1 epochs\n",
            "Loading encoded training data...\n",
            "Loading encoded test data...\n",
            "Size of encoded training data torch.Size([60000, 10000]) and test data torch.Size([10000, 10000])\n",
            "Epoch: 1\n",
            "49 0.9711 100.0\n",
            "99 0.6084 81.25\n",
            "149 0.2380 93.75\n",
            "199 0.3480 87.5\n",
            "249 0.1981 93.75\n",
            "299 0.2357 87.5\n",
            "349 0.7013 81.25\n",
            "399 0.4321 87.5\n",
            "449 0.2818 87.5\n",
            "499 0.3373 81.25\n",
            "549 0.1901 93.75\n",
            "599 0.3705 87.5\n",
            "649 0.6105 81.25\n",
            "699 0.1197 93.75\n",
            "749 0.1999 93.75\n",
            "799 0.1904 87.5\n",
            "849 0.2078 93.75\n",
            "899 0.0383 100.0\n",
            "949 0.3434 93.75\n",
            "999 0.0508 100.0\n",
            "1049 0.4512 93.75\n",
            "1099 0.0142 100.0\n",
            "1149 0.1370 100.0\n",
            "1199 0.4662 87.5\n",
            "1249 0.0576 100.0\n",
            "1299 0.1112 100.0\n",
            "1349 0.0544 100.0\n",
            "1399 0.0294 100.0\n",
            "1449 0.0817 100.0\n",
            "1499 0.0212 100.0\n",
            "1549 0.0264 100.0\n",
            "1599 0.3586 93.75\n",
            "1649 0.1465 87.5\n",
            "1699 0.5596 93.75\n",
            "1749 0.1112 100.0\n",
            "1799 0.0609 100.0\n",
            "1849 0.1252 100.0\n",
            "1899 0.2041 93.75\n",
            "1949 0.2134 87.5\n",
            "1999 0.2694 87.5\n",
            "2049 0.1473 93.75\n",
            "2099 0.0988 100.0\n",
            "2149 0.0129 100.0\n",
            "2199 0.0232 100.0\n",
            "2249 0.0494 100.0\n",
            "2299 0.0812 100.0\n",
            "2349 0.0055 100.0\n",
            "2399 0.2186 93.75\n",
            "2449 0.0356 100.0\n",
            "2499 0.0115 100.0\n",
            "2549 0.1034 100.0\n",
            "2599 0.0046 100.0\n",
            "2649 0.3229 93.75\n",
            "2699 0.0200 100.0\n",
            "2749 0.0893 93.75\n",
            "2799 0.0204 100.0\n",
            "2849 0.5040 93.75\n",
            "2899 0.1895 87.5\n",
            "2949 0.0188 100.0\n",
            "2999 0.2121 93.75\n",
            "3049 0.0818 100.0\n",
            "3099 0.0645 93.75\n",
            "3149 0.0028 100.0\n",
            "3199 0.1967 93.75\n",
            "3249 0.0973 93.75\n",
            "3299 0.0996 93.75\n",
            "3349 0.3297 93.75\n",
            "3399 0.0416 100.0\n",
            "3449 0.2856 93.75\n",
            "3499 0.0886 93.75\n",
            "3549 0.0208 100.0\n",
            "3599 0.0087 100.0\n",
            "3649 0.0723 100.0\n",
            "3699 0.0197 100.0\n",
            "3749 0.0037 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0069, Accuracy: 9652/10000 (96.52%)\n",
            "\n",
            "--- 16.81315803527832 seconds ---\n"
          ]
        }
      ],
      "source": [
        "#mnist epoch 1 rffg16vsa\n",
        "class ArgumentParser:\n",
        "    def __init__(self):\n",
        "        self.args = {\n",
        "            'lr': 0.01,            # Learning rate for optimizing class representative\n",
        "            'gamma': 0.3,          # Kernel parameter for computing covariance\n",
        "            'epoch': 1,            # Epochs of training\n",
        "            'gorder': 16,           # Order of the cyclic group required for G-VSA (8 ou 16)\n",
        "            'dim': 10000,          # Dimension of hypervectors c le grand D\n",
        "            'seed': 43,            # Random seed for reproducing results\n",
        "            'resume': False,       # Resume from existing encoded hypervectors\n",
        "            'data_dir': './encoded_data', # Directory to save encoded data (hypervectors)\n",
        "            'dataset': 'mnist',    # Dataset choice choices=['mnist', 'fmnist', 'cifar', 'isolet', 'ucihar']\n",
        "            'raw_data_dir': './dataset',   # Raw data directory\n",
        "            'model': 'rff-gvsa'    # Model type choices=['rff-hdc', 'linear-hdc', 'rff-gvsa']\n",
        "        }\n",
        "        self.seed = self.args['seed']\n",
        "        self.lr = self.args['lr']\n",
        "        self.gamma = self.args['gamma']\n",
        "        self.epoch = self.args['epoch']\n",
        "        self.gorder = self.args['gorder']\n",
        "        self.dim = self.args['dim']\n",
        "        self.resume = self.args['resume']\n",
        "        self.data_dir = self.args['data_dir']\n",
        "        self.dataset = self.args['dataset']\n",
        "        self.raw_data_dir = self.args['raw_data_dir']\n",
        "        self.model = self.args['model']\n",
        "\n",
        "\n",
        "    def get_args(self):\n",
        "        return self.args\n",
        "args = ArgumentParser()\n",
        "torch.manual_seed(args.seed)\n",
        "np.random.seed(args.seed)\n",
        "if 'hdc' in args.model:\n",
        "    args.gorder = 2\n",
        "    print(\"Use binary HDC with random fourier features, ignoring gorder, set to 2.\")\n",
        "args.data_dir = f'{args.data_dir}/{args.dataset}_{args.model}_order{args.gorder}_gamma{args.gamma}_dim{args.dim}'\n",
        "try:\n",
        "    os.makedirs(args.data_dir)\n",
        "except FileExistsError:\n",
        "    print('Encoded data folder already exists')\n",
        "if not args.resume:\n",
        "    print('Encode the dataset into hypervectors and save')\n",
        "    encode_and_save(args)\n",
        "    print('Finish encoding and saving')\n",
        "print(f'Optimizing class representatives for {args.epoch} epochs')\n",
        "train(args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01e5479f-5fdf-4c18-bae8-bf17d5c2c11b",
        "id": "CAsRn2ukyb1m"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoded data folder already exists\n",
            "Optimizing class representatives for 10 epochs\n",
            "Loading encoded training data...\n",
            "Loading encoded test data...\n",
            "Size of encoded training data torch.Size([60000, 10000]) and test data torch.Size([10000, 10000])\n",
            "Epoch: 1\n",
            "49 1.5492 68.75\n",
            "99 0.5797 87.5\n",
            "149 0.2853 93.75\n",
            "199 0.1788 100.0\n",
            "249 0.1036 100.0\n",
            "299 0.3539 81.25\n",
            "349 0.3697 75.0\n",
            "399 0.2319 93.75\n",
            "449 0.2126 87.5\n",
            "499 0.3083 81.25\n",
            "549 0.1738 87.5\n",
            "599 0.4366 81.25\n",
            "649 0.0674 100.0\n",
            "699 0.0882 93.75\n",
            "749 0.0677 100.0\n",
            "799 0.1423 100.0\n",
            "849 0.4121 87.5\n",
            "899 0.1597 93.75\n",
            "949 0.0490 100.0\n",
            "999 0.1582 93.75\n",
            "1049 0.0594 100.0\n",
            "1099 0.1573 93.75\n",
            "1149 0.0354 100.0\n",
            "1199 0.0602 100.0\n",
            "1249 0.0620 100.0\n",
            "1299 0.0447 100.0\n",
            "1349 0.0101 100.0\n",
            "1399 0.1730 93.75\n",
            "1449 0.0313 100.0\n",
            "1499 0.1358 93.75\n",
            "1549 0.0145 100.0\n",
            "1599 0.0576 100.0\n",
            "1649 0.0722 93.75\n",
            "1699 0.0700 93.75\n",
            "1749 0.0435 100.0\n",
            "1799 0.1330 93.75\n",
            "1849 0.1034 100.0\n",
            "1899 0.1491 93.75\n",
            "1949 0.1142 93.75\n",
            "1999 0.0782 100.0\n",
            "2049 0.0054 100.0\n",
            "2099 0.2290 93.75\n",
            "2149 0.1540 93.75\n",
            "2199 0.0109 100.0\n",
            "2249 0.6037 87.5\n",
            "2299 0.0392 100.0\n",
            "2349 0.0082 100.0\n",
            "2399 0.0575 100.0\n",
            "2449 0.0335 100.0\n",
            "2499 0.1125 93.75\n",
            "2549 0.0440 100.0\n",
            "2599 0.0344 100.0\n",
            "2649 0.0357 100.0\n",
            "2699 0.0855 100.0\n",
            "2749 0.0218 100.0\n",
            "2799 0.0046 100.0\n",
            "2849 0.1194 93.75\n",
            "2899 0.0745 93.75\n",
            "2949 0.2482 93.75\n",
            "2999 0.1484 93.75\n",
            "3049 0.0055 100.0\n",
            "3099 0.1995 93.75\n",
            "3149 0.0830 100.0\n",
            "3199 0.2365 87.5\n",
            "3249 0.1285 93.75\n",
            "3299 0.0599 100.0\n",
            "3349 0.1840 93.75\n",
            "3399 0.1074 93.75\n",
            "3449 0.0353 100.0\n",
            "3499 0.0465 100.0\n",
            "3549 0.0254 100.0\n",
            "3599 0.0051 100.0\n",
            "3649 0.2862 87.5\n",
            "3699 0.0794 100.0\n",
            "3749 0.0891 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0073, Accuracy: 9659/10000 (96.59%)\n",
            "\n",
            "--- 16.823177337646484 seconds ---\n",
            "Epoch: 2\n",
            "49 0.3515 87.5\n",
            "99 0.0131 100.0\n",
            "149 0.0108 100.0\n",
            "199 0.0517 100.0\n",
            "249 0.0639 93.75\n",
            "299 0.0212 100.0\n",
            "349 0.0275 100.0\n",
            "399 0.0149 100.0\n",
            "449 0.0398 100.0\n",
            "499 0.0199 100.0\n",
            "549 0.0061 100.0\n",
            "599 0.3077 93.75\n",
            "649 0.0078 100.0\n",
            "699 0.0176 100.0\n",
            "749 0.0661 100.0\n",
            "799 0.0103 100.0\n",
            "849 0.1261 93.75\n",
            "899 0.0989 93.75\n",
            "949 0.0011 100.0\n",
            "999 0.0162 100.0\n",
            "1049 0.0010 100.0\n",
            "1099 0.0428 100.0\n",
            "1149 0.1872 93.75\n",
            "1199 0.0193 100.0\n",
            "1249 0.0559 100.0\n",
            "1299 0.0174 100.0\n",
            "1349 0.0047 100.0\n",
            "1399 0.0098 100.0\n",
            "1449 0.5696 93.75\n",
            "1499 0.0003 100.0\n",
            "1549 0.0196 100.0\n",
            "1599 0.0455 100.0\n",
            "1649 0.0034 100.0\n",
            "1699 0.0950 93.75\n",
            "1749 0.0011 100.0\n",
            "1799 0.0934 93.75\n",
            "1849 0.0963 93.75\n",
            "1899 0.0089 100.0\n",
            "1949 0.1256 93.75\n",
            "1999 0.0117 100.0\n",
            "2049 0.0120 100.0\n",
            "2099 0.0149 100.0\n",
            "2149 0.0235 100.0\n",
            "2199 0.0040 100.0\n",
            "2249 0.2065 93.75\n",
            "2299 0.0468 100.0\n",
            "2349 0.0248 100.0\n",
            "2399 0.0082 100.0\n",
            "2449 0.0056 100.0\n",
            "2499 0.0117 100.0\n",
            "2549 0.0671 93.75\n",
            "2599 0.0077 100.0\n",
            "2649 0.0010 100.0\n",
            "2699 0.0107 100.0\n",
            "2749 0.0462 93.75\n",
            "2799 0.0116 100.0\n",
            "2849 0.0279 100.0\n",
            "2899 0.0168 100.0\n",
            "2949 0.0052 100.0\n",
            "2999 0.0267 100.0\n",
            "3049 0.1700 93.75\n",
            "3099 0.1020 93.75\n",
            "3149 0.0076 100.0\n",
            "3199 0.0905 93.75\n",
            "3249 0.0629 93.75\n",
            "3299 0.0290 100.0\n",
            "3349 0.0017 100.0\n",
            "3399 0.0230 100.0\n",
            "3449 0.0150 100.0\n",
            "3499 0.0186 100.0\n",
            "3549 0.0162 100.0\n",
            "3599 0.0004 100.0\n",
            "3649 0.0005 100.0\n",
            "3699 0.0004 100.0\n",
            "3749 0.0041 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0077, Accuracy: 9651/10000 (96.51%)\n",
            "\n",
            "--- 17.059151887893677 seconds ---\n",
            "Epoch: 3\n",
            "49 0.1391 93.75\n",
            "99 0.0095 100.0\n",
            "149 0.0102 100.0\n",
            "199 0.0015 100.0\n",
            "249 0.0239 100.0\n",
            "299 0.0013 100.0\n",
            "349 0.1493 93.75\n",
            "399 0.0081 100.0\n",
            "449 0.0009 100.0\n",
            "499 0.0039 100.0\n",
            "549 0.0275 100.0\n",
            "599 0.0053 100.0\n",
            "649 0.0036 100.0\n",
            "699 0.0023 100.0\n",
            "749 0.0026 100.0\n",
            "799 0.0306 100.0\n",
            "849 0.1409 93.75\n",
            "899 0.0000 100.0\n",
            "949 0.0003 100.0\n",
            "999 0.0943 93.75\n",
            "1049 0.0011 100.0\n",
            "1099 0.0030 100.0\n",
            "1149 0.0002 100.0\n",
            "1199 0.0045 100.0\n",
            "1249 0.1608 93.75\n",
            "1299 0.0025 100.0\n",
            "1349 0.2444 87.5\n",
            "1399 0.0458 100.0\n",
            "1449 0.1098 93.75\n",
            "1499 0.0024 100.0\n",
            "1549 0.0042 100.0\n",
            "1599 0.0040 100.0\n",
            "1649 0.0078 100.0\n",
            "1699 0.0023 100.0\n",
            "1749 0.0051 100.0\n",
            "1799 0.3382 93.75\n",
            "1849 0.0089 100.0\n",
            "1899 0.0926 93.75\n",
            "1949 0.0002 100.0\n",
            "1999 0.0019 100.0\n",
            "2049 0.0002 100.0\n",
            "2099 0.0089 100.0\n",
            "2149 0.1743 93.75\n",
            "2199 0.0022 100.0\n",
            "2249 0.0001 100.0\n",
            "2299 0.0003 100.0\n",
            "2349 0.0393 100.0\n",
            "2399 0.0243 100.0\n",
            "2449 0.0016 100.0\n",
            "2499 0.0349 100.0\n",
            "2549 0.0006 100.0\n",
            "2599 0.0180 100.0\n",
            "2649 0.0020 100.0\n",
            "2699 0.1644 93.75\n",
            "2749 0.0029 100.0\n",
            "2799 0.0014 100.0\n",
            "2849 0.0541 100.0\n",
            "2899 0.0009 100.0\n",
            "2949 0.0012 100.0\n",
            "2999 0.0165 100.0\n",
            "3049 0.0268 100.0\n",
            "3099 0.0671 93.75\n",
            "3149 0.0013 100.0\n",
            "3199 0.0175 100.0\n",
            "3249 0.0005 100.0\n",
            "3299 0.0012 100.0\n",
            "3349 0.0001 100.0\n",
            "3399 0.0191 100.0\n",
            "3449 0.0708 93.75\n",
            "3499 0.0044 100.0\n",
            "3549 0.0292 100.0\n",
            "3599 0.0001 100.0\n",
            "3649 0.0430 100.0\n",
            "3699 0.0016 100.0\n",
            "3749 0.0028 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0083, Accuracy: 9681/10000 (96.81%)\n",
            "\n",
            "--- 17.36479377746582 seconds ---\n",
            "Epoch: 4\n",
            "49 0.0100 100.0\n",
            "99 0.1816 93.75\n",
            "149 0.0004 100.0\n",
            "199 0.0011 100.0\n",
            "249 0.0006 100.0\n",
            "299 0.0001 100.0\n",
            "349 0.0012 100.0\n",
            "399 0.0001 100.0\n",
            "449 0.0201 100.0\n",
            "499 0.0000 100.0\n",
            "549 0.0067 100.0\n",
            "599 0.0708 100.0\n",
            "649 0.0227 100.0\n",
            "699 0.0790 93.75\n",
            "749 0.0008 100.0\n",
            "799 0.0001 100.0\n",
            "849 0.0073 100.0\n",
            "899 0.0000 100.0\n",
            "949 0.0075 100.0\n",
            "999 0.0003 100.0\n",
            "1049 0.0008 100.0\n",
            "1099 0.0020 100.0\n",
            "1149 0.0013 100.0\n",
            "1199 0.0005 100.0\n",
            "1249 0.0056 100.0\n",
            "1299 0.0006 100.0\n",
            "1349 0.0004 100.0\n",
            "1399 0.0000 100.0\n",
            "1449 0.0000 100.0\n",
            "1499 0.0005 100.0\n",
            "1549 0.0022 100.0\n",
            "1599 0.0001 100.0\n",
            "1649 0.0005 100.0\n",
            "1699 0.0000 100.0\n",
            "1749 0.0070 100.0\n",
            "1799 0.0027 100.0\n",
            "1849 0.0001 100.0\n",
            "1899 0.0003 100.0\n",
            "1949 0.0016 100.0\n",
            "1999 0.0870 100.0\n",
            "2049 0.0009 100.0\n",
            "2099 0.0002 100.0\n",
            "2149 0.0000 100.0\n",
            "2199 0.0001 100.0\n",
            "2249 0.0100 100.0\n",
            "2299 0.1167 93.75\n",
            "2349 0.0016 100.0\n",
            "2399 0.0001 100.0\n",
            "2449 0.0052 100.0\n",
            "2499 0.0203 100.0\n",
            "2549 0.0014 100.0\n",
            "2599 0.0050 100.0\n",
            "2649 0.0031 100.0\n",
            "2699 0.0336 100.0\n",
            "2749 0.0935 93.75\n",
            "2799 0.0268 100.0\n",
            "2849 0.0000 100.0\n",
            "2899 0.0001 100.0\n",
            "2949 0.1319 93.75\n",
            "2999 0.0000 100.0\n",
            "3049 0.0001 100.0\n",
            "3099 0.0002 100.0\n",
            "3149 0.0115 100.0\n",
            "3199 0.0000 100.0\n",
            "3249 0.0070 100.0\n",
            "3299 0.0670 93.75\n",
            "3349 0.0524 93.75\n",
            "3399 0.0000 100.0\n",
            "3449 0.0125 100.0\n",
            "3499 0.0012 100.0\n",
            "3549 0.0001 100.0\n",
            "3599 0.0102 100.0\n",
            "3649 0.0009 100.0\n",
            "3699 0.0054 100.0\n",
            "3749 0.0001 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0105, Accuracy: 9634/10000 (96.34%)\n",
            "\n",
            "--- 16.97447180747986 seconds ---\n",
            "Epoch: 5\n",
            "49 0.0017 100.0\n",
            "99 0.0230 100.0\n",
            "149 0.0769 93.75\n",
            "199 0.0001 100.0\n",
            "249 0.0049 100.0\n",
            "299 0.0071 100.0\n",
            "349 0.0001 100.0\n",
            "399 0.0097 100.0\n",
            "449 0.0001 100.0\n",
            "499 0.0000 100.0\n",
            "549 0.0005 100.0\n",
            "599 0.0006 100.0\n",
            "649 0.0008 100.0\n",
            "699 0.0002 100.0\n",
            "749 0.0131 100.0\n",
            "799 0.0002 100.0\n",
            "849 0.0001 100.0\n",
            "899 0.0053 100.0\n",
            "949 0.0000 100.0\n",
            "999 0.0057 100.0\n",
            "1049 0.0022 100.0\n",
            "1099 0.0000 100.0\n",
            "1149 0.0000 100.0\n",
            "1199 0.0108 100.0\n",
            "1249 0.0000 100.0\n",
            "1299 0.0564 93.75\n",
            "1349 0.0004 100.0\n",
            "1399 0.0003 100.0\n",
            "1449 0.0007 100.0\n",
            "1499 0.0037 100.0\n",
            "1549 0.0002 100.0\n",
            "1599 0.0014 100.0\n",
            "1649 0.0003 100.0\n",
            "1699 0.0000 100.0\n",
            "1749 0.0631 93.75\n",
            "1799 0.0012 100.0\n",
            "1849 0.0013 100.0\n",
            "1899 0.0032 100.0\n",
            "1949 0.0000 100.0\n",
            "1999 0.0032 100.0\n",
            "2049 0.0001 100.0\n",
            "2099 0.0079 100.0\n",
            "2149 0.0027 100.0\n",
            "2199 0.0007 100.0\n",
            "2249 0.0005 100.0\n",
            "2299 0.1197 93.75\n",
            "2349 0.0002 100.0\n",
            "2399 0.0000 100.0\n",
            "2449 0.0398 100.0\n",
            "2499 0.0001 100.0\n",
            "2549 0.0003 100.0\n",
            "2599 0.0211 100.0\n",
            "2649 0.0910 93.75\n",
            "2699 0.0003 100.0\n",
            "2749 0.0050 100.0\n",
            "2799 0.0002 100.0\n",
            "2849 0.1209 93.75\n",
            "2899 0.0059 100.0\n",
            "2949 0.0082 100.0\n",
            "2999 0.0000 100.0\n",
            "3049 0.0000 100.0\n",
            "3099 0.0001 100.0\n",
            "3149 0.0013 100.0\n",
            "3199 0.0102 100.0\n",
            "3249 0.0133 100.0\n",
            "3299 0.0016 100.0\n",
            "3349 0.0014 100.0\n",
            "3399 0.0405 100.0\n",
            "3449 0.0000 100.0\n",
            "3499 0.0011 100.0\n",
            "3549 0.0001 100.0\n",
            "3599 0.0006 100.0\n",
            "3649 0.0213 100.0\n",
            "3699 0.0030 100.0\n",
            "3749 0.0000 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0094, Accuracy: 9694/10000 (96.94%)\n",
            "\n",
            "--- 17.38780951499939 seconds ---\n",
            "Epoch: 6\n",
            "49 0.0023 100.0\n",
            "99 0.0008 100.0\n",
            "149 0.0001 100.0\n",
            "199 0.0000 100.0\n",
            "249 0.0003 100.0\n",
            "299 0.0000 100.0\n",
            "349 0.0101 100.0\n",
            "399 0.0001 100.0\n",
            "449 0.0000 100.0\n",
            "499 0.0025 100.0\n",
            "549 0.0002 100.0\n",
            "599 0.0004 100.0\n",
            "649 0.0000 100.0\n",
            "699 0.0000 100.0\n",
            "749 0.0004 100.0\n",
            "799 0.0007 100.0\n",
            "849 0.0021 100.0\n",
            "899 0.0041 100.0\n",
            "949 0.0008 100.0\n",
            "999 0.0011 100.0\n",
            "1049 0.0001 100.0\n",
            "1099 0.0000 100.0\n",
            "1149 0.0015 100.0\n",
            "1199 0.0000 100.0\n",
            "1249 0.0010 100.0\n",
            "1299 0.0015 100.0\n",
            "1349 0.0023 100.0\n",
            "1399 0.0000 100.0\n",
            "1449 0.0524 93.75\n",
            "1499 0.0038 100.0\n",
            "1549 0.0118 100.0\n",
            "1599 0.0009 100.0\n",
            "1649 0.0005 100.0\n",
            "1699 0.0002 100.0\n",
            "1749 0.0001 100.0\n",
            "1799 0.0000 100.0\n",
            "1849 0.0002 100.0\n",
            "1899 0.0190 100.0\n",
            "1949 0.0000 100.0\n",
            "1999 0.0001 100.0\n",
            "2049 0.0000 100.0\n",
            "2099 0.0000 100.0\n",
            "2149 0.0000 100.0\n",
            "2199 0.0000 100.0\n",
            "2249 0.0000 100.0\n",
            "2299 0.0003 100.0\n",
            "2349 0.0000 100.0\n",
            "2399 0.0832 93.75\n",
            "2449 0.0001 100.0\n",
            "2499 0.0001 100.0\n",
            "2549 0.0019 100.0\n",
            "2599 0.0000 100.0\n",
            "2649 0.0001 100.0\n",
            "2699 0.0005 100.0\n",
            "2749 0.0167 100.0\n",
            "2799 0.0000 100.0\n",
            "2849 0.0001 100.0\n",
            "2899 0.0056 100.0\n",
            "2949 0.0004 100.0\n",
            "2999 0.0000 100.0\n",
            "3049 0.0103 100.0\n",
            "3099 0.0003 100.0\n",
            "3149 0.0000 100.0\n",
            "3199 0.0009 100.0\n",
            "3249 0.1214 93.75\n",
            "3299 0.0000 100.0\n",
            "3349 0.0059 100.0\n",
            "3399 0.1292 93.75\n",
            "3449 0.0026 100.0\n",
            "3499 0.0082 100.0\n",
            "3549 0.0005 100.0\n",
            "3599 0.0018 100.0\n",
            "3649 0.0011 100.0\n",
            "3699 0.0001 100.0\n",
            "3749 0.0001 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0116, Accuracy: 9648/10000 (96.48%)\n",
            "\n",
            "--- 17.101076364517212 seconds ---\n",
            "Epoch: 7\n",
            "49 0.0003 100.0\n",
            "99 0.0000 100.0\n",
            "149 0.0030 100.0\n",
            "199 0.0033 100.0\n",
            "249 0.0000 100.0\n",
            "299 0.0000 100.0\n",
            "349 0.0005 100.0\n",
            "399 0.0011 100.0\n",
            "449 0.0000 100.0\n",
            "499 0.0000 100.0\n",
            "549 0.0000 100.0\n",
            "599 0.0000 100.0\n",
            "649 0.0000 100.0\n",
            "699 0.0000 100.0\n",
            "749 0.0019 100.0\n",
            "799 0.0011 100.0\n",
            "849 0.0001 100.0\n",
            "899 0.0001 100.0\n",
            "949 0.0040 100.0\n",
            "999 0.0002 100.0\n",
            "1049 0.0000 100.0\n",
            "1099 0.0003 100.0\n",
            "1149 0.0011 100.0\n",
            "1199 0.0008 100.0\n",
            "1249 0.0002 100.0\n",
            "1299 0.0001 100.0\n",
            "1349 0.0006 100.0\n",
            "1399 0.0023 100.0\n",
            "1449 0.0000 100.0\n",
            "1499 0.0000 100.0\n",
            "1549 0.0000 100.0\n",
            "1599 0.0004 100.0\n",
            "1649 0.0000 100.0\n",
            "1699 0.0005 100.0\n",
            "1749 0.0018 100.0\n",
            "1799 0.0004 100.0\n",
            "1849 0.0002 100.0\n",
            "1899 0.0003 100.0\n",
            "1949 0.0046 100.0\n",
            "1999 0.0000 100.0\n",
            "2049 0.0005 100.0\n",
            "2099 0.0034 100.0\n",
            "2149 0.0124 100.0\n",
            "2199 0.0004 100.0\n",
            "2249 0.0713 93.75\n",
            "2299 0.0650 93.75\n",
            "2349 0.0045 100.0\n",
            "2399 0.0001 100.0\n",
            "2449 0.0083 100.0\n",
            "2499 0.0035 100.0\n",
            "2549 0.0003 100.0\n",
            "2599 0.0001 100.0\n",
            "2649 0.0003 100.0\n",
            "2699 0.0305 100.0\n",
            "2749 0.1039 93.75\n",
            "2799 0.0009 100.0\n",
            "2849 0.0007 100.0\n",
            "2899 0.0000 100.0\n",
            "2949 0.0001 100.0\n",
            "2999 0.0001 100.0\n",
            "3049 0.0000 100.0\n",
            "3099 0.0008 100.0\n",
            "3149 0.0798 93.75\n",
            "3199 0.0492 100.0\n",
            "3249 0.0000 100.0\n",
            "3299 0.0001 100.0\n",
            "3349 0.0008 100.0\n",
            "3399 0.0000 100.0\n",
            "3449 0.0001 100.0\n",
            "3499 0.0000 100.0\n",
            "3549 0.0000 100.0\n",
            "3599 0.0001 100.0\n",
            "3649 0.0268 100.0\n",
            "3699 0.0941 93.75\n",
            "3749 0.0021 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0114, Accuracy: 9678/10000 (96.78%)\n",
            "\n",
            "--- 17.086557626724243 seconds ---\n",
            "Epoch: 8\n",
            "49 0.0568 93.75\n",
            "99 0.0006 100.0\n",
            "149 0.0010 100.0\n",
            "199 0.0232 100.0\n",
            "249 0.0158 100.0\n",
            "299 0.0000 100.0\n",
            "349 0.0000 100.0\n",
            "399 0.0006 100.0\n",
            "449 0.0004 100.0\n",
            "499 0.0020 100.0\n",
            "549 0.0000 100.0\n",
            "599 0.0000 100.0\n",
            "649 0.0001 100.0\n",
            "699 0.0003 100.0\n",
            "749 0.0002 100.0\n",
            "799 0.0000 100.0\n",
            "849 0.0000 100.0\n",
            "899 0.0002 100.0\n",
            "949 0.0000 100.0\n",
            "999 0.0005 100.0\n",
            "1049 0.0201 100.0\n",
            "1099 0.0000 100.0\n",
            "1149 0.0001 100.0\n",
            "1199 0.0018 100.0\n",
            "1249 0.2108 93.75\n",
            "1299 0.0461 93.75\n",
            "1349 0.0001 100.0\n",
            "1399 0.0012 100.0\n",
            "1449 0.1624 93.75\n",
            "1499 0.0006 100.0\n",
            "1549 0.0000 100.0\n",
            "1599 0.0000 100.0\n",
            "1649 0.0009 100.0\n",
            "1699 0.0000 100.0\n",
            "1749 0.0000 100.0\n",
            "1799 0.0002 100.0\n",
            "1849 0.0053 100.0\n",
            "1899 0.0129 100.0\n",
            "1949 0.0000 100.0\n",
            "1999 0.0000 100.0\n",
            "2049 0.0000 100.0\n",
            "2099 0.0000 100.0\n",
            "2149 0.0008 100.0\n",
            "2199 0.0000 100.0\n",
            "2249 0.0002 100.0\n",
            "2299 0.0003 100.0\n",
            "2349 0.0176 100.0\n",
            "2399 0.0000 100.0\n",
            "2449 0.0192 100.0\n",
            "2499 0.0982 93.75\n",
            "2549 0.0120 100.0\n",
            "2599 0.0000 100.0\n",
            "2649 0.0005 100.0\n",
            "2699 0.0001 100.0\n",
            "2749 0.0005 100.0\n",
            "2799 0.0002 100.0\n",
            "2849 0.0001 100.0\n",
            "2899 0.0028 100.0\n",
            "2949 0.0003 100.0\n",
            "2999 0.0002 100.0\n",
            "3049 0.0001 100.0\n",
            "3099 0.0000 100.0\n",
            "3149 0.2727 93.75\n",
            "3199 0.0002 100.0\n",
            "3249 0.0000 100.0\n",
            "3299 0.0003 100.0\n",
            "3349 0.0000 100.0\n",
            "3399 0.0224 100.0\n",
            "3449 0.0001 100.0\n",
            "3499 0.0007 100.0\n",
            "3549 0.0031 100.0\n",
            "3599 0.0005 100.0\n",
            "3649 0.0061 100.0\n",
            "3699 0.0299 100.0\n",
            "3749 0.0000 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0105, Accuracy: 9678/10000 (96.78%)\n",
            "\n",
            "--- 17.94843292236328 seconds ---\n",
            "Epoch: 9\n",
            "49 0.0001 100.0\n",
            "99 0.0000 100.0\n",
            "149 0.0018 100.0\n",
            "199 0.0000 100.0\n",
            "249 0.0001 100.0\n",
            "299 0.0011 100.0\n",
            "349 0.0000 100.0\n",
            "399 0.0001 100.0\n",
            "449 0.0016 100.0\n",
            "499 0.0016 100.0\n",
            "549 0.0000 100.0\n",
            "599 0.0000 100.0\n",
            "649 0.0057 100.0\n",
            "699 0.0000 100.0\n",
            "749 0.0000 100.0\n",
            "799 0.0004 100.0\n",
            "849 0.0025 100.0\n",
            "899 0.0003 100.0\n",
            "949 0.0000 100.0\n",
            "999 0.1064 93.75\n",
            "1049 0.0006 100.0\n",
            "1099 0.0000 100.0\n",
            "1149 0.0003 100.0\n",
            "1199 0.0001 100.0\n",
            "1249 0.0007 100.0\n",
            "1299 0.0001 100.0\n",
            "1349 0.0001 100.0\n",
            "1399 0.0000 100.0\n",
            "1449 0.0001 100.0\n",
            "1499 0.0246 100.0\n",
            "1549 0.0000 100.0\n",
            "1599 0.0017 100.0\n",
            "1649 0.0000 100.0\n",
            "1699 0.0001 100.0\n",
            "1749 0.0005 100.0\n",
            "1799 0.0000 100.0\n",
            "1849 0.0118 100.0\n",
            "1899 0.0001 100.0\n",
            "1949 0.0000 100.0\n",
            "1999 0.0001 100.0\n",
            "2049 0.0009 100.0\n",
            "2099 0.0007 100.0\n",
            "2149 0.0000 100.0\n",
            "2199 0.0000 100.0\n",
            "2249 0.0000 100.0\n",
            "2299 0.0005 100.0\n",
            "2349 0.0000 100.0\n",
            "2399 0.0000 100.0\n",
            "2449 0.0000 100.0\n",
            "2499 0.0008 100.0\n",
            "2549 0.0001 100.0\n",
            "2599 0.0001 100.0\n",
            "2649 0.0041 100.0\n",
            "2699 0.0001 100.0\n",
            "2749 0.0006 100.0\n",
            "2799 0.0007 100.0\n",
            "2849 0.0001 100.0\n",
            "2899 0.0000 100.0\n",
            "2949 0.0002 100.0\n",
            "2999 0.0043 100.0\n",
            "3049 0.0002 100.0\n",
            "3099 0.0001 100.0\n",
            "3149 0.0047 100.0\n",
            "3199 0.0001 100.0\n",
            "3249 0.0014 100.0\n",
            "3299 0.0790 93.75\n",
            "3349 0.0008 100.0\n",
            "3399 0.0001 100.0\n",
            "3449 0.0003 100.0\n",
            "3499 0.0014 100.0\n",
            "3549 0.0002 100.0\n",
            "3599 0.0000 100.0\n",
            "3649 0.0000 100.0\n",
            "3699 0.0021 100.0\n",
            "3749 0.0000 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0108, Accuracy: 9702/10000 (97.02%)\n",
            "\n",
            "--- 17.090156316757202 seconds ---\n",
            "Epoch: 10\n",
            "49 0.0013 100.0\n",
            "99 0.0001 100.0\n",
            "149 0.0002 100.0\n",
            "199 0.0002 100.0\n",
            "249 0.0000 100.0\n",
            "299 0.0021 100.0\n",
            "349 0.0000 100.0\n",
            "399 0.0000 100.0\n",
            "449 0.0000 100.0\n",
            "499 0.0002 100.0\n",
            "549 0.0048 100.0\n",
            "599 0.0000 100.0\n",
            "649 0.0000 100.0\n",
            "699 0.0000 100.0\n",
            "749 0.0168 100.0\n",
            "799 0.0000 100.0\n",
            "849 0.0016 100.0\n",
            "899 0.0000 100.0\n",
            "949 0.0542 93.75\n",
            "999 0.0761 93.75\n",
            "1049 0.0000 100.0\n",
            "1099 0.0000 100.0\n",
            "1149 0.0001 100.0\n",
            "1199 0.0005 100.0\n",
            "1249 0.0026 100.0\n",
            "1299 0.0000 100.0\n",
            "1349 0.0000 100.0\n",
            "1399 0.0000 100.0\n",
            "1449 0.0000 100.0\n",
            "1499 0.0001 100.0\n",
            "1549 0.0015 100.0\n",
            "1599 0.0000 100.0\n",
            "1649 0.0000 100.0\n",
            "1699 0.0001 100.0\n",
            "1749 0.0155 100.0\n",
            "1799 0.0000 100.0\n",
            "1849 0.0000 100.0\n",
            "1899 0.0010 100.0\n",
            "1949 0.0000 100.0\n",
            "1999 0.0005 100.0\n",
            "2049 0.0000 100.0\n",
            "2099 0.0000 100.0\n",
            "2149 0.0001 100.0\n",
            "2199 0.0000 100.0\n",
            "2249 0.0000 100.0\n",
            "2299 0.0000 100.0\n",
            "2349 0.0001 100.0\n",
            "2399 0.0000 100.0\n",
            "2449 0.0003 100.0\n",
            "2499 0.0001 100.0\n",
            "2549 0.0000 100.0\n",
            "2599 0.0000 100.0\n",
            "2649 0.0000 100.0\n",
            "2699 0.0046 100.0\n",
            "2749 0.0000 100.0\n",
            "2799 0.2542 93.75\n",
            "2849 0.0023 100.0\n",
            "2899 0.0000 100.0\n",
            "2949 0.0000 100.0\n",
            "2999 0.0000 100.0\n",
            "3049 0.0000 100.0\n",
            "3099 0.0010 100.0\n",
            "3149 0.0005 100.0\n",
            "3199 0.0005 100.0\n",
            "3249 0.0000 100.0\n",
            "3299 0.0000 100.0\n",
            "3349 0.0001 100.0\n",
            "3399 0.0013 100.0\n",
            "3449 0.0000 100.0\n",
            "3499 0.0010 100.0\n",
            "3549 0.0000 100.0\n",
            "3599 0.0006 100.0\n",
            "3649 0.0002 100.0\n",
            "3699 0.0181 100.0\n",
            "3749 0.0004 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0104, Accuracy: 9702/10000 (97.02%)\n",
            "\n",
            "--- 16.92150354385376 seconds ---\n"
          ]
        }
      ],
      "source": [
        "#mnist epoch 10 rffg16vsa\n",
        "class ArgumentParser:\n",
        "    def __init__(self):\n",
        "        self.args = {\n",
        "            'lr': 0.01,            # Learning rate for optimizing class representative\n",
        "            'gamma': 0.3,          # Kernel parameter for computing covariance\n",
        "            'epoch': 10,            # Epochs of training\n",
        "            'gorder': 16,           # Order of the cyclic group required for G-VSA (8 ou 16)\n",
        "            'dim': 10000,          # Dimension of hypervectors c le grand D\n",
        "            'seed': 43,            # Random seed for reproducing results\n",
        "            'resume': True,       # Resume from existing encoded hypervectors\n",
        "            'data_dir': './encoded_data', # Directory to save encoded data (hypervectors)\n",
        "            'dataset': 'mnist',    # Dataset choice choices=['mnist', 'fmnist', 'cifar', 'isolet', 'ucihar']\n",
        "            'raw_data_dir': './dataset',   # Raw data directory\n",
        "            'model': 'rff-gvsa'    # Model type choices=['rff-hdc', 'linear-hdc', 'rff-gvsa']\n",
        "        }\n",
        "        self.seed = self.args['seed']\n",
        "        self.lr = self.args['lr']\n",
        "        self.gamma = self.args['gamma']\n",
        "        self.epoch = self.args['epoch']\n",
        "        self.gorder = self.args['gorder']\n",
        "        self.dim = self.args['dim']\n",
        "        self.resume = self.args['resume']\n",
        "        self.data_dir = self.args['data_dir']\n",
        "        self.dataset = self.args['dataset']\n",
        "        self.raw_data_dir = self.args['raw_data_dir']\n",
        "        self.model = self.args['model']\n",
        "\n",
        "\n",
        "    def get_args(self):\n",
        "        return self.args\n",
        "args = ArgumentParser()\n",
        "torch.manual_seed(args.seed)\n",
        "np.random.seed(args.seed)\n",
        "if 'hdc' in args.model:\n",
        "    args.gorder = 2\n",
        "    print(\"Use binary HDC with random fourier features, ignoring gorder, set to 2.\")\n",
        "args.data_dir = f'{args.data_dir}/{args.dataset}_{args.model}_order{args.gorder}_gamma{args.gamma}_dim{args.dim}'\n",
        "try:\n",
        "    os.makedirs(args.data_dir)\n",
        "except FileExistsError:\n",
        "    print('Encoded data folder already exists')\n",
        "if not args.resume:\n",
        "    print('Encode the dataset into hypervectors and save')\n",
        "    encode_and_save(args)\n",
        "    print('Finish encoding and saving')\n",
        "print(f'Optimizing class representatives for {args.epoch} epochs')\n",
        "train(args)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FMNIST"
      ],
      "metadata": {
        "id": "NdfgO_tSyb1n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#fminst epoch 1 rffg16vsa\n",
        "class ArgumentParser:\n",
        "    def __init__(self):\n",
        "        self.args = {\n",
        "            'lr': 0.01,            # Learning rate for optimizing class representative\n",
        "            'gamma': 0.3,          # Kernel parameter for computing covariance\n",
        "            'epoch': 1,            # Epochs of training\n",
        "            'gorder': 16,           # Order of the cyclic group required for G-VSA (8 ou 16)\n",
        "            'dim': 10000,          # Dimension of hypervectors c le grand D\n",
        "            'seed': 43,            # Random seed for reproducing results\n",
        "            'resume': False,       # Resume from existing encoded hypervectors\n",
        "            'data_dir': './encoded_data', # Directory to save encoded data (hypervectors)\n",
        "            'dataset': 'fmnist',    # Dataset choice choices=['mnist', 'fmnist', 'cifar', 'isolet', 'ucihar']\n",
        "            'raw_data_dir': './dataset',   # Raw data directory\n",
        "            'model': 'rff-gvsa'    # Model type choices=['rff-hdc', 'linear-hdc', 'rff-gvsa']\n",
        "        }\n",
        "        self.seed = self.args['seed']\n",
        "        self.lr = self.args['lr']\n",
        "        self.gamma = self.args['gamma']\n",
        "        self.epoch = self.args['epoch']\n",
        "        self.gorder = self.args['gorder']\n",
        "        self.dim = self.args['dim']\n",
        "        self.resume = self.args['resume']\n",
        "        self.data_dir = self.args['data_dir']\n",
        "        self.dataset = self.args['dataset']\n",
        "        self.raw_data_dir = self.args['raw_data_dir']\n",
        "        self.model = self.args['model']\n",
        "\n",
        "\n",
        "    def get_args(self):\n",
        "        return self.args\n",
        "\n",
        "args = ArgumentParser()\n",
        "torch.manual_seed(args.seed)\n",
        "np.random.seed(args.seed)\n",
        "if 'hdc' in args.model:\n",
        "    args.gorder = 2\n",
        "    print(\"Use binary HDC with random fourier features, ignoring gorder, set to 2.\")\n",
        "args.data_dir = f'{args.data_dir}/{args.dataset}_{args.model}_order{args.gorder}_gamma{args.gamma}_dim{args.dim}'\n",
        "try:\n",
        "    os.makedirs(args.data_dir)\n",
        "except FileExistsError:\n",
        "    print('Encoded data folder already exists')\n",
        "if not args.resume:\n",
        "    print('Encode the dataset into hypervectors and save')\n",
        "    encode_and_save(args)\n",
        "    print('Finish encoding and saving')\n",
        "print(f'Optimizing class representatives for {args.epoch} epochs')\n",
        "train(args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1RoJ4Ssjyb1n",
        "outputId": "8049eaeb-d3e5-4164-c32e-941e0d70a237"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encode the dataset into hypervectors and save\n",
            "# of channels of data 1\n",
            "# of training samples and test samples 60000 10000\n",
            "Encoding with random fourier features encoder.\n",
            "the threshold to discretize fourier features to group elements tensor([-1.5343, -1.1505, -0.8872, -0.6746, -0.4888, -0.3187, -0.1573,  0.0000,\n",
            "         0.1573,  0.3187,  0.4888,  0.6746,  0.8872,  1.1505,  1.5343])\n",
            "Encoded pixels to hypervectors with size:  torch.Size([256, 10000])\n",
            "Encoding training data...\n",
            "Start encoding data\n",
            "12800 images encoded. Total time elapse = 27.142229557037354\n",
            "25600 images encoded. Total time elapse = 54.38840985298157\n",
            "38400 images encoded. Total time elapse = 81.68396663665771\n",
            "51200 images encoded. Total time elapse = 108.8210518360138\n",
            "Finish encoding data\n",
            "Encoding test data...\n",
            "Start encoding data\n",
            "Finish encoding data\n",
            "Finish encoding and saving\n",
            "Optimizing class representatives for 1 epochs\n",
            "Loading encoded training data...\n",
            "Loading encoded test data...\n",
            "Size of encoded training data torch.Size([60000, 10000]) and test data torch.Size([10000, 10000])\n",
            "Epoch: 1\n",
            "49 0.9557 81.25\n",
            "99 1.1625 56.25\n",
            "149 1.0410 50.0\n",
            "199 0.1883 100.0\n",
            "249 0.1916 93.75\n",
            "299 0.6869 75.0\n",
            "349 0.8298 68.75\n",
            "399 0.4908 75.0\n",
            "449 0.3757 75.0\n",
            "499 0.5104 75.0\n",
            "549 0.2404 93.75\n",
            "599 0.2827 93.75\n",
            "649 0.3056 87.5\n",
            "699 0.1542 100.0\n",
            "749 0.0958 93.75\n",
            "799 0.3303 87.5\n",
            "849 0.2257 93.75\n",
            "899 0.2234 87.5\n",
            "949 0.3197 81.25\n",
            "999 0.4080 81.25\n",
            "1049 0.3961 81.25\n",
            "1099 0.2916 81.25\n",
            "1149 0.3142 87.5\n",
            "1199 0.6316 93.75\n",
            "1249 0.3243 87.5\n",
            "1299 0.2111 93.75\n",
            "1349 0.0793 100.0\n",
            "1399 0.4517 87.5\n",
            "1449 0.7354 75.0\n",
            "1499 0.0916 100.0\n",
            "1549 0.4833 75.0\n",
            "1599 0.1059 100.0\n",
            "1649 0.3688 81.25\n",
            "1699 0.2106 87.5\n",
            "1749 0.4622 87.5\n",
            "1799 0.2826 93.75\n",
            "1849 0.0881 100.0\n",
            "1899 0.2748 87.5\n",
            "1949 0.0863 100.0\n",
            "1999 0.4132 87.5\n",
            "2049 0.0981 93.75\n",
            "2099 0.7443 75.0\n",
            "2149 0.3284 81.25\n",
            "2199 0.6624 81.25\n",
            "2249 0.3600 87.5\n",
            "2299 0.2396 87.5\n",
            "2349 0.3706 75.0\n",
            "2399 0.4702 75.0\n",
            "2449 0.4129 87.5\n",
            "2499 0.2027 93.75\n",
            "2549 0.3218 87.5\n",
            "2599 0.5073 81.25\n",
            "2649 0.2561 87.5\n",
            "2699 0.5848 81.25\n",
            "2749 0.3145 81.25\n",
            "2799 0.0496 100.0\n",
            "2849 0.1038 100.0\n",
            "2899 0.2688 87.5\n",
            "2949 0.1212 100.0\n",
            "2999 0.5566 75.0\n",
            "3049 0.4228 87.5\n",
            "3099 0.4550 68.75\n",
            "3149 0.4218 87.5\n",
            "3199 0.1341 100.0\n",
            "3249 0.1437 93.75\n",
            "3299 0.3273 81.25\n",
            "3349 0.3832 81.25\n",
            "3399 0.6197 75.0\n",
            "3449 0.0659 100.0\n",
            "3499 0.2890 87.5\n",
            "3549 0.4385 81.25\n",
            "3599 0.0891 100.0\n",
            "3649 0.3854 75.0\n",
            "3699 0.4502 87.5\n",
            "3749 0.2629 93.75\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0220, Accuracy: 8709/10000 (87.09%)\n",
            "\n",
            "--- 16.825145959854126 seconds ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "441f8d99-6f9c-48ba-f745-a61928964beb",
        "id": "jJifIUk_yb1n"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoded data folder already exists\n",
            "Optimizing class representatives for 10 epochs\n",
            "Loading encoded training data...\n",
            "Loading encoded test data...\n",
            "Size of encoded training data torch.Size([60000, 10000]) and test data torch.Size([10000, 10000])\n",
            "Epoch: 1\n",
            "49 0.8041 93.75\n",
            "99 0.7669 75.0\n",
            "149 0.7923 62.5\n",
            "199 0.5341 81.25\n",
            "249 0.4300 87.5\n",
            "299 0.4295 81.25\n",
            "349 0.2961 81.25\n",
            "399 0.3494 87.5\n",
            "449 0.3736 87.5\n",
            "499 0.2432 93.75\n",
            "549 0.2971 93.75\n",
            "599 0.2208 87.5\n",
            "649 0.1537 93.75\n",
            "699 0.3024 93.75\n",
            "749 0.6402 75.0\n",
            "799 0.3485 87.5\n",
            "849 0.0934 100.0\n",
            "899 0.2670 87.5\n",
            "949 0.0810 100.0\n",
            "999 0.3599 81.25\n",
            "1049 0.6758 81.25\n",
            "1099 0.4542 87.5\n",
            "1149 0.1788 93.75\n",
            "1199 0.1975 93.75\n",
            "1249 0.1341 93.75\n",
            "1299 0.1457 93.75\n",
            "1349 0.2346 87.5\n",
            "1399 0.1789 87.5\n",
            "1449 0.1848 93.75\n",
            "1499 0.3786 75.0\n",
            "1549 0.5201 87.5\n",
            "1599 0.4463 81.25\n",
            "1649 0.2644 81.25\n",
            "1699 0.2733 87.5\n",
            "1749 0.1503 93.75\n",
            "1799 0.2226 93.75\n",
            "1849 0.2084 93.75\n",
            "1899 0.4137 87.5\n",
            "1949 0.3125 87.5\n",
            "1999 0.2820 87.5\n",
            "2049 0.3287 87.5\n",
            "2099 0.4737 75.0\n",
            "2149 0.0928 100.0\n",
            "2199 0.3912 93.75\n",
            "2249 0.3160 87.5\n",
            "2299 0.4015 81.25\n",
            "2349 0.4722 81.25\n",
            "2399 0.0920 100.0\n",
            "2449 0.3562 87.5\n",
            "2499 0.4940 75.0\n",
            "2549 0.4055 87.5\n",
            "2599 0.4397 81.25\n",
            "2649 0.2018 87.5\n",
            "2699 0.1514 87.5\n",
            "2749 0.0781 100.0\n",
            "2799 0.0722 93.75\n",
            "2849 0.2544 93.75\n",
            "2899 0.2895 81.25\n",
            "2949 0.7044 81.25\n",
            "2999 0.3082 93.75\n",
            "3049 0.7631 87.5\n",
            "3099 0.2352 93.75\n",
            "3149 0.3742 81.25\n",
            "3199 0.2799 87.5\n",
            "3249 0.1274 93.75\n",
            "3299 0.1175 93.75\n",
            "3349 0.5456 75.0\n",
            "3399 0.4419 93.75\n",
            "3449 0.4863 81.25\n",
            "3499 0.2929 93.75\n",
            "3549 0.3964 93.75\n",
            "3599 0.0652 100.0\n",
            "3649 0.5560 87.5\n",
            "3699 0.0974 100.0\n",
            "3749 0.2336 93.75\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0216, Accuracy: 8737/10000 (87.37%)\n",
            "\n",
            "--- 16.659645557403564 seconds ---\n",
            "Epoch: 2\n",
            "49 0.1399 93.75\n",
            "99 0.1217 93.75\n",
            "149 0.1911 93.75\n",
            "199 0.1029 100.0\n",
            "249 0.2231 87.5\n",
            "299 0.0343 100.0\n",
            "349 0.6385 81.25\n",
            "399 0.1076 100.0\n",
            "449 0.2303 93.75\n",
            "499 0.3871 87.5\n",
            "549 0.0943 93.75\n",
            "599 0.1485 93.75\n",
            "649 0.2432 87.5\n",
            "699 0.2215 93.75\n",
            "749 0.1501 87.5\n",
            "799 0.1166 93.75\n",
            "849 0.1168 93.75\n",
            "899 0.2711 93.75\n",
            "949 0.2628 87.5\n",
            "999 0.5369 87.5\n",
            "1049 0.1565 87.5\n",
            "1099 0.1248 93.75\n",
            "1149 0.1637 87.5\n",
            "1199 0.1018 100.0\n",
            "1249 0.5533 81.25\n",
            "1299 0.1972 93.75\n",
            "1349 0.0537 100.0\n",
            "1399 0.0541 100.0\n",
            "1449 0.3453 87.5\n",
            "1499 0.0824 93.75\n",
            "1549 0.1161 93.75\n",
            "1599 0.3369 81.25\n",
            "1649 0.0376 100.0\n",
            "1699 0.0664 100.0\n",
            "1749 0.1629 87.5\n",
            "1799 0.0501 100.0\n",
            "1849 0.3117 87.5\n",
            "1899 0.2360 93.75\n",
            "1949 0.5081 81.25\n",
            "1999 0.4667 75.0\n",
            "2049 0.0784 100.0\n",
            "2099 0.0881 100.0\n",
            "2149 0.3945 93.75\n",
            "2199 0.1908 93.75\n",
            "2249 0.2313 87.5\n",
            "2299 0.0367 100.0\n",
            "2349 0.1531 87.5\n",
            "2399 0.1099 93.75\n",
            "2449 0.0456 100.0\n",
            "2499 0.2242 93.75\n",
            "2549 0.1571 87.5\n",
            "2599 0.0775 100.0\n",
            "2649 0.1197 100.0\n",
            "2699 0.1513 93.75\n",
            "2749 0.1609 93.75\n",
            "2799 0.0516 100.0\n",
            "2849 0.2463 93.75\n",
            "2899 0.2721 87.5\n",
            "2949 0.2559 87.5\n",
            "2999 0.4042 75.0\n",
            "3049 0.1831 87.5\n",
            "3099 0.2331 93.75\n",
            "3149 0.3484 87.5\n",
            "3199 0.1507 93.75\n",
            "3249 0.3098 93.75\n",
            "3299 0.0908 93.75\n",
            "3349 0.1450 93.75\n",
            "3399 0.0589 100.0\n",
            "3449 0.0354 100.0\n",
            "3499 0.2535 87.5\n",
            "3549 0.0738 93.75\n",
            "3599 0.1008 100.0\n",
            "3649 0.1042 100.0\n",
            "3699 0.3694 81.25\n",
            "3749 0.1582 93.75\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0238, Accuracy: 8661/10000 (86.61%)\n",
            "\n",
            "--- 17.050408840179443 seconds ---\n",
            "Epoch: 3\n",
            "49 0.1951 87.5\n",
            "99 0.0608 100.0\n",
            "149 0.1219 100.0\n",
            "199 0.0814 100.0\n",
            "249 0.0621 93.75\n",
            "299 0.1101 93.75\n",
            "349 0.0164 100.0\n",
            "399 0.0693 93.75\n",
            "449 0.2290 93.75\n",
            "499 0.0269 100.0\n",
            "549 0.0284 100.0\n",
            "599 0.1456 93.75\n",
            "649 0.0050 100.0\n",
            "699 0.0911 93.75\n",
            "749 0.0214 100.0\n",
            "799 0.1447 93.75\n",
            "849 0.2748 87.5\n",
            "899 0.1660 93.75\n",
            "949 0.1482 93.75\n",
            "999 0.0634 100.0\n",
            "1049 0.6112 87.5\n",
            "1099 0.0192 100.0\n",
            "1149 0.0341 100.0\n",
            "1199 0.2967 87.5\n",
            "1249 0.0770 93.75\n",
            "1299 0.1045 93.75\n",
            "1349 0.0368 100.0\n",
            "1399 0.1045 93.75\n",
            "1449 0.3247 87.5\n",
            "1499 0.4095 93.75\n",
            "1549 0.3219 81.25\n",
            "1599 0.0348 100.0\n",
            "1649 0.0911 100.0\n",
            "1699 0.3776 93.75\n",
            "1749 0.0877 100.0\n",
            "1799 0.0882 93.75\n",
            "1849 0.0380 100.0\n",
            "1899 0.0769 93.75\n",
            "1949 0.0532 100.0\n",
            "1999 0.0303 100.0\n",
            "2049 0.0851 93.75\n",
            "2099 0.0462 100.0\n",
            "2149 0.0514 100.0\n",
            "2199 0.0406 100.0\n",
            "2249 0.0920 93.75\n",
            "2299 0.0194 100.0\n",
            "2349 0.0576 100.0\n",
            "2399 0.0534 100.0\n",
            "2449 0.1246 93.75\n",
            "2499 0.2293 93.75\n",
            "2549 0.0972 93.75\n",
            "2599 0.0427 100.0\n",
            "2649 0.0353 100.0\n",
            "2699 0.0664 100.0\n",
            "2749 0.0255 100.0\n",
            "2799 0.0426 100.0\n",
            "2849 0.0186 100.0\n",
            "2899 0.3651 87.5\n",
            "2949 0.0415 100.0\n",
            "2999 0.0714 100.0\n",
            "3049 0.2716 93.75\n",
            "3099 0.1612 93.75\n",
            "3149 0.0720 93.75\n",
            "3199 0.3359 93.75\n",
            "3249 0.1500 87.5\n",
            "3299 0.3013 93.75\n",
            "3349 0.0089 100.0\n",
            "3399 0.0347 100.0\n",
            "3449 0.0604 100.0\n",
            "3499 0.0335 100.0\n",
            "3549 0.0652 100.0\n",
            "3599 0.2593 93.75\n",
            "3649 0.0193 100.0\n",
            "3699 0.0711 100.0\n",
            "3749 0.1105 93.75\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0281, Accuracy: 8658/10000 (86.58%)\n",
            "\n",
            "--- 17.729957342147827 seconds ---\n",
            "Epoch: 4\n",
            "49 0.0167 100.0\n",
            "99 0.0624 100.0\n",
            "149 0.0554 100.0\n",
            "199 0.0100 100.0\n",
            "249 0.4270 93.75\n",
            "299 0.0210 100.0\n",
            "349 0.0253 100.0\n",
            "399 0.0032 100.0\n",
            "449 0.0141 100.0\n",
            "499 0.0300 100.0\n",
            "549 0.0578 100.0\n",
            "599 0.0064 100.0\n",
            "649 0.2117 93.75\n",
            "699 0.0224 100.0\n",
            "749 0.0057 100.0\n",
            "799 0.1183 93.75\n",
            "849 0.0156 100.0\n",
            "899 0.0085 100.0\n",
            "949 0.0429 100.0\n",
            "999 0.0386 100.0\n",
            "1049 0.0111 100.0\n",
            "1099 0.1704 93.75\n",
            "1149 0.1418 93.75\n",
            "1199 0.0113 100.0\n",
            "1249 0.1636 87.5\n",
            "1299 0.0435 100.0\n",
            "1349 0.0423 100.0\n",
            "1399 0.3968 87.5\n",
            "1449 0.0313 100.0\n",
            "1499 0.0729 93.75\n",
            "1549 0.3269 87.5\n",
            "1599 0.0697 100.0\n",
            "1649 0.3232 87.5\n",
            "1699 0.1329 93.75\n",
            "1749 0.3574 93.75\n",
            "1799 0.0283 100.0\n",
            "1849 0.0154 100.0\n",
            "1899 0.0268 100.0\n",
            "1949 0.0432 100.0\n",
            "1999 0.2501 87.5\n",
            "2049 0.0205 100.0\n",
            "2099 0.0660 93.75\n",
            "2149 0.0336 100.0\n",
            "2199 0.3989 93.75\n",
            "2249 0.1613 93.75\n",
            "2299 0.0026 100.0\n",
            "2349 0.0107 100.0\n",
            "2399 0.0827 93.75\n",
            "2449 0.0116 100.0\n",
            "2499 0.2084 93.75\n",
            "2549 0.2275 93.75\n",
            "2599 0.0067 100.0\n",
            "2649 0.0522 100.0\n",
            "2699 0.0188 100.0\n",
            "2749 0.1074 93.75\n",
            "2799 0.0004 100.0\n",
            "2849 0.0057 100.0\n",
            "2899 0.0429 100.0\n",
            "2949 0.1100 93.75\n",
            "2999 0.1215 93.75\n",
            "3049 0.0484 100.0\n",
            "3099 0.3564 87.5\n",
            "3149 0.0899 93.75\n",
            "3199 0.0061 100.0\n",
            "3249 0.0420 100.0\n",
            "3299 0.0238 100.0\n",
            "3349 0.0870 93.75\n",
            "3399 0.3461 87.5\n",
            "3449 0.1361 93.75\n",
            "3499 0.1968 93.75\n",
            "3549 0.0735 93.75\n",
            "3599 0.0443 100.0\n",
            "3649 0.0101 100.0\n",
            "3699 0.0053 100.0\n",
            "3749 0.0554 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0323, Accuracy: 8654/10000 (86.54%)\n",
            "\n",
            "--- 17.299436807632446 seconds ---\n",
            "Epoch: 5\n",
            "49 0.0361 100.0\n",
            "99 0.0046 100.0\n",
            "149 0.1902 87.5\n",
            "199 0.0895 93.75\n",
            "249 0.0046 100.0\n",
            "299 0.3511 87.5\n",
            "349 0.0132 100.0\n",
            "399 0.0173 100.0\n",
            "449 0.1337 93.75\n",
            "499 0.0047 100.0\n",
            "549 0.0007 100.0\n",
            "599 0.0687 100.0\n",
            "649 0.0149 100.0\n",
            "699 0.0508 100.0\n",
            "749 0.1840 93.75\n",
            "799 0.2503 93.75\n",
            "849 0.0158 100.0\n",
            "899 0.0006 100.0\n",
            "949 0.0287 100.0\n",
            "999 0.0253 100.0\n",
            "1049 0.0171 100.0\n",
            "1099 0.2304 93.75\n",
            "1149 0.1841 93.75\n",
            "1199 0.0633 100.0\n",
            "1249 0.0244 100.0\n",
            "1299 0.0095 100.0\n",
            "1349 0.0197 100.0\n",
            "1399 0.0386 100.0\n",
            "1449 0.0135 100.0\n",
            "1499 0.0067 100.0\n",
            "1549 0.0183 100.0\n",
            "1599 0.0665 100.0\n",
            "1649 0.0289 100.0\n",
            "1699 0.0194 100.0\n",
            "1749 0.0047 100.0\n",
            "1799 0.0776 93.75\n",
            "1849 0.0082 100.0\n",
            "1899 0.1848 81.25\n",
            "1949 0.0023 100.0\n",
            "1999 0.0415 100.0\n",
            "2049 0.2605 93.75\n",
            "2099 0.1116 93.75\n",
            "2149 0.1880 93.75\n",
            "2199 0.1480 93.75\n",
            "2249 0.0032 100.0\n",
            "2299 0.0729 93.75\n",
            "2349 0.0000 100.0\n",
            "2399 0.0009 100.0\n",
            "2449 0.1695 93.75\n",
            "2499 0.0004 100.0\n",
            "2549 0.1034 93.75\n",
            "2599 0.0235 100.0\n",
            "2649 0.0915 100.0\n",
            "2699 0.1261 93.75\n",
            "2749 0.0237 100.0\n",
            "2799 0.0260 100.0\n",
            "2849 0.0289 100.0\n",
            "2899 0.2539 93.75\n",
            "2949 0.0372 100.0\n",
            "2999 0.0701 100.0\n",
            "3049 0.0209 100.0\n",
            "3099 0.2045 93.75\n",
            "3149 0.2948 93.75\n",
            "3199 0.0362 100.0\n",
            "3249 0.0017 100.0\n",
            "3299 0.0793 93.75\n",
            "3349 0.0053 100.0\n",
            "3399 0.0021 100.0\n",
            "3449 0.1644 93.75\n",
            "3499 0.1358 93.75\n",
            "3549 0.0118 100.0\n",
            "3599 0.0248 100.0\n",
            "3649 0.2067 87.5\n",
            "3699 0.0072 100.0\n",
            "3749 0.0433 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0358, Accuracy: 8691/10000 (86.91%)\n",
            "\n",
            "--- 17.45985507965088 seconds ---\n",
            "Epoch: 6\n",
            "49 0.0484 100.0\n",
            "99 0.0384 100.0\n",
            "149 0.1383 93.75\n",
            "199 0.0866 93.75\n",
            "249 0.0035 100.0\n",
            "299 0.0156 100.0\n",
            "349 0.0043 100.0\n",
            "399 0.0384 100.0\n",
            "449 0.0002 100.0\n",
            "499 0.0451 100.0\n",
            "549 0.0020 100.0\n",
            "599 0.0133 100.0\n",
            "649 0.0941 93.75\n",
            "699 0.2477 87.5\n",
            "749 0.2663 93.75\n",
            "799 0.0247 100.0\n",
            "849 0.0413 100.0\n",
            "899 0.1444 93.75\n",
            "949 0.0743 100.0\n",
            "999 0.0042 100.0\n",
            "1049 0.0066 100.0\n",
            "1099 0.0108 100.0\n",
            "1149 0.0223 100.0\n",
            "1199 0.0023 100.0\n",
            "1249 0.0316 100.0\n",
            "1299 0.0003 100.0\n",
            "1349 0.0863 93.75\n",
            "1399 0.0229 100.0\n",
            "1449 0.0015 100.0\n",
            "1499 0.0021 100.0\n",
            "1549 0.0212 100.0\n",
            "1599 0.0015 100.0\n",
            "1649 0.0008 100.0\n",
            "1699 0.0117 100.0\n",
            "1749 0.0168 100.0\n",
            "1799 0.0557 100.0\n",
            "1849 0.0029 100.0\n",
            "1899 0.0977 93.75\n",
            "1949 0.1355 93.75\n",
            "1999 0.0714 100.0\n",
            "2049 0.0100 100.0\n",
            "2099 0.0070 100.0\n",
            "2149 0.0010 100.0\n",
            "2199 0.0200 100.0\n",
            "2249 0.0507 93.75\n",
            "2299 0.0278 100.0\n",
            "2349 0.0787 93.75\n",
            "2399 0.0080 100.0\n",
            "2449 0.0478 100.0\n",
            "2499 0.0033 100.0\n",
            "2549 0.0183 100.0\n",
            "2599 0.1407 93.75\n",
            "2649 0.0011 100.0\n",
            "2699 0.0409 100.0\n",
            "2749 0.0053 100.0\n",
            "2799 0.0163 100.0\n",
            "2849 0.0011 100.0\n",
            "2899 0.0610 100.0\n",
            "2949 0.0802 100.0\n",
            "2999 0.0225 100.0\n",
            "3049 0.2711 93.75\n",
            "3099 0.0042 100.0\n",
            "3149 0.0179 100.0\n",
            "3199 0.1459 93.75\n",
            "3249 0.0396 100.0\n",
            "3299 0.4155 87.5\n",
            "3349 0.0176 100.0\n",
            "3399 0.0219 100.0\n",
            "3449 0.1759 93.75\n",
            "3499 0.1244 93.75\n",
            "3549 0.0112 100.0\n",
            "3599 0.0454 100.0\n",
            "3649 0.0127 100.0\n",
            "3699 0.0015 100.0\n",
            "3749 0.1949 93.75\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0437, Accuracy: 8645/10000 (86.45%)\n",
            "\n",
            "--- 17.61484169960022 seconds ---\n",
            "Epoch: 7\n",
            "49 0.0006 100.0\n",
            "99 0.0533 93.75\n",
            "149 0.0010 100.0\n",
            "199 0.0027 100.0\n",
            "249 0.0041 100.0\n",
            "299 0.0013 100.0\n",
            "349 0.0015 100.0\n",
            "399 0.0354 100.0\n",
            "449 0.0030 100.0\n",
            "499 0.0021 100.0\n",
            "549 0.3333 93.75\n",
            "599 0.0064 100.0\n",
            "649 0.0302 100.0\n",
            "699 0.0141 100.0\n",
            "749 0.0414 100.0\n",
            "799 0.0449 100.0\n",
            "849 0.0552 93.75\n",
            "899 0.0133 100.0\n",
            "949 0.0015 100.0\n",
            "999 0.0047 100.0\n",
            "1049 0.0001 100.0\n",
            "1099 0.0001 100.0\n",
            "1149 0.0008 100.0\n",
            "1199 0.0045 100.0\n",
            "1249 0.2255 93.75\n",
            "1299 0.0234 100.0\n",
            "1349 0.0083 100.0\n",
            "1399 0.0122 100.0\n",
            "1449 0.0365 100.0\n",
            "1499 0.0011 100.0\n",
            "1549 0.0322 100.0\n",
            "1599 0.0013 100.0\n",
            "1649 0.0095 100.0\n",
            "1699 0.0049 100.0\n",
            "1749 0.0016 100.0\n",
            "1799 0.0880 93.75\n",
            "1849 0.0003 100.0\n",
            "1899 0.0432 100.0\n",
            "1949 0.0084 100.0\n",
            "1999 0.0249 100.0\n",
            "2049 0.1821 93.75\n",
            "2099 0.0278 100.0\n",
            "2149 0.0006 100.0\n",
            "2199 0.0050 100.0\n",
            "2249 0.0032 100.0\n",
            "2299 0.0184 100.0\n",
            "2349 0.0006 100.0\n",
            "2399 0.0011 100.0\n",
            "2449 0.0320 100.0\n",
            "2499 0.0084 100.0\n",
            "2549 0.1651 93.75\n",
            "2599 0.0335 100.0\n",
            "2649 0.1522 87.5\n",
            "2699 0.3514 87.5\n",
            "2749 0.0187 100.0\n",
            "2799 0.0539 93.75\n",
            "2849 0.0057 100.0\n",
            "2899 0.1405 93.75\n",
            "2949 0.0055 100.0\n",
            "2999 0.0424 100.0\n",
            "3049 0.0263 100.0\n",
            "3099 0.2212 93.75\n",
            "3149 0.2108 87.5\n",
            "3199 0.0261 100.0\n",
            "3249 0.0125 100.0\n",
            "3299 0.0355 100.0\n",
            "3349 0.0169 100.0\n",
            "3399 0.1254 93.75\n",
            "3449 0.0173 100.0\n",
            "3499 0.0849 93.75\n",
            "3549 0.0177 100.0\n",
            "3599 0.0636 100.0\n",
            "3649 0.0256 100.0\n",
            "3699 0.0171 100.0\n",
            "3749 0.1335 93.75\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0499, Accuracy: 8584/10000 (85.84%)\n",
            "\n",
            "--- 17.458659172058105 seconds ---\n",
            "Epoch: 8\n",
            "49 0.0183 100.0\n",
            "99 0.0085 100.0\n",
            "149 0.0002 100.0\n",
            "199 0.0002 100.0\n",
            "249 0.0121 100.0\n",
            "299 0.0044 100.0\n",
            "349 0.0003 100.0\n",
            "399 0.0025 100.0\n",
            "449 0.0145 100.0\n",
            "499 0.0568 93.75\n",
            "549 0.0062 100.0\n",
            "599 0.0546 100.0\n",
            "649 0.0004 100.0\n",
            "699 0.0218 100.0\n",
            "749 0.0017 100.0\n",
            "799 0.0020 100.0\n",
            "849 0.0032 100.0\n",
            "899 0.0085 100.0\n",
            "949 0.0004 100.0\n",
            "999 0.0540 100.0\n",
            "1049 0.3507 87.5\n",
            "1099 0.0309 100.0\n",
            "1149 0.0015 100.0\n",
            "1199 0.0034 100.0\n",
            "1249 0.0938 93.75\n",
            "1299 0.0784 93.75\n",
            "1349 0.0083 100.0\n",
            "1399 0.0006 100.0\n",
            "1449 0.0099 100.0\n",
            "1499 0.0007 100.0\n",
            "1549 0.0128 100.0\n",
            "1599 0.0224 100.0\n",
            "1649 0.0772 93.75\n",
            "1699 0.0219 100.0\n",
            "1749 0.0190 100.0\n",
            "1799 0.0083 100.0\n",
            "1849 0.0013 100.0\n",
            "1899 0.0408 100.0\n",
            "1949 0.0515 93.75\n",
            "1999 0.0039 100.0\n",
            "2049 0.0346 100.0\n",
            "2099 0.0004 100.0\n",
            "2149 0.0008 100.0\n",
            "2199 0.0061 100.0\n",
            "2249 0.0570 93.75\n",
            "2299 0.0074 100.0\n",
            "2349 0.1822 93.75\n",
            "2399 0.0030 100.0\n",
            "2449 0.1305 87.5\n",
            "2499 0.0009 100.0\n",
            "2549 0.0025 100.0\n",
            "2599 0.0097 100.0\n",
            "2649 0.0867 93.75\n",
            "2699 0.0002 100.0\n",
            "2749 0.0068 100.0\n",
            "2799 0.0179 100.0\n",
            "2849 0.0900 93.75\n",
            "2899 0.0053 100.0\n",
            "2949 0.0394 100.0\n",
            "2999 0.0013 100.0\n",
            "3049 0.0047 100.0\n",
            "3099 0.0094 100.0\n",
            "3149 0.2877 87.5\n",
            "3199 0.0149 100.0\n",
            "3249 0.0066 100.0\n",
            "3299 0.0311 100.0\n",
            "3349 0.0093 100.0\n",
            "3399 0.0403 100.0\n",
            "3449 0.0188 100.0\n",
            "3499 0.0040 100.0\n",
            "3549 0.0208 100.0\n",
            "3599 0.0269 100.0\n",
            "3649 0.1339 93.75\n",
            "3699 0.0027 100.0\n",
            "3749 0.0668 93.75\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0506, Accuracy: 8689/10000 (86.89%)\n",
            "\n",
            "--- 18.022214651107788 seconds ---\n",
            "Epoch: 9\n",
            "49 0.0001 100.0\n",
            "99 0.0019 100.0\n",
            "149 0.0027 100.0\n",
            "199 0.3176 87.5\n",
            "249 0.0049 100.0\n",
            "299 0.0009 100.0\n",
            "349 0.0029 100.0\n",
            "399 0.0029 100.0\n",
            "449 0.0007 100.0\n",
            "499 0.0000 100.0\n",
            "549 0.0721 93.75\n",
            "599 0.0008 100.0\n",
            "649 0.0007 100.0\n",
            "699 0.1577 87.5\n",
            "749 0.0077 100.0\n",
            "799 0.0002 100.0\n",
            "849 0.0017 100.0\n",
            "899 0.0034 100.0\n",
            "949 0.0082 100.0\n",
            "999 0.0018 100.0\n",
            "1049 0.1646 93.75\n",
            "1099 0.0013 100.0\n",
            "1149 0.0065 100.0\n",
            "1199 0.0053 100.0\n",
            "1249 0.0080 100.0\n",
            "1299 0.0080 100.0\n",
            "1349 0.0615 93.75\n",
            "1399 0.0333 100.0\n",
            "1449 0.0012 100.0\n",
            "1499 0.0047 100.0\n",
            "1549 0.0078 100.0\n",
            "1599 0.0209 100.0\n",
            "1649 0.1487 93.75\n",
            "1699 0.0007 100.0\n",
            "1749 0.0246 100.0\n",
            "1799 0.0047 100.0\n",
            "1849 0.0223 100.0\n",
            "1899 0.1812 93.75\n",
            "1949 0.1071 93.75\n",
            "1999 0.0059 100.0\n",
            "2049 0.1144 93.75\n",
            "2099 0.0081 100.0\n",
            "2149 0.0115 100.0\n",
            "2199 0.0043 100.0\n",
            "2249 0.0608 93.75\n",
            "2299 0.4426 87.5\n",
            "2349 0.0595 100.0\n",
            "2399 0.1225 93.75\n",
            "2449 0.0603 93.75\n",
            "2499 0.0376 100.0\n",
            "2549 0.0413 100.0\n",
            "2599 0.0002 100.0\n",
            "2649 0.0089 100.0\n",
            "2699 0.0033 100.0\n",
            "2749 0.3814 93.75\n",
            "2799 0.4178 93.75\n",
            "2849 0.0000 100.0\n",
            "2899 0.0362 100.0\n",
            "2949 0.0002 100.0\n",
            "2999 0.0164 100.0\n",
            "3049 0.0089 100.0\n",
            "3099 0.0009 100.0\n",
            "3149 0.0006 100.0\n",
            "3199 0.1156 93.75\n",
            "3249 0.0273 100.0\n",
            "3299 0.0126 100.0\n",
            "3349 0.0767 93.75\n",
            "3399 0.0095 100.0\n",
            "3449 0.0055 100.0\n",
            "3499 0.2060 93.75\n",
            "3549 0.0026 100.0\n",
            "3599 0.1675 87.5\n",
            "3649 0.0115 100.0\n",
            "3699 0.0660 93.75\n",
            "3749 0.0124 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0540, Accuracy: 8626/10000 (86.26%)\n",
            "\n",
            "--- 17.29578924179077 seconds ---\n",
            "Epoch: 10\n",
            "49 0.0702 93.75\n",
            "99 0.0093 100.0\n",
            "149 0.0027 100.0\n",
            "199 0.0000 100.0\n",
            "249 0.0727 93.75\n",
            "299 0.0011 100.0\n",
            "349 0.0013 100.0\n",
            "399 0.0016 100.0\n",
            "449 0.0025 100.0\n",
            "499 0.0371 100.0\n",
            "549 0.0012 100.0\n",
            "599 0.0008 100.0\n",
            "649 0.0059 100.0\n",
            "699 0.0002 100.0\n",
            "749 0.0001 100.0\n",
            "799 0.0188 100.0\n",
            "849 0.1279 93.75\n",
            "899 0.0994 93.75\n",
            "949 0.0335 100.0\n",
            "999 0.0126 100.0\n",
            "1049 0.0043 100.0\n",
            "1099 0.0131 100.0\n",
            "1149 0.0003 100.0\n",
            "1199 0.0286 100.0\n",
            "1249 0.0022 100.0\n",
            "1299 0.0004 100.0\n",
            "1349 0.0019 100.0\n",
            "1399 0.0012 100.0\n",
            "1449 0.0025 100.0\n",
            "1499 0.0004 100.0\n",
            "1549 0.0003 100.0\n",
            "1599 0.0458 100.0\n",
            "1649 0.0000 100.0\n",
            "1699 0.0180 100.0\n",
            "1749 0.0087 100.0\n",
            "1799 0.0042 100.0\n",
            "1849 0.0402 100.0\n",
            "1899 0.0473 100.0\n",
            "1949 0.0026 100.0\n",
            "1999 0.0012 100.0\n",
            "2049 0.0260 100.0\n",
            "2099 0.0011 100.0\n",
            "2149 0.0000 100.0\n",
            "2199 0.0000 100.0\n",
            "2249 0.1408 93.75\n",
            "2299 0.0034 100.0\n",
            "2349 0.0016 100.0\n",
            "2399 0.0654 100.0\n",
            "2449 0.0003 100.0\n",
            "2499 0.0231 100.0\n",
            "2549 0.0094 100.0\n",
            "2599 0.0063 100.0\n",
            "2649 0.0573 93.75\n",
            "2699 0.0143 100.0\n",
            "2749 0.0305 100.0\n",
            "2799 0.0026 100.0\n",
            "2849 0.0079 100.0\n",
            "2899 0.0005 100.0\n",
            "2949 0.0056 100.0\n",
            "2999 0.0131 100.0\n",
            "3049 0.4444 93.75\n",
            "3099 0.0045 100.0\n",
            "3149 0.0188 100.0\n",
            "3199 0.1307 93.75\n",
            "3249 0.0145 100.0\n",
            "3299 0.0151 100.0\n",
            "3349 0.0158 100.0\n",
            "3399 0.0090 100.0\n",
            "3449 0.0348 100.0\n",
            "3499 0.0014 100.0\n",
            "3549 0.0017 100.0\n",
            "3599 0.1882 93.75\n",
            "3649 0.0026 100.0\n",
            "3699 0.0195 100.0\n",
            "3749 0.0186 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0568, Accuracy: 8575/10000 (85.75%)\n",
            "\n",
            "--- 17.11533808708191 seconds ---\n"
          ]
        }
      ],
      "source": [
        "#fminst epoch 10 rffg16vsa\n",
        "class ArgumentParser:\n",
        "    def __init__(self):\n",
        "        self.args = {\n",
        "            'lr': 0.01,            # Learning rate for optimizing class representative\n",
        "            'gamma': 0.3,          # Kernel parameter for computing covariance\n",
        "            'epoch': 10,            # Epochs of training\n",
        "            'gorder': 16,           # Order of the cyclic group required for G-VSA (8 ou 16)\n",
        "            'dim': 10000,          # Dimension of hypervectors c le grand D\n",
        "            'seed': 43,            # Random seed for reproducing results\n",
        "            'resume': True,       # Resume from existing encoded hypervectors\n",
        "            'data_dir': './encoded_data', # Directory to save encoded data (hypervectors)\n",
        "            'dataset': 'fmnist',    # Dataset choice choices=['mnist', 'fmnist', 'cifar', 'isolet', 'ucihar']\n",
        "            'raw_data_dir': './dataset',   # Raw data directory\n",
        "            'model': 'rff-gvsa'    # Model type choices=['rff-hdc', 'linear-hdc', 'rff-gvsa']\n",
        "        }\n",
        "        self.seed = self.args['seed']\n",
        "        self.lr = self.args['lr']\n",
        "        self.gamma = self.args['gamma']\n",
        "        self.epoch = self.args['epoch']\n",
        "        self.gorder = self.args['gorder']\n",
        "        self.dim = self.args['dim']\n",
        "        self.resume = self.args['resume']\n",
        "        self.data_dir = self.args['data_dir']\n",
        "        self.dataset = self.args['dataset']\n",
        "        self.raw_data_dir = self.args['raw_data_dir']\n",
        "        self.model = self.args['model']\n",
        "\n",
        "\n",
        "    def get_args(self):\n",
        "        return self.args\n",
        "args = ArgumentParser()\n",
        "torch.manual_seed(args.seed)\n",
        "np.random.seed(args.seed)\n",
        "if 'hdc' in args.model:\n",
        "    args.gorder = 2\n",
        "    print(\"Use binary HDC with random fourier features, ignoring gorder, set to 2.\")\n",
        "args.data_dir = f'{args.data_dir}/{args.dataset}_{args.model}_order{args.gorder}_gamma{args.gamma}_dim{args.dim}'\n",
        "try:\n",
        "    os.makedirs(args.data_dir)\n",
        "except FileExistsError:\n",
        "    print('Encoded data folder already exists')\n",
        "if not args.resume:\n",
        "    print('Encode the dataset into hypervectors and save')\n",
        "    encode_and_save(args)\n",
        "    print('Finish encoding and saving')\n",
        "print(f'Optimizing class representatives for {args.epoch} epochs')\n",
        "train(args)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# G($2^5$)-VSA"
      ],
      "metadata": {
        "id": "T2wupXNZZ-F3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ISOLET"
      ],
      "metadata": {
        "id": "0KTP7GG5Z-F5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d6ce412-7c78-482c-bade-5c703657aa40",
        "id": "EpnCnrNXZ-F5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encode the dataset into hypervectors and save\n",
            "# of channels of data 1\n",
            "# of training samples and test samples 6238 1559\n",
            "Encoding with random fourier features encoder.\n",
            "the threshold to discretize fourier features to group elements tensor([-1.8629, -1.5343, -1.3182, -1.1505, -1.0101, -0.8872, -0.7765, -0.6746,\n",
            "        -0.5792, -0.4888, -0.4023, -0.3187, -0.2372, -0.1573, -0.0784,  0.0000,\n",
            "         0.0784,  0.1573,  0.2372,  0.3187,  0.4023,  0.4888,  0.5792,  0.6746,\n",
            "         0.7765,  0.8872,  1.0101,  1.1505,  1.3182,  1.5343,  1.8629])\n",
            "Encoded pixels to hypervectors with size:  torch.Size([256, 10000])\n",
            "Encoding training data...\n",
            "Start encoding data\n",
            "Finish encoding data\n",
            "Encoding test data...\n",
            "Start encoding data\n",
            "Finish encoding data\n",
            "Finish encoding and saving\n",
            "Optimizing class representatives for 1 epochs\n",
            "Loading encoded training data...\n",
            "Loading encoded test data...\n",
            "Size of encoded training data torch.Size([6238, 10000]) and test data torch.Size([1559, 10000])\n",
            "Epoch: 1\n",
            "49 2.5880 50.0\n",
            "99 1.6071 81.25\n",
            "149 1.0270 75.0\n",
            "199 0.6585 81.25\n",
            "249 0.6165 81.25\n",
            "299 0.6529 81.25\n",
            "349 0.2344 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0214, Accuracy: 1453/1559 (93.20%)\n",
            "\n",
            "--- 2.6142191886901855 seconds ---\n"
          ]
        }
      ],
      "source": [
        "#isolet epoch 1 rffg32vsa\n",
        "class ArgumentParser:\n",
        "    def __init__(self):\n",
        "        self.args = {\n",
        "            'lr': 0.01,            # Learning rate for optimizing class representative\n",
        "            'gamma': 0.3,          # Kernel parameter for computing covariance\n",
        "            'epoch': 1,            # Epochs of training\n",
        "            'gorder': 32,           # Order of the cyclic group required for G-VSA (8 ou 16)\n",
        "            'dim': 10000,          # Dimension of hypervectors c le grand D\n",
        "            'seed': 43,            # Random seed for reproducing results\n",
        "            'resume': False,       # Resume from existing encoded hypervectors\n",
        "            'data_dir': './encoded_data', # Directory to save encoded data (hypervectors)\n",
        "            'dataset': 'isolet',    # Dataset choice choices=['mnist', 'fmnist', 'cifar', 'isolet', 'ucihar']\n",
        "            'raw_data_dir': './dataset',   # Raw data directory\n",
        "            'model': 'rff-gvsa'    # Model type choices=['rff-hdc', 'linear-hdc', 'rff-gvsa']\n",
        "        }\n",
        "        self.seed = self.args['seed']\n",
        "        self.lr = self.args['lr']\n",
        "        self.gamma = self.args['gamma']\n",
        "        self.epoch = self.args['epoch']\n",
        "        self.gorder = self.args['gorder']\n",
        "        self.dim = self.args['dim']\n",
        "        self.resume = self.args['resume']\n",
        "        self.data_dir = self.args['data_dir']\n",
        "        self.dataset = self.args['dataset']\n",
        "        self.raw_data_dir = self.args['raw_data_dir']\n",
        "        self.model = self.args['model']\n",
        "\n",
        "\n",
        "    def get_args(self):\n",
        "        return self.args\n",
        "args = ArgumentParser()\n",
        "torch.manual_seed(args.seed)\n",
        "np.random.seed(args.seed)\n",
        "if 'hdc' in args.model:\n",
        "    args.gorder = 2\n",
        "    print(\"Use binary HDC with random fourier features, ignoring gorder, set to 2.\")\n",
        "args.data_dir = f'{args.data_dir}/{args.dataset}_{args.model}_order{args.gorder}_gamma{args.gamma}_dim{args.dim}'\n",
        "try:\n",
        "    os.makedirs(args.data_dir)\n",
        "except FileExistsError:\n",
        "    print('Encoded data folder already exists')\n",
        "if not args.resume:\n",
        "    print('Encode the dataset into hypervectors and save')\n",
        "    encode_and_save(args)\n",
        "    print('Finish encoding and saving')\n",
        "print(f'Optimizing class representatives for {args.epoch} epochs')\n",
        "train(args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d241d0a-0aeb-496d-c118-7e70b4c32d30",
        "id": "CPr2QJ3LZ-F6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoded data folder already exists\n",
            "Optimizing class representatives for 10 epochs\n",
            "Loading encoded training data...\n",
            "Loading encoded test data...\n",
            "Size of encoded training data torch.Size([6238, 10000]) and test data torch.Size([1559, 10000])\n",
            "Epoch: 1\n",
            "49 2.6985 31.25\n",
            "99 1.8406 56.25\n",
            "149 0.5459 100.0\n",
            "199 0.9736 68.75\n",
            "249 0.5405 81.25\n",
            "299 0.5080 87.5\n",
            "349 0.2111 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0212, Accuracy: 1452/1559 (93.14%)\n",
            "\n",
            "--- 2.117562770843506 seconds ---\n",
            "Epoch: 2\n",
            "49 0.1760 93.75\n",
            "99 0.2331 93.75\n",
            "149 0.1206 100.0\n",
            "199 0.0513 100.0\n",
            "249 0.2029 93.75\n",
            "299 0.0600 100.0\n",
            "349 0.1174 93.75\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0129, Accuracy: 1483/1559 (95.13%)\n",
            "\n",
            "--- 1.971390724182129 seconds ---\n",
            "Epoch: 3\n",
            "49 0.0518 100.0\n",
            "99 0.0393 100.0\n",
            "149 0.0412 100.0\n",
            "199 0.2849 87.5\n",
            "249 0.0486 100.0\n",
            "299 0.0579 100.0\n",
            "349 0.0409 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0117, Accuracy: 1465/1559 (93.97%)\n",
            "\n",
            "--- 2.0153322219848633 seconds ---\n",
            "Epoch: 4\n",
            "49 0.0076 100.0\n",
            "99 0.0479 100.0\n",
            "149 0.0301 100.0\n",
            "199 0.0120 100.0\n",
            "249 0.0136 100.0\n",
            "299 0.0153 100.0\n",
            "349 0.0485 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0094, Accuracy: 1489/1559 (95.51%)\n",
            "\n",
            "--- 1.971301555633545 seconds ---\n",
            "Epoch: 5\n",
            "49 0.0378 100.0\n",
            "99 0.0127 100.0\n",
            "149 0.0082 100.0\n",
            "199 0.0128 100.0\n",
            "249 0.0081 100.0\n",
            "299 0.0180 100.0\n",
            "349 0.0143 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0094, Accuracy: 1483/1559 (95.13%)\n",
            "\n",
            "--- 1.96854829788208 seconds ---\n",
            "Epoch: 6\n",
            "49 0.0018 100.0\n",
            "99 0.0058 100.0\n",
            "149 0.0254 100.0\n",
            "199 0.0077 100.0\n",
            "249 0.0048 100.0\n",
            "299 0.0255 100.0\n",
            "349 0.0089 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0087, Accuracy: 1487/1559 (95.38%)\n",
            "\n",
            "--- 2.823488712310791 seconds ---\n",
            "Epoch: 7\n",
            "49 0.0081 100.0\n",
            "99 0.0048 100.0\n",
            "149 0.0049 100.0\n",
            "199 0.0039 100.0\n",
            "249 0.0068 100.0\n",
            "299 0.0071 100.0\n",
            "349 0.0018 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0086, Accuracy: 1492/1559 (95.70%)\n",
            "\n",
            "--- 2.074708938598633 seconds ---\n",
            "Epoch: 8\n",
            "49 0.0046 100.0\n",
            "99 0.0047 100.0\n",
            "149 0.0023 100.0\n",
            "199 0.0018 100.0\n",
            "249 0.0043 100.0\n",
            "299 0.0039 100.0\n",
            "349 0.0035 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0088, Accuracy: 1489/1559 (95.51%)\n",
            "\n",
            "--- 1.9858975410461426 seconds ---\n",
            "Epoch: 9\n",
            "49 0.0032 100.0\n",
            "99 0.0019 100.0\n",
            "149 0.0016 100.0\n",
            "199 0.0007 100.0\n",
            "249 0.0027 100.0\n",
            "299 0.0026 100.0\n",
            "349 0.0023 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0087, Accuracy: 1492/1559 (95.70%)\n",
            "\n",
            "--- 2.0155081748962402 seconds ---\n",
            "Epoch: 10\n",
            "49 0.0046 100.0\n",
            "99 0.0024 100.0\n",
            "149 0.0013 100.0\n",
            "199 0.0005 100.0\n",
            "249 0.0018 100.0\n",
            "299 0.0044 100.0\n",
            "349 0.0027 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0084, Accuracy: 1491/1559 (95.64%)\n",
            "\n",
            "--- 1.9659390449523926 seconds ---\n"
          ]
        }
      ],
      "source": [
        "#isolet epoch 10 rffg32vsa\n",
        "class ArgumentParser:\n",
        "    def __init__(self):\n",
        "        self.args = {\n",
        "            'lr': 0.01,            # Learning rate for optimizing class representative\n",
        "            'gamma': 0.3,          # Kernel parameter for computing covariance\n",
        "            'epoch': 10,            # Epochs of training\n",
        "            'gorder': 32,           # Order of the cyclic group required for G-VSA (8 ou 16)\n",
        "            'dim': 10000,          # Dimension of hypervectors c le grand D\n",
        "            'seed': 43,            # Random seed for reproducing results\n",
        "            'resume': True,       # Resume from existing encoded hypervectors\n",
        "            'data_dir': './encoded_data', # Directory to save encoded data (hypervectors)\n",
        "            'dataset': 'isolet',    # Dataset choice choices=['mnist', 'fmnist', 'cifar', 'isolet', 'ucihar']\n",
        "            'raw_data_dir': './dataset',   # Raw data directory\n",
        "            'model': 'rff-gvsa'    # Model type choices=['rff-hdc', 'linear-hdc', 'rff-gvsa']\n",
        "        }\n",
        "        self.seed = self.args['seed']\n",
        "        self.lr = self.args['lr']\n",
        "        self.gamma = self.args['gamma']\n",
        "        self.epoch = self.args['epoch']\n",
        "        self.gorder = self.args['gorder']\n",
        "        self.dim = self.args['dim']\n",
        "        self.resume = self.args['resume']\n",
        "        self.data_dir = self.args['data_dir']\n",
        "        self.dataset = self.args['dataset']\n",
        "        self.raw_data_dir = self.args['raw_data_dir']\n",
        "        self.model = self.args['model']\n",
        "\n",
        "\n",
        "    def get_args(self):\n",
        "        return self.args\n",
        "args = ArgumentParser()\n",
        "torch.manual_seed(args.seed)\n",
        "np.random.seed(args.seed)\n",
        "if 'hdc' in args.model:\n",
        "    args.gorder = 2\n",
        "    print(\"Use binary HDC with random fourier features, ignoring gorder, set to 2.\")\n",
        "args.data_dir = f'{args.data_dir}/{args.dataset}_{args.model}_order{args.gorder}_gamma{args.gamma}_dim{args.dim}'\n",
        "try:\n",
        "    os.makedirs(args.data_dir)\n",
        "except FileExistsError:\n",
        "    print('Encoded data folder already exists')\n",
        "if not args.resume:\n",
        "    print('Encode the dataset into hypervectors and save')\n",
        "    encode_and_save(args)\n",
        "    print('Finish encoding and saving')\n",
        "print(f'Optimizing class representatives for {args.epoch} epochs')\n",
        "train(args)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## UCIHAR"
      ],
      "metadata": {
        "id": "ctruec8XZ-F7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec36f8ae-fd90-40bf-f34f-f5911b39f5e6",
        "id": "Bb3UWKTHZ-F8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encode the dataset into hypervectors and save\n",
            "# of channels of data 1\n",
            "# of training samples and test samples 7352 2947\n",
            "Encoding with random fourier features encoder.\n",
            "the threshold to discretize fourier features to group elements tensor([-1.8629, -1.5343, -1.3182, -1.1505, -1.0101, -0.8872, -0.7765, -0.6746,\n",
            "        -0.5792, -0.4888, -0.4023, -0.3187, -0.2372, -0.1573, -0.0784,  0.0000,\n",
            "         0.0784,  0.1573,  0.2372,  0.3187,  0.4023,  0.4888,  0.5792,  0.6746,\n",
            "         0.7765,  0.8872,  1.0101,  1.1505,  1.3182,  1.5343,  1.8629])\n",
            "Encoded pixels to hypervectors with size:  torch.Size([256, 10000])\n",
            "Encoding training data...\n",
            "Start encoding data\n",
            "Finish encoding data\n",
            "Encoding test data...\n",
            "Start encoding data\n",
            "Finish encoding data\n",
            "Finish encoding and saving\n",
            "Optimizing class representatives for 1 epochs\n",
            "Loading encoded training data...\n",
            "Loading encoded test data...\n",
            "Size of encoded training data torch.Size([7352, 10000]) and test data torch.Size([2947, 10000])\n",
            "Epoch: 1\n",
            "49 1.0779 37.5\n",
            "99 0.5147 87.5\n",
            "149 0.3429 93.75\n",
            "199 0.3301 93.75\n",
            "249 0.2540 87.5\n",
            "299 0.1947 100.0\n",
            "349 0.2053 87.5\n",
            "399 0.2215 100.0\n",
            "449 0.1441 93.75\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0129, Accuracy: 2760/2947 (93.65%)\n",
            "\n",
            "--- 2.5015745162963867 seconds ---\n"
          ]
        }
      ],
      "source": [
        "#ucihar epoch 1 rffg32vsa\n",
        "class ArgumentParser:\n",
        "    def __init__(self):\n",
        "        self.args = {\n",
        "            'lr': 0.01,            # Learning rate for optimizing class representative\n",
        "            'gamma': 0.3,          # Kernel parameter for computing covariance\n",
        "            'epoch': 1,            # Epochs of training\n",
        "            'gorder': 32,           # Order of the cyclic group required for G-VSA (8 ou 16)\n",
        "            'dim': 10000,          # Dimension of hypervectors c le grand D\n",
        "            'seed': 43,            # Random seed for reproducing results\n",
        "            'resume': False,       # Resume from existing encoded hypervectors\n",
        "            'data_dir': './encoded_data', # Directory to save encoded data (hypervectors)\n",
        "            'dataset': 'ucihar',    # Dataset choice choices=['mnist', 'fmnist', 'cifar', 'isolet', 'ucihar']\n",
        "            'raw_data_dir': './dataset',   # Raw data directory\n",
        "            'model': 'rff-gvsa'    # Model type choices=['rff-hdc', 'linear-hdc', 'rff-gvsa']\n",
        "        }\n",
        "        self.seed = self.args['seed']\n",
        "        self.lr = self.args['lr']\n",
        "        self.gamma = self.args['gamma']\n",
        "        self.epoch = self.args['epoch']\n",
        "        self.gorder = self.args['gorder']\n",
        "        self.dim = self.args['dim']\n",
        "        self.resume = self.args['resume']\n",
        "        self.data_dir = self.args['data_dir']\n",
        "        self.dataset = self.args['dataset']\n",
        "        self.raw_data_dir = self.args['raw_data_dir']\n",
        "        self.model = self.args['model']\n",
        "\n",
        "\n",
        "    def get_args(self):\n",
        "        return self.args\n",
        "args = ArgumentParser()\n",
        "torch.manual_seed(args.seed)\n",
        "np.random.seed(args.seed)\n",
        "if 'hdc' in args.model:\n",
        "    args.gorder = 2\n",
        "    print(\"Use binary HDC with random fourier features, ignoring gorder, set to 2.\")\n",
        "args.data_dir = f'{args.data_dir}/{args.dataset}_{args.model}_order{args.gorder}_gamma{args.gamma}_dim{args.dim}'\n",
        "try:\n",
        "    os.makedirs(args.data_dir)\n",
        "except FileExistsError:\n",
        "    print('Encoded data folder already exists')\n",
        "if not args.resume:\n",
        "    print('Encode the dataset into hypervectors and save')\n",
        "    encode_and_save(args)\n",
        "    print('Finish encoding and saving')\n",
        "print(f'Optimizing class representatives for {args.epoch} epochs')\n",
        "train(args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "567e9bf2-bc40-4747-90fa-981c4ea3fa40",
        "id": "bmpIbFm-Z-F8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoded data folder already exists\n",
            "Optimizing class representatives for 10 epochs\n",
            "Loading encoded training data...\n",
            "Loading encoded test data...\n",
            "Size of encoded training data torch.Size([7352, 10000]) and test data torch.Size([2947, 10000])\n",
            "Epoch: 1\n",
            "49 0.8994 75.0\n",
            "99 0.7113 68.75\n",
            "149 0.4762 75.0\n",
            "199 0.4215 81.25\n",
            "249 0.3189 81.25\n",
            "299 0.1786 93.75\n",
            "349 0.2867 87.5\n",
            "399 0.1463 100.0\n",
            "449 0.1418 93.75\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0135, Accuracy: 2729/2947 (92.60%)\n",
            "\n",
            "--- 2.8200371265411377 seconds ---\n",
            "Epoch: 2\n",
            "49 0.0385 100.0\n",
            "99 0.1366 93.75\n",
            "149 0.0526 100.0\n",
            "199 0.2466 75.0\n",
            "249 0.1228 100.0\n",
            "299 0.0685 100.0\n",
            "349 0.0413 100.0\n",
            "399 0.0222 100.0\n",
            "449 0.0215 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0089, Accuracy: 2812/2947 (95.42%)\n",
            "\n",
            "--- 3.1088883876800537 seconds ---\n",
            "Epoch: 3\n",
            "49 0.0480 100.0\n",
            "99 0.0493 100.0\n",
            "149 0.0146 100.0\n",
            "199 0.0875 93.75\n",
            "249 0.0265 100.0\n",
            "299 0.0223 100.0\n",
            "349 0.0974 100.0\n",
            "399 0.0353 100.0\n",
            "449 0.0721 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0118, Accuracy: 2731/2947 (92.67%)\n",
            "\n",
            "--- 2.5118722915649414 seconds ---\n",
            "Epoch: 4\n",
            "49 0.2028 81.25\n",
            "99 0.0869 100.0\n",
            "149 0.0288 100.0\n",
            "199 0.0218 100.0\n",
            "249 0.0116 100.0\n",
            "299 0.1927 93.75\n",
            "349 0.0228 100.0\n",
            "399 0.0052 100.0\n",
            "449 0.0337 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0077, Accuracy: 2817/2947 (95.59%)\n",
            "\n",
            "--- 2.5487868785858154 seconds ---\n",
            "Epoch: 5\n",
            "49 0.0113 100.0\n",
            "99 0.0217 100.0\n",
            "149 0.0030 100.0\n",
            "199 0.0075 100.0\n",
            "249 0.0719 100.0\n",
            "299 0.1872 87.5\n",
            "349 0.2031 93.75\n",
            "399 0.0386 100.0\n",
            "449 0.0426 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0083, Accuracy: 2802/2947 (95.08%)\n",
            "\n",
            "--- 2.5548086166381836 seconds ---\n",
            "Epoch: 6\n",
            "49 0.0212 100.0\n",
            "99 0.0024 100.0\n",
            "149 0.0037 100.0\n",
            "199 0.0121 100.0\n",
            "249 0.0260 100.0\n",
            "299 0.0567 93.75\n",
            "349 0.0042 100.0\n",
            "399 0.0146 100.0\n",
            "449 0.1395 93.75\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0105, Accuracy: 2748/2947 (93.25%)\n",
            "\n",
            "--- 3.0946435928344727 seconds ---\n",
            "Epoch: 7\n",
            "49 0.0081 100.0\n",
            "99 0.0067 100.0\n",
            "149 0.1259 93.75\n",
            "199 0.0314 100.0\n",
            "249 0.0302 100.0\n",
            "299 0.0176 100.0\n",
            "349 0.0007 100.0\n",
            "399 0.0008 100.0\n",
            "449 0.0002 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0076, Accuracy: 2819/2947 (95.66%)\n",
            "\n",
            "--- 2.6403651237487793 seconds ---\n",
            "Epoch: 8\n",
            "49 0.0130 100.0\n",
            "99 0.0021 100.0\n",
            "149 0.0155 100.0\n",
            "199 0.0071 100.0\n",
            "249 0.0039 100.0\n",
            "299 0.0015 100.0\n",
            "349 0.0001 100.0\n",
            "399 0.0257 100.0\n",
            "449 0.0010 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0069, Accuracy: 2833/2947 (96.13%)\n",
            "\n",
            "--- 2.5762062072753906 seconds ---\n",
            "Epoch: 9\n",
            "49 0.0147 100.0\n",
            "99 0.0369 100.0\n",
            "149 0.0102 100.0\n",
            "199 0.0804 93.75\n",
            "249 0.0067 100.0\n",
            "299 0.0029 100.0\n",
            "349 0.0047 100.0\n",
            "399 0.0128 100.0\n",
            "449 0.0047 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0083, Accuracy: 2804/2947 (95.15%)\n",
            "\n",
            "--- 2.5760338306427 seconds ---\n",
            "Epoch: 10\n",
            "49 0.0000 100.0\n",
            "99 0.0008 100.0\n",
            "149 0.0005 100.0\n",
            "199 0.0004 100.0\n",
            "249 0.0002 100.0\n",
            "299 0.0295 100.0\n",
            "349 0.0013 100.0\n",
            "399 0.0019 100.0\n",
            "449 0.0044 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0068, Accuracy: 2840/2947 (96.37%)\n",
            "\n",
            "--- 2.563004970550537 seconds ---\n"
          ]
        }
      ],
      "source": [
        "#ucihar epoch 10 rffg32vsa\n",
        "class ArgumentParser:\n",
        "    def __init__(self):\n",
        "        self.args = {\n",
        "            'lr': 0.01,            # Learning rate for optimizing class representative\n",
        "            'gamma': 0.3,          # Kernel parameter for computing covariance\n",
        "            'epoch': 10,            # Epochs of training\n",
        "            'gorder': 32,           # Order of the cyclic group required for G-VSA (8 ou 16)\n",
        "            'dim': 10000,          # Dimension of hypervectors c le grand D\n",
        "            'seed': 43,            # Random seed for reproducing results\n",
        "            'resume': True,       # Resume from existing encoded hypervectors\n",
        "            'data_dir': './encoded_data', # Directory to save encoded data (hypervectors)\n",
        "            'dataset': 'ucihar',    # Dataset choice choices=['mnist', 'fmnist', 'cifar', 'isolet', 'ucihar']\n",
        "            'raw_data_dir': './dataset',   # Raw data directory\n",
        "            'model': 'rff-gvsa'    # Model type choices=['rff-hdc', 'linear-hdc', 'rff-gvsa']\n",
        "        }\n",
        "        self.seed = self.args['seed']\n",
        "        self.lr = self.args['lr']\n",
        "        self.gamma = self.args['gamma']\n",
        "        self.epoch = self.args['epoch']\n",
        "        self.gorder = self.args['gorder']\n",
        "        self.dim = self.args['dim']\n",
        "        self.resume = self.args['resume']\n",
        "        self.data_dir = self.args['data_dir']\n",
        "        self.dataset = self.args['dataset']\n",
        "        self.raw_data_dir = self.args['raw_data_dir']\n",
        "        self.model = self.args['model']\n",
        "\n",
        "\n",
        "    def get_args(self):\n",
        "        return self.args\n",
        "args = ArgumentParser()\n",
        "torch.manual_seed(args.seed)\n",
        "np.random.seed(args.seed)\n",
        "if 'hdc' in args.model:\n",
        "    args.gorder = 2\n",
        "    print(\"Use binary HDC with random fourier features, ignoring gorder, set to 2.\")\n",
        "args.data_dir = f'{args.data_dir}/{args.dataset}_{args.model}_order{args.gorder}_gamma{args.gamma}_dim{args.dim}'\n",
        "try:\n",
        "    os.makedirs(args.data_dir)\n",
        "except FileExistsError:\n",
        "    print('Encoded data folder already exists')\n",
        "if not args.resume:\n",
        "    print('Encode the dataset into hypervectors and save')\n",
        "    encode_and_save(args)\n",
        "    print('Finish encoding and saving')\n",
        "print(f'Optimizing class representatives for {args.epoch} epochs')\n",
        "train(args)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MNIST"
      ],
      "metadata": {
        "id": "xWplsA37Z-F9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df905600-2dfe-46d5-ae68-ab0715f7939f",
        "id": "j0DtpLZQZ-F-"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encode the dataset into hypervectors and save\n",
            "# of channels of data 1\n",
            "# of training samples and test samples 60000 10000\n",
            "Encoding with random fourier features encoder.\n",
            "the threshold to discretize fourier features to group elements tensor([-1.8629, -1.5343, -1.3182, -1.1505, -1.0101, -0.8872, -0.7765, -0.6746,\n",
            "        -0.5792, -0.4888, -0.4023, -0.3187, -0.2372, -0.1573, -0.0784,  0.0000,\n",
            "         0.0784,  0.1573,  0.2372,  0.3187,  0.4023,  0.4888,  0.5792,  0.6746,\n",
            "         0.7765,  0.8872,  1.0101,  1.1505,  1.3182,  1.5343,  1.8629])\n",
            "Encoded pixels to hypervectors with size:  torch.Size([256, 10000])\n",
            "Encoding training data...\n",
            "Start encoding data\n",
            "12800 images encoded. Total time elapse = 22.589444875717163\n",
            "25600 images encoded. Total time elapse = 45.20040726661682\n",
            "38400 images encoded. Total time elapse = 69.20489263534546\n",
            "51200 images encoded. Total time elapse = 92.57166004180908\n",
            "Finish encoding data\n",
            "Encoding test data...\n",
            "Start encoding data\n",
            "Finish encoding data\n",
            "Finish encoding and saving\n",
            "Optimizing class representatives for 1 epochs\n",
            "Loading encoded training data...\n",
            "Loading encoded test data...\n",
            "Size of encoded training data torch.Size([60000, 10000]) and test data torch.Size([10000, 10000])\n",
            "Epoch: 1\n",
            "49 1.4081 87.5\n",
            "99 0.8706 75.0\n",
            "149 0.4009 93.75\n",
            "199 0.4519 81.25\n",
            "249 0.2758 93.75\n",
            "299 0.4463 81.25\n",
            "349 0.7735 81.25\n",
            "399 0.5191 93.75\n",
            "449 0.3930 87.5\n",
            "499 0.4776 81.25\n",
            "549 0.1946 93.75\n",
            "599 0.5245 81.25\n",
            "649 0.5724 81.25\n",
            "699 0.1823 93.75\n",
            "749 0.2095 93.75\n",
            "799 0.2380 93.75\n",
            "849 0.2088 93.75\n",
            "899 0.1049 100.0\n",
            "949 0.3231 93.75\n",
            "999 0.1020 100.0\n",
            "1049 0.4594 93.75\n",
            "1099 0.0130 100.0\n",
            "1149 0.2469 93.75\n",
            "1199 0.5545 81.25\n",
            "1249 0.1661 93.75\n",
            "1299 0.2315 93.75\n",
            "1349 0.1083 100.0\n",
            "1399 0.0352 100.0\n",
            "1449 0.1580 100.0\n",
            "1499 0.1082 100.0\n",
            "1549 0.0835 100.0\n",
            "1599 0.3624 93.75\n",
            "1649 0.1808 87.5\n",
            "1699 0.5608 87.5\n",
            "1749 0.1391 93.75\n",
            "1799 0.1537 93.75\n",
            "1849 0.2793 93.75\n",
            "1899 0.3405 87.5\n",
            "1949 0.4436 81.25\n",
            "1999 0.4065 87.5\n",
            "2049 0.3749 87.5\n",
            "2099 0.1494 93.75\n",
            "2149 0.0140 100.0\n",
            "2199 0.1082 93.75\n",
            "2249 0.0999 93.75\n",
            "2299 0.2695 87.5\n",
            "2349 0.0204 100.0\n",
            "2399 0.3574 87.5\n",
            "2449 0.1027 93.75\n",
            "2499 0.0177 100.0\n",
            "2549 0.1695 93.75\n",
            "2599 0.0195 100.0\n",
            "2649 0.3466 87.5\n",
            "2699 0.0979 93.75\n",
            "2749 0.1253 93.75\n",
            "2799 0.0830 100.0\n",
            "2849 0.3981 93.75\n",
            "2899 0.1703 93.75\n",
            "2949 0.0522 100.0\n",
            "2999 0.1273 100.0\n",
            "3049 0.0730 100.0\n",
            "3099 0.1193 93.75\n",
            "3149 0.0092 100.0\n",
            "3199 0.2666 93.75\n",
            "3249 0.1320 93.75\n",
            "3299 0.2355 93.75\n",
            "3349 0.3411 87.5\n",
            "3399 0.0644 100.0\n",
            "3449 0.1320 93.75\n",
            "3499 0.1883 93.75\n",
            "3549 0.0114 100.0\n",
            "3599 0.0218 100.0\n",
            "3649 0.1797 93.75\n",
            "3699 0.0953 100.0\n",
            "3749 0.0077 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0087, Accuracy: 9583/10000 (95.83%)\n",
            "\n",
            "--- 17.313066959381104 seconds ---\n"
          ]
        }
      ],
      "source": [
        "#mnist epoch 1 rffg32vsa\n",
        "class ArgumentParser:\n",
        "    def __init__(self):\n",
        "        self.args = {\n",
        "            'lr': 0.01,            # Learning rate for optimizing class representative\n",
        "            'gamma': 0.3,          # Kernel parameter for computing covariance\n",
        "            'epoch': 1,            # Epochs of training\n",
        "            'gorder': 32,           # Order of the cyclic group required for G-VSA (8 ou 16)\n",
        "            'dim': 10000,          # Dimension of hypervectors c le grand D\n",
        "            'seed': 43,            # Random seed for reproducing results\n",
        "            'resume': False,       # Resume from existing encoded hypervectors\n",
        "            'data_dir': './encoded_data', # Directory to save encoded data (hypervectors)\n",
        "            'dataset': 'mnist',    # Dataset choice choices=['mnist', 'fmnist', 'cifar', 'isolet', 'ucihar']\n",
        "            'raw_data_dir': './dataset',   # Raw data directory\n",
        "            'model': 'rff-gvsa'    # Model type choices=['rff-hdc', 'linear-hdc', 'rff-gvsa']\n",
        "        }\n",
        "        self.seed = self.args['seed']\n",
        "        self.lr = self.args['lr']\n",
        "        self.gamma = self.args['gamma']\n",
        "        self.epoch = self.args['epoch']\n",
        "        self.gorder = self.args['gorder']\n",
        "        self.dim = self.args['dim']\n",
        "        self.resume = self.args['resume']\n",
        "        self.data_dir = self.args['data_dir']\n",
        "        self.dataset = self.args['dataset']\n",
        "        self.raw_data_dir = self.args['raw_data_dir']\n",
        "        self.model = self.args['model']\n",
        "\n",
        "\n",
        "    def get_args(self):\n",
        "        return self.args\n",
        "args = ArgumentParser()\n",
        "torch.manual_seed(args.seed)\n",
        "np.random.seed(args.seed)\n",
        "if 'hdc' in args.model:\n",
        "    args.gorder = 2\n",
        "    print(\"Use binary HDC with random fourier features, ignoring gorder, set to 2.\")\n",
        "args.data_dir = f'{args.data_dir}/{args.dataset}_{args.model}_order{args.gorder}_gamma{args.gamma}_dim{args.dim}'\n",
        "try:\n",
        "    os.makedirs(args.data_dir)\n",
        "except FileExistsError:\n",
        "    print('Encoded data folder already exists')\n",
        "if not args.resume:\n",
        "    print('Encode the dataset into hypervectors and save')\n",
        "    encode_and_save(args)\n",
        "    print('Finish encoding and saving')\n",
        "print(f'Optimizing class representatives for {args.epoch} epochs')\n",
        "train(args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3bbb61fa-35f1-4ad4-e1d1-246efc924e86",
        "id": "Gq2QWaMnZ-F-"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoded data folder already exists\n",
            "Optimizing class representatives for 10 epochs\n",
            "Loading encoded training data...\n",
            "Loading encoded test data...\n",
            "Size of encoded training data torch.Size([60000, 10000]) and test data torch.Size([10000, 10000])\n",
            "Epoch: 1\n",
            "49 1.8620 43.75\n",
            "99 0.7994 81.25\n",
            "149 0.4383 93.75\n",
            "199 0.3310 100.0\n",
            "249 0.1780 100.0\n",
            "299 0.4666 81.25\n",
            "349 0.4385 75.0\n",
            "399 0.3052 93.75\n",
            "449 0.3609 81.25\n",
            "499 0.2869 87.5\n",
            "549 0.1467 100.0\n",
            "599 0.6299 75.0\n",
            "649 0.0974 100.0\n",
            "699 0.1343 100.0\n",
            "749 0.1033 93.75\n",
            "799 0.2795 87.5\n",
            "849 0.4467 87.5\n",
            "899 0.2145 93.75\n",
            "949 0.1088 100.0\n",
            "999 0.2838 93.75\n",
            "1049 0.0475 100.0\n",
            "1099 0.1929 93.75\n",
            "1149 0.0500 100.0\n",
            "1199 0.0797 100.0\n",
            "1249 0.1630 93.75\n",
            "1299 0.1221 93.75\n",
            "1349 0.0265 100.0\n",
            "1399 0.2546 87.5\n",
            "1449 0.0282 100.0\n",
            "1499 0.1408 87.5\n",
            "1549 0.0169 100.0\n",
            "1599 0.0303 100.0\n",
            "1649 0.1208 93.75\n",
            "1699 0.1225 93.75\n",
            "1749 0.0390 100.0\n",
            "1799 0.1322 93.75\n",
            "1849 0.0826 93.75\n",
            "1899 0.1964 93.75\n",
            "1949 0.1441 93.75\n",
            "1999 0.0732 100.0\n",
            "2049 0.0133 100.0\n",
            "2099 0.4554 93.75\n",
            "2149 0.2924 93.75\n",
            "2199 0.0212 100.0\n",
            "2249 0.4874 87.5\n",
            "2299 0.0736 93.75\n",
            "2349 0.0234 100.0\n",
            "2399 0.0942 93.75\n",
            "2449 0.0679 100.0\n",
            "2499 0.1331 93.75\n",
            "2549 0.0754 100.0\n",
            "2599 0.0806 100.0\n",
            "2649 0.0352 100.0\n",
            "2699 0.2413 93.75\n",
            "2749 0.1679 87.5\n",
            "2799 0.0156 100.0\n",
            "2849 0.1130 93.75\n",
            "2899 0.0508 93.75\n",
            "2949 0.2981 93.75\n",
            "2999 0.1088 93.75\n",
            "3049 0.0085 100.0\n",
            "3099 0.1284 93.75\n",
            "3149 0.1583 93.75\n",
            "3199 0.5195 87.5\n",
            "3249 0.0572 100.0\n",
            "3299 0.1284 100.0\n",
            "3349 0.1031 93.75\n",
            "3399 0.1245 93.75\n",
            "3449 0.0763 100.0\n",
            "3499 0.2156 93.75\n",
            "3549 0.0463 100.0\n",
            "3599 0.0340 100.0\n",
            "3649 0.2171 87.5\n",
            "3699 0.0984 100.0\n",
            "3749 0.0393 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0091, Accuracy: 9541/10000 (95.41%)\n",
            "\n",
            "--- 16.949431657791138 seconds ---\n",
            "Epoch: 2\n",
            "49 0.3078 81.25\n",
            "99 0.0367 100.0\n",
            "149 0.1391 93.75\n",
            "199 0.3663 93.75\n",
            "249 0.1107 93.75\n",
            "299 0.0865 100.0\n",
            "349 0.2376 93.75\n",
            "399 0.0258 100.0\n",
            "449 0.0560 100.0\n",
            "499 0.0421 100.0\n",
            "549 0.0192 100.0\n",
            "599 0.7471 87.5\n",
            "649 0.0316 100.0\n",
            "699 0.0337 100.0\n",
            "749 0.4905 93.75\n",
            "799 0.0539 100.0\n",
            "849 0.1783 93.75\n",
            "899 0.0555 100.0\n",
            "949 0.0070 100.0\n",
            "999 0.1460 93.75\n",
            "1049 0.0235 100.0\n",
            "1099 0.1144 93.75\n",
            "1149 0.1361 93.75\n",
            "1199 0.0190 100.0\n",
            "1249 0.1526 87.5\n",
            "1299 0.0692 93.75\n",
            "1349 0.0215 100.0\n",
            "1399 0.1739 93.75\n",
            "1449 0.6880 81.25\n",
            "1499 0.0034 100.0\n",
            "1549 0.0493 100.0\n",
            "1599 0.0266 100.0\n",
            "1649 0.0152 100.0\n",
            "1699 0.1965 93.75\n",
            "1749 0.0306 100.0\n",
            "1799 0.0210 100.0\n",
            "1849 0.0412 100.0\n",
            "1899 0.0251 100.0\n",
            "1949 0.0749 93.75\n",
            "1999 0.0115 100.0\n",
            "2049 0.0814 93.75\n",
            "2099 0.0088 100.0\n",
            "2149 0.1149 93.75\n",
            "2199 0.0011 100.0\n",
            "2249 0.0495 100.0\n",
            "2299 0.1153 100.0\n",
            "2349 0.0167 100.0\n",
            "2399 0.0359 100.0\n",
            "2449 0.0271 100.0\n",
            "2499 0.1731 93.75\n",
            "2549 0.0312 100.0\n",
            "2599 0.0640 100.0\n",
            "2649 0.0371 100.0\n",
            "2699 0.0248 100.0\n",
            "2749 0.0308 100.0\n",
            "2799 0.0249 100.0\n",
            "2849 0.0948 93.75\n",
            "2899 0.1756 93.75\n",
            "2949 0.0280 100.0\n",
            "2999 0.0695 93.75\n",
            "3049 0.2402 93.75\n",
            "3099 0.0464 100.0\n",
            "3149 0.1274 93.75\n",
            "3199 0.4381 93.75\n",
            "3249 0.0287 100.0\n",
            "3299 0.0455 100.0\n",
            "3349 0.0023 100.0\n",
            "3399 0.1519 93.75\n",
            "3449 0.0481 100.0\n",
            "3499 0.0104 100.0\n",
            "3549 0.1249 93.75\n",
            "3599 0.0010 100.0\n",
            "3649 0.0011 100.0\n",
            "3699 0.0551 93.75\n",
            "3749 0.0280 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0073, Accuracy: 9650/10000 (96.50%)\n",
            "\n",
            "--- 16.919917821884155 seconds ---\n",
            "Epoch: 3\n",
            "49 0.2622 93.75\n",
            "99 0.1184 93.75\n",
            "149 0.0237 100.0\n",
            "199 0.0123 100.0\n",
            "249 0.0185 100.0\n",
            "299 0.0600 100.0\n",
            "349 0.1864 93.75\n",
            "399 0.0051 100.0\n",
            "449 0.0085 100.0\n",
            "499 0.0751 100.0\n",
            "549 0.0251 100.0\n",
            "599 0.0375 100.0\n",
            "649 0.0139 100.0\n",
            "699 0.0532 100.0\n",
            "749 0.0230 100.0\n",
            "799 0.1040 93.75\n",
            "849 0.0904 93.75\n",
            "899 0.0011 100.0\n",
            "949 0.0047 100.0\n",
            "999 0.0856 100.0\n",
            "1049 0.0861 93.75\n",
            "1099 0.0189 100.0\n",
            "1149 0.0057 100.0\n",
            "1199 0.0877 93.75\n",
            "1249 0.4184 93.75\n",
            "1299 0.0111 100.0\n",
            "1349 0.2165 93.75\n",
            "1399 0.1642 93.75\n",
            "1449 0.0815 100.0\n",
            "1499 0.1767 93.75\n",
            "1549 0.0382 100.0\n",
            "1599 0.0123 100.0\n",
            "1649 0.0420 100.0\n",
            "1699 0.0209 100.0\n",
            "1749 0.1521 93.75\n",
            "1799 0.3478 87.5\n",
            "1849 0.1906 93.75\n",
            "1899 0.0245 100.0\n",
            "1949 0.0036 100.0\n",
            "1999 0.0207 100.0\n",
            "2049 0.0016 100.0\n",
            "2099 0.0140 100.0\n",
            "2149 0.2732 93.75\n",
            "2199 0.0381 100.0\n",
            "2249 0.0002 100.0\n",
            "2299 0.0302 100.0\n",
            "2349 0.0437 100.0\n",
            "2399 0.0063 100.0\n",
            "2449 0.0023 100.0\n",
            "2499 0.1965 93.75\n",
            "2549 0.0053 100.0\n",
            "2599 0.2098 93.75\n",
            "2649 0.0269 100.0\n",
            "2699 0.0954 93.75\n",
            "2749 0.0274 100.0\n",
            "2799 0.0029 100.0\n",
            "2849 0.1274 93.75\n",
            "2899 0.0023 100.0\n",
            "2949 0.0049 100.0\n",
            "2999 0.1997 93.75\n",
            "3049 0.0203 100.0\n",
            "3099 0.0373 100.0\n",
            "3149 0.0052 100.0\n",
            "3199 0.1268 93.75\n",
            "3249 0.0026 100.0\n",
            "3299 0.0281 100.0\n",
            "3349 0.0054 100.0\n",
            "3399 0.0045 100.0\n",
            "3449 0.2034 93.75\n",
            "3499 0.0348 100.0\n",
            "3549 0.0165 100.0\n",
            "3599 0.0071 100.0\n",
            "3649 0.0034 100.0\n",
            "3699 0.0022 100.0\n",
            "3749 0.0291 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0071, Accuracy: 9663/10000 (96.63%)\n",
            "\n",
            "--- 17.821156978607178 seconds ---\n",
            "Epoch: 4\n",
            "49 0.0968 93.75\n",
            "99 0.0014 100.0\n",
            "149 0.0338 100.0\n",
            "199 0.0133 100.0\n",
            "249 0.0097 100.0\n",
            "299 0.0604 100.0\n",
            "349 0.0005 100.0\n",
            "399 0.0114 100.0\n",
            "449 0.0061 100.0\n",
            "499 0.0277 100.0\n",
            "549 0.0081 100.0\n",
            "599 0.1604 93.75\n",
            "649 0.0001 100.0\n",
            "699 0.2256 93.75\n",
            "749 0.2027 93.75\n",
            "799 0.0012 100.0\n",
            "849 0.0118 100.0\n",
            "899 0.0015 100.0\n",
            "949 0.0714 100.0\n",
            "999 0.0430 100.0\n",
            "1049 0.0023 100.0\n",
            "1099 0.0004 100.0\n",
            "1149 0.2476 93.75\n",
            "1199 0.0058 100.0\n",
            "1249 0.1030 93.75\n",
            "1299 0.0706 93.75\n",
            "1349 0.0030 100.0\n",
            "1399 0.0071 100.0\n",
            "1449 0.0003 100.0\n",
            "1499 0.0013 100.0\n",
            "1549 0.0380 100.0\n",
            "1599 0.0066 100.0\n",
            "1649 0.0024 100.0\n",
            "1699 0.0002 100.0\n",
            "1749 0.2817 93.75\n",
            "1799 0.0038 100.0\n",
            "1849 0.0044 100.0\n",
            "1899 0.0049 100.0\n",
            "1949 0.0089 100.0\n",
            "1999 0.0019 100.0\n",
            "2049 0.0149 100.0\n",
            "2099 0.0447 100.0\n",
            "2149 0.0003 100.0\n",
            "2199 0.0094 100.0\n",
            "2249 0.0125 100.0\n",
            "2299 0.0097 100.0\n",
            "2349 0.0387 100.0\n",
            "2399 0.0151 100.0\n",
            "2449 0.0199 100.0\n",
            "2499 0.3285 93.75\n",
            "2549 0.0017 100.0\n",
            "2599 0.0380 100.0\n",
            "2649 0.0034 100.0\n",
            "2699 0.0199 100.0\n",
            "2749 0.5057 93.75\n",
            "2799 0.3471 87.5\n",
            "2849 0.0015 100.0\n",
            "2899 0.0004 100.0\n",
            "2949 0.4746 93.75\n",
            "2999 0.0018 100.0\n",
            "3049 0.0011 100.0\n",
            "3099 0.3095 93.75\n",
            "3149 0.0732 100.0\n",
            "3199 0.0004 100.0\n",
            "3249 0.0201 100.0\n",
            "3299 0.0138 100.0\n",
            "3349 0.0414 100.0\n",
            "3399 0.0000 100.0\n",
            "3449 0.0260 100.0\n",
            "3499 0.0191 100.0\n",
            "3549 0.0112 100.0\n",
            "3599 0.0074 100.0\n",
            "3649 0.0077 100.0\n",
            "3699 0.0042 100.0\n",
            "3749 0.0186 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0076, Accuracy: 9692/10000 (96.92%)\n",
            "\n",
            "--- 17.16023564338684 seconds ---\n",
            "Epoch: 5\n",
            "49 0.0059 100.0\n",
            "99 0.0049 100.0\n",
            "149 0.0070 100.0\n",
            "199 0.0021 100.0\n",
            "249 0.0024 100.0\n",
            "299 0.0007 100.0\n",
            "349 0.0331 100.0\n",
            "399 0.0171 100.0\n",
            "449 0.0012 100.0\n",
            "499 0.0009 100.0\n",
            "549 0.0985 93.75\n",
            "599 0.0089 100.0\n",
            "649 0.0014 100.0\n",
            "699 0.0289 100.0\n",
            "749 0.0034 100.0\n",
            "799 0.0006 100.0\n",
            "849 0.0190 100.0\n",
            "899 0.0109 100.0\n",
            "949 0.0003 100.0\n",
            "999 0.0139 100.0\n",
            "1049 0.1108 93.75\n",
            "1099 0.0038 100.0\n",
            "1149 0.0006 100.0\n",
            "1199 0.1836 93.75\n",
            "1249 0.0013 100.0\n",
            "1299 0.0043 100.0\n",
            "1349 0.0395 100.0\n",
            "1399 0.0020 100.0\n",
            "1449 0.0008 100.0\n",
            "1499 0.0022 100.0\n",
            "1549 0.0120 100.0\n",
            "1599 0.0432 100.0\n",
            "1649 0.0027 100.0\n",
            "1699 0.0008 100.0\n",
            "1749 0.0466 100.0\n",
            "1799 0.0248 100.0\n",
            "1849 0.0005 100.0\n",
            "1899 0.0268 100.0\n",
            "1949 0.0000 100.0\n",
            "1999 0.0046 100.0\n",
            "2049 0.0011 100.0\n",
            "2099 0.0027 100.0\n",
            "2149 0.0006 100.0\n",
            "2199 0.0013 100.0\n",
            "2249 0.0232 100.0\n",
            "2299 0.0039 100.0\n",
            "2349 0.0000 100.0\n",
            "2399 0.0007 100.0\n",
            "2449 0.0180 100.0\n",
            "2499 0.0010 100.0\n",
            "2549 0.0036 100.0\n",
            "2599 0.0212 100.0\n",
            "2649 0.0005 100.0\n",
            "2699 0.0305 100.0\n",
            "2749 0.0199 100.0\n",
            "2799 0.0011 100.0\n",
            "2849 0.1553 93.75\n",
            "2899 0.0009 100.0\n",
            "2949 0.0046 100.0\n",
            "2999 0.0003 100.0\n",
            "3049 0.0014 100.0\n",
            "3099 0.0003 100.0\n",
            "3149 0.0644 93.75\n",
            "3199 0.0005 100.0\n",
            "3249 0.0034 100.0\n",
            "3299 0.0109 100.0\n",
            "3349 0.0071 100.0\n",
            "3399 0.0170 100.0\n",
            "3449 0.0000 100.0\n",
            "3499 0.0004 100.0\n",
            "3549 0.0106 100.0\n",
            "3599 0.0005 100.0\n",
            "3649 0.0204 100.0\n",
            "3699 0.0002 100.0\n",
            "3749 0.0004 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0084, Accuracy: 9634/10000 (96.34%)\n",
            "\n",
            "--- 17.024815559387207 seconds ---\n",
            "Epoch: 6\n",
            "49 0.0038 100.0\n",
            "99 0.1639 93.75\n",
            "149 0.0347 100.0\n",
            "199 0.0020 100.0\n",
            "249 0.0033 100.0\n",
            "299 0.0002 100.0\n",
            "349 0.0045 100.0\n",
            "399 0.0075 100.0\n",
            "449 0.0022 100.0\n",
            "499 0.0148 100.0\n",
            "549 0.0046 100.0\n",
            "599 0.0011 100.0\n",
            "649 0.0076 100.0\n",
            "699 0.0140 100.0\n",
            "749 0.0005 100.0\n",
            "799 0.0044 100.0\n",
            "849 0.0001 100.0\n",
            "899 0.0191 100.0\n",
            "949 0.0150 100.0\n",
            "999 0.0003 100.0\n",
            "1049 0.0001 100.0\n",
            "1099 0.0000 100.0\n",
            "1149 0.0010 100.0\n",
            "1199 0.0001 100.0\n",
            "1249 0.0003 100.0\n",
            "1299 0.0060 100.0\n",
            "1349 0.0046 100.0\n",
            "1399 0.0651 93.75\n",
            "1449 0.1098 93.75\n",
            "1499 0.0881 93.75\n",
            "1549 0.0062 100.0\n",
            "1599 0.0009 100.0\n",
            "1649 0.0102 100.0\n",
            "1699 0.0621 93.75\n",
            "1749 0.0008 100.0\n",
            "1799 0.0000 100.0\n",
            "1849 0.0001 100.0\n",
            "1899 0.0291 100.0\n",
            "1949 0.0002 100.0\n",
            "1999 0.0260 100.0\n",
            "2049 0.0017 100.0\n",
            "2099 0.0000 100.0\n",
            "2149 0.0042 100.0\n",
            "2199 0.0000 100.0\n",
            "2249 0.0048 100.0\n",
            "2299 0.0015 100.0\n",
            "2349 0.0000 100.0\n",
            "2399 0.2493 87.5\n",
            "2449 0.0027 100.0\n",
            "2499 0.0081 100.0\n",
            "2549 0.0039 100.0\n",
            "2599 0.0096 100.0\n",
            "2649 0.0001 100.0\n",
            "2699 0.1464 93.75\n",
            "2749 0.0013 100.0\n",
            "2799 0.0000 100.0\n",
            "2849 0.0001 100.0\n",
            "2899 0.0494 100.0\n",
            "2949 0.0149 100.0\n",
            "2999 0.0000 100.0\n",
            "3049 0.0026 100.0\n",
            "3099 0.0041 100.0\n",
            "3149 0.0005 100.0\n",
            "3199 0.0271 100.0\n",
            "3249 0.0201 100.0\n",
            "3299 0.0005 100.0\n",
            "3349 0.0848 93.75\n",
            "3399 0.0006 100.0\n",
            "3449 0.0004 100.0\n",
            "3499 0.0002 100.0\n",
            "3549 0.0042 100.0\n",
            "3599 0.0234 100.0\n",
            "3649 0.0002 100.0\n",
            "3699 0.0233 100.0\n",
            "3749 0.0308 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0090, Accuracy: 9690/10000 (96.90%)\n",
            "\n",
            "--- 18.00170135498047 seconds ---\n",
            "Epoch: 7\n",
            "49 0.0057 100.0\n",
            "99 0.0638 93.75\n",
            "149 0.0041 100.0\n",
            "199 0.0002 100.0\n",
            "249 0.0000 100.0\n",
            "299 0.0019 100.0\n",
            "349 0.0001 100.0\n",
            "399 0.0065 100.0\n",
            "449 0.0000 100.0\n",
            "499 0.0003 100.0\n",
            "549 0.0012 100.0\n",
            "599 0.0006 100.0\n",
            "649 0.0008 100.0\n",
            "699 0.0000 100.0\n",
            "749 0.0010 100.0\n",
            "799 0.0012 100.0\n",
            "849 0.0070 100.0\n",
            "899 0.0001 100.0\n",
            "949 0.0016 100.0\n",
            "999 0.0305 100.0\n",
            "1049 0.0029 100.0\n",
            "1099 0.0092 100.0\n",
            "1149 0.0199 100.0\n",
            "1199 0.0014 100.0\n",
            "1249 0.0092 100.0\n",
            "1299 0.0103 100.0\n",
            "1349 0.0041 100.0\n",
            "1399 0.0389 100.0\n",
            "1449 0.0002 100.0\n",
            "1499 0.0316 100.0\n",
            "1549 0.0002 100.0\n",
            "1599 0.0022 100.0\n",
            "1649 0.0000 100.0\n",
            "1699 0.0000 100.0\n",
            "1749 0.0206 100.0\n",
            "1799 0.0008 100.0\n",
            "1849 0.0008 100.0\n",
            "1899 0.0109 100.0\n",
            "1949 0.0009 100.0\n",
            "1999 0.0091 100.0\n",
            "2049 0.0028 100.0\n",
            "2099 0.0000 100.0\n",
            "2149 0.1114 93.75\n",
            "2199 0.0004 100.0\n",
            "2249 0.0016 100.0\n",
            "2299 0.0025 100.0\n",
            "2349 0.0000 100.0\n",
            "2399 0.0112 100.0\n",
            "2449 0.0047 100.0\n",
            "2499 0.0001 100.0\n",
            "2549 0.0000 100.0\n",
            "2599 0.0000 100.0\n",
            "2649 0.0000 100.0\n",
            "2699 0.0003 100.0\n",
            "2749 0.0005 100.0\n",
            "2799 0.0011 100.0\n",
            "2849 0.0069 100.0\n",
            "2899 0.0004 100.0\n",
            "2949 0.0081 100.0\n",
            "2999 0.0001 100.0\n",
            "3049 0.0001 100.0\n",
            "3099 0.0122 100.0\n",
            "3149 0.0204 100.0\n",
            "3199 0.1092 93.75\n",
            "3249 0.0029 100.0\n",
            "3299 0.0000 100.0\n",
            "3349 0.1965 87.5\n",
            "3399 0.0042 100.0\n",
            "3449 0.0004 100.0\n",
            "3499 0.0000 100.0\n",
            "3549 0.0024 100.0\n",
            "3599 0.0033 100.0\n",
            "3649 0.2012 87.5\n",
            "3699 0.0233 100.0\n",
            "3749 0.0010 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0097, Accuracy: 9679/10000 (96.79%)\n",
            "\n",
            "--- 17.23363447189331 seconds ---\n",
            "Epoch: 8\n",
            "49 0.0000 100.0\n",
            "99 0.0015 100.0\n",
            "149 0.0005 100.0\n",
            "199 0.0134 100.0\n",
            "249 0.0200 100.0\n",
            "299 0.0001 100.0\n",
            "349 0.0001 100.0\n",
            "399 0.0023 100.0\n",
            "449 0.0001 100.0\n",
            "499 0.0154 100.0\n",
            "549 0.0008 100.0\n",
            "599 0.0001 100.0\n",
            "649 0.0005 100.0\n",
            "699 0.0010 100.0\n",
            "749 0.0003 100.0\n",
            "799 0.0067 100.0\n",
            "849 0.0002 100.0\n",
            "899 0.0022 100.0\n",
            "949 0.0001 100.0\n",
            "999 0.0125 100.0\n",
            "1049 0.0035 100.0\n",
            "1099 0.0002 100.0\n",
            "1149 0.0000 100.0\n",
            "1199 0.0005 100.0\n",
            "1249 0.0002 100.0\n",
            "1299 0.0637 93.75\n",
            "1349 0.0041 100.0\n",
            "1399 0.0000 100.0\n",
            "1449 0.0068 100.0\n",
            "1499 0.0000 100.0\n",
            "1549 0.0323 100.0\n",
            "1599 0.0019 100.0\n",
            "1649 0.0501 93.75\n",
            "1699 0.0002 100.0\n",
            "1749 0.0000 100.0\n",
            "1799 0.0095 100.0\n",
            "1849 0.0022 100.0\n",
            "1899 0.0007 100.0\n",
            "1949 0.0021 100.0\n",
            "1999 0.0000 100.0\n",
            "2049 0.0003 100.0\n",
            "2099 0.0004 100.0\n",
            "2149 0.0009 100.0\n",
            "2199 0.0074 100.0\n",
            "2249 0.0257 100.0\n",
            "2299 0.0009 100.0\n",
            "2349 0.0013 100.0\n",
            "2399 0.0000 100.0\n",
            "2449 0.0073 100.0\n",
            "2499 0.0014 100.0\n",
            "2549 0.0028 100.0\n",
            "2599 0.0044 100.0\n",
            "2649 0.0152 100.0\n",
            "2699 0.0032 100.0\n",
            "2749 0.0268 100.0\n",
            "2799 0.0012 100.0\n",
            "2849 0.0185 100.0\n",
            "2899 0.0002 100.0\n",
            "2949 0.0005 100.0\n",
            "2999 0.0002 100.0\n",
            "3049 0.0000 100.0\n",
            "3099 0.0024 100.0\n",
            "3149 0.0010 100.0\n",
            "3199 0.0001 100.0\n",
            "3249 0.0000 100.0\n",
            "3299 0.0744 93.75\n",
            "3349 0.0002 100.0\n",
            "3399 0.0071 100.0\n",
            "3449 0.0023 100.0\n",
            "3499 0.0035 100.0\n",
            "3549 0.0003 100.0\n",
            "3599 0.0001 100.0\n",
            "3649 0.0005 100.0\n",
            "3699 0.0007 100.0\n",
            "3749 0.1064 93.75\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0092, Accuracy: 9701/10000 (97.01%)\n",
            "\n",
            "--- 17.240419387817383 seconds ---\n",
            "Epoch: 9\n",
            "49 0.0036 100.0\n",
            "99 0.0001 100.0\n",
            "149 0.0000 100.0\n",
            "199 0.0005 100.0\n",
            "249 0.0128 100.0\n",
            "299 0.0004 100.0\n",
            "349 0.0000 100.0\n",
            "399 0.0171 100.0\n",
            "449 0.0008 100.0\n",
            "499 0.0006 100.0\n",
            "549 0.0004 100.0\n",
            "599 0.0037 100.0\n",
            "649 0.0007 100.0\n",
            "699 0.0000 100.0\n",
            "749 0.0424 100.0\n",
            "799 0.0008 100.0\n",
            "849 0.0004 100.0\n",
            "899 0.0001 100.0\n",
            "949 0.0065 100.0\n",
            "999 0.2330 93.75\n",
            "1049 0.0000 100.0\n",
            "1099 0.0005 100.0\n",
            "1149 0.0000 100.0\n",
            "1199 0.0006 100.0\n",
            "1249 0.0003 100.0\n",
            "1299 0.0000 100.0\n",
            "1349 0.0000 100.0\n",
            "1399 0.0030 100.0\n",
            "1449 0.0003 100.0\n",
            "1499 0.0017 100.0\n",
            "1549 0.0024 100.0\n",
            "1599 0.0051 100.0\n",
            "1649 0.0001 100.0\n",
            "1699 0.0031 100.0\n",
            "1749 0.0017 100.0\n",
            "1799 0.0011 100.0\n",
            "1849 0.0003 100.0\n",
            "1899 0.0002 100.0\n",
            "1949 0.0001 100.0\n",
            "1999 0.0002 100.0\n",
            "2049 0.0000 100.0\n",
            "2099 0.0043 100.0\n",
            "2149 0.0000 100.0\n",
            "2199 0.0144 100.0\n",
            "2249 0.0001 100.0\n",
            "2299 0.0002 100.0\n",
            "2349 0.0002 100.0\n",
            "2399 0.0001 100.0\n",
            "2449 0.0002 100.0\n",
            "2499 0.0013 100.0\n",
            "2549 0.0058 100.0\n",
            "2599 0.0000 100.0\n",
            "2649 0.0006 100.0\n",
            "2699 0.0069 100.0\n",
            "2749 0.0008 100.0\n",
            "2799 0.0015 100.0\n",
            "2849 0.0008 100.0\n",
            "2899 0.0026 100.0\n",
            "2949 0.0057 100.0\n",
            "2999 0.0000 100.0\n",
            "3049 0.0776 93.75\n",
            "3099 0.0001 100.0\n",
            "3149 0.2865 93.75\n",
            "3199 0.0021 100.0\n",
            "3249 0.1303 93.75\n",
            "3299 0.0377 100.0\n",
            "3349 0.1128 93.75\n",
            "3399 0.0036 100.0\n",
            "3449 0.0000 100.0\n",
            "3499 0.0000 100.0\n",
            "3549 0.0406 100.0\n",
            "3599 0.0189 100.0\n",
            "3649 0.0457 93.75\n",
            "3699 0.0001 100.0\n",
            "3749 0.0002 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0115, Accuracy: 9641/10000 (96.41%)\n",
            "\n",
            "--- 18.149298429489136 seconds ---\n",
            "Epoch: 10\n",
            "49 0.0034 100.0\n",
            "99 0.0003 100.0\n",
            "149 0.0001 100.0\n",
            "199 0.0003 100.0\n",
            "249 0.0013 100.0\n",
            "299 0.0007 100.0\n",
            "349 0.0006 100.0\n",
            "399 0.0001 100.0\n",
            "449 0.0002 100.0\n",
            "499 0.0049 100.0\n",
            "549 0.0003 100.0\n",
            "599 0.0003 100.0\n",
            "649 0.0008 100.0\n",
            "699 0.0568 93.75\n",
            "749 0.0000 100.0\n",
            "799 0.0001 100.0\n",
            "849 0.0004 100.0\n",
            "899 0.0000 100.0\n",
            "949 0.0000 100.0\n",
            "999 0.0073 100.0\n",
            "1049 0.0000 100.0\n",
            "1099 0.0215 100.0\n",
            "1149 0.0013 100.0\n",
            "1199 0.0030 100.0\n",
            "1249 0.0027 100.0\n",
            "1299 0.0000 100.0\n",
            "1349 0.0005 100.0\n",
            "1399 0.0406 100.0\n",
            "1449 0.0006 100.0\n",
            "1499 0.0020 100.0\n",
            "1549 0.2282 93.75\n",
            "1599 0.0014 100.0\n",
            "1649 0.0127 100.0\n",
            "1699 0.0020 100.0\n",
            "1749 0.0003 100.0\n",
            "1799 0.0024 100.0\n",
            "1849 0.0000 100.0\n",
            "1899 0.0544 93.75\n",
            "1949 0.0060 100.0\n",
            "1999 0.0000 100.0\n",
            "2049 0.0001 100.0\n",
            "2099 0.0002 100.0\n",
            "2149 0.0134 100.0\n",
            "2199 0.0000 100.0\n",
            "2249 0.0000 100.0\n",
            "2299 0.0003 100.0\n",
            "2349 0.0058 100.0\n",
            "2399 0.0000 100.0\n",
            "2449 0.0001 100.0\n",
            "2499 0.0000 100.0\n",
            "2549 0.0000 100.0\n",
            "2599 0.0109 100.0\n",
            "2649 0.0000 100.0\n",
            "2699 0.0346 100.0\n",
            "2749 0.0000 100.0\n",
            "2799 0.0001 100.0\n",
            "2849 0.0059 100.0\n",
            "2899 0.0000 100.0\n",
            "2949 0.0000 100.0\n",
            "2999 0.0055 100.0\n",
            "3049 0.0105 100.0\n",
            "3099 0.0174 100.0\n",
            "3149 0.0000 100.0\n",
            "3199 0.0002 100.0\n",
            "3249 0.0220 100.0\n",
            "3299 0.0000 100.0\n",
            "3349 0.0324 100.0\n",
            "3399 0.0093 100.0\n",
            "3449 0.0000 100.0\n",
            "3499 0.0340 100.0\n",
            "3549 0.0000 100.0\n",
            "3599 0.0067 100.0\n",
            "3649 0.0038 100.0\n",
            "3699 0.0001 100.0\n",
            "3749 0.0056 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0102, Accuracy: 9691/10000 (96.91%)\n",
            "\n",
            "--- 17.454207181930542 seconds ---\n"
          ]
        }
      ],
      "source": [
        "#mnist epoch 10 rffg32vsa\n",
        "class ArgumentParser:\n",
        "    def __init__(self):\n",
        "        self.args = {\n",
        "            'lr': 0.01,            # Learning rate for optimizing class representative\n",
        "            'gamma': 0.3,          # Kernel parameter for computing covariance\n",
        "            'epoch': 10,            # Epochs of training\n",
        "            'gorder': 32,           # Order of the cyclic group required for G-VSA (8 ou 16)\n",
        "            'dim': 10000,          # Dimension of hypervectors c le grand D\n",
        "            'seed': 43,            # Random seed for reproducing results\n",
        "            'resume': True,       # Resume from existing encoded hypervectors\n",
        "            'data_dir': './encoded_data', # Directory to save encoded data (hypervectors)\n",
        "            'dataset': 'mnist',    # Dataset choice choices=['mnist', 'fmnist', 'cifar', 'isolet', 'ucihar']\n",
        "            'raw_data_dir': './dataset',   # Raw data directory\n",
        "            'model': 'rff-gvsa'    # Model type choices=['rff-hdc', 'linear-hdc', 'rff-gvsa']\n",
        "        }\n",
        "        self.seed = self.args['seed']\n",
        "        self.lr = self.args['lr']\n",
        "        self.gamma = self.args['gamma']\n",
        "        self.epoch = self.args['epoch']\n",
        "        self.gorder = self.args['gorder']\n",
        "        self.dim = self.args['dim']\n",
        "        self.resume = self.args['resume']\n",
        "        self.data_dir = self.args['data_dir']\n",
        "        self.dataset = self.args['dataset']\n",
        "        self.raw_data_dir = self.args['raw_data_dir']\n",
        "        self.model = self.args['model']\n",
        "\n",
        "\n",
        "    def get_args(self):\n",
        "        return self.args\n",
        "args = ArgumentParser()\n",
        "torch.manual_seed(args.seed)\n",
        "np.random.seed(args.seed)\n",
        "if 'hdc' in args.model:\n",
        "    args.gorder = 2\n",
        "    print(\"Use binary HDC with random fourier features, ignoring gorder, set to 2.\")\n",
        "args.data_dir = f'{args.data_dir}/{args.dataset}_{args.model}_order{args.gorder}_gamma{args.gamma}_dim{args.dim}'\n",
        "try:\n",
        "    os.makedirs(args.data_dir)\n",
        "except FileExistsError:\n",
        "    print('Encoded data folder already exists')\n",
        "if not args.resume:\n",
        "    print('Encode the dataset into hypervectors and save')\n",
        "    encode_and_save(args)\n",
        "    print('Finish encoding and saving')\n",
        "print(f'Optimizing class representatives for {args.epoch} epochs')\n",
        "train(args)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FMNIST"
      ],
      "metadata": {
        "id": "h2FVnN8bZ-F_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#fminst epoch 1 rffg32vsa\n",
        "class ArgumentParser:\n",
        "    def __init__(self):\n",
        "        self.args = {\n",
        "            'lr': 0.01,            # Learning rate for optimizing class representative\n",
        "            'gamma': 0.3,          # Kernel parameter for computing covariance\n",
        "            'epoch': 1,            # Epochs of training\n",
        "            'gorder': 32,           # Order of the cyclic group required for G-VSA (8 ou 16)\n",
        "            'dim': 10000,          # Dimension of hypervectors c le grand D\n",
        "            'seed': 43,            # Random seed for reproducing results\n",
        "            'resume': False,       # Resume from existing encoded hypervectors\n",
        "            'data_dir': './encoded_data', # Directory to save encoded data (hypervectors)\n",
        "            'dataset': 'fmnist',    # Dataset choice choices=['mnist', 'fmnist', 'cifar', 'isolet', 'ucihar']\n",
        "            'raw_data_dir': './dataset',   # Raw data directory\n",
        "            'model': 'rff-gvsa'    # Model type choices=['rff-hdc', 'linear-hdc', 'rff-gvsa']\n",
        "        }\n",
        "        self.seed = self.args['seed']\n",
        "        self.lr = self.args['lr']\n",
        "        self.gamma = self.args['gamma']\n",
        "        self.epoch = self.args['epoch']\n",
        "        self.gorder = self.args['gorder']\n",
        "        self.dim = self.args['dim']\n",
        "        self.resume = self.args['resume']\n",
        "        self.data_dir = self.args['data_dir']\n",
        "        self.dataset = self.args['dataset']\n",
        "        self.raw_data_dir = self.args['raw_data_dir']\n",
        "        self.model = self.args['model']\n",
        "\n",
        "\n",
        "    def get_args(self):\n",
        "        return self.args\n",
        "\n",
        "args = ArgumentParser()\n",
        "torch.manual_seed(args.seed)\n",
        "np.random.seed(args.seed)\n",
        "if 'hdc' in args.model:\n",
        "    args.gorder = 2\n",
        "    print(\"Use binary HDC with random fourier features, ignoring gorder, set to 2.\")\n",
        "args.data_dir = f'{args.data_dir}/{args.dataset}_{args.model}_order{args.gorder}_gamma{args.gamma}_dim{args.dim}'\n",
        "try:\n",
        "    os.makedirs(args.data_dir)\n",
        "except FileExistsError:\n",
        "    print('Encoded data folder already exists')\n",
        "if not args.resume:\n",
        "    print('Encode the dataset into hypervectors and save')\n",
        "    encode_and_save(args)\n",
        "    print('Finish encoding and saving')\n",
        "print(f'Optimizing class representatives for {args.epoch} epochs')\n",
        "train(args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05be452b-233b-4668-945c-ce84ebf457f2",
        "id": "pzPJthh4Z-F_"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encode the dataset into hypervectors and save\n",
            "# of channels of data 1\n",
            "# of training samples and test samples 60000 10000\n",
            "Encoding with random fourier features encoder.\n",
            "the threshold to discretize fourier features to group elements tensor([-1.8629, -1.5343, -1.3182, -1.1505, -1.0101, -0.8872, -0.7765, -0.6746,\n",
            "        -0.5792, -0.4888, -0.4023, -0.3187, -0.2372, -0.1573, -0.0784,  0.0000,\n",
            "         0.0784,  0.1573,  0.2372,  0.3187,  0.4023,  0.4888,  0.5792,  0.6746,\n",
            "         0.7765,  0.8872,  1.0101,  1.1505,  1.3182,  1.5343,  1.8629])\n",
            "Encoded pixels to hypervectors with size:  torch.Size([256, 10000])\n",
            "Encoding training data...\n",
            "Start encoding data\n",
            "12800 images encoded. Total time elapse = 26.83484959602356\n",
            "25600 images encoded. Total time elapse = 54.309898138046265\n",
            "38400 images encoded. Total time elapse = 82.32806849479675\n",
            "51200 images encoded. Total time elapse = 111.32183146476746\n",
            "Finish encoding data\n",
            "Encoding test data...\n",
            "Start encoding data\n",
            "Finish encoding data\n",
            "Finish encoding and saving\n",
            "Optimizing class representatives for 1 epochs\n",
            "Loading encoded training data...\n",
            "Loading encoded test data...\n",
            "Size of encoded training data torch.Size([60000, 10000]) and test data torch.Size([10000, 10000])\n",
            "Epoch: 1\n",
            "49 1.1739 68.75\n",
            "99 1.2914 56.25\n",
            "149 1.0656 50.0\n",
            "199 0.3002 93.75\n",
            "249 0.2123 93.75\n",
            "299 0.6721 75.0\n",
            "349 0.8759 75.0\n",
            "399 0.5760 75.0\n",
            "449 0.4057 68.75\n",
            "499 0.4546 81.25\n",
            "549 0.2484 87.5\n",
            "599 0.3119 93.75\n",
            "649 0.3156 87.5\n",
            "699 0.1706 100.0\n",
            "749 0.1390 93.75\n",
            "799 0.3476 87.5\n",
            "849 0.2179 93.75\n",
            "899 0.2980 87.5\n",
            "949 0.4009 81.25\n",
            "999 0.4243 81.25\n",
            "1049 0.4271 75.0\n",
            "1099 0.4016 75.0\n",
            "1149 0.3019 93.75\n",
            "1199 0.5671 87.5\n",
            "1249 0.4320 81.25\n",
            "1299 0.3984 87.5\n",
            "1349 0.1110 100.0\n",
            "1399 0.5448 81.25\n",
            "1449 0.6812 75.0\n",
            "1499 0.1399 93.75\n",
            "1549 0.4827 68.75\n",
            "1599 0.1992 93.75\n",
            "1649 0.4533 75.0\n",
            "1699 0.2093 93.75\n",
            "1749 0.4087 87.5\n",
            "1799 0.3033 93.75\n",
            "1849 0.0576 100.0\n",
            "1899 0.3790 87.5\n",
            "1949 0.1002 100.0\n",
            "1999 0.4204 93.75\n",
            "2049 0.1732 93.75\n",
            "2099 0.7993 75.0\n",
            "2149 0.2524 87.5\n",
            "2199 0.5141 81.25\n",
            "2249 0.2386 87.5\n",
            "2299 0.2718 93.75\n",
            "2349 0.4710 81.25\n",
            "2399 0.5835 81.25\n",
            "2449 0.6659 87.5\n",
            "2499 0.1839 93.75\n",
            "2549 0.3253 93.75\n",
            "2599 0.4198 81.25\n",
            "2649 0.3061 87.5\n",
            "2699 0.7122 81.25\n",
            "2749 0.2224 93.75\n",
            "2799 0.1078 100.0\n",
            "2849 0.1117 100.0\n",
            "2899 0.2601 87.5\n",
            "2949 0.1271 100.0\n",
            "2999 0.4811 75.0\n",
            "3049 0.3903 87.5\n",
            "3099 0.5054 68.75\n",
            "3149 0.4739 87.5\n",
            "3199 0.1539 93.75\n",
            "3249 0.2159 93.75\n",
            "3299 0.4332 81.25\n",
            "3349 0.5324 75.0\n",
            "3399 0.6152 81.25\n",
            "3449 0.0898 93.75\n",
            "3499 0.2277 93.75\n",
            "3549 0.5994 75.0\n",
            "3599 0.1189 100.0\n",
            "3649 0.4316 81.25\n",
            "3699 0.4352 87.5\n",
            "3749 0.1020 93.75\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0227, Accuracy: 8702/10000 (87.02%)\n",
            "\n",
            "--- 17.311539888381958 seconds ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e894922b-1db2-4979-d3da-90861f82e3f2",
        "id": "E4NJ0G1BZ-GA"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoded data folder already exists\n",
            "Optimizing class representatives for 10 epochs\n",
            "Loading encoded training data...\n",
            "Loading encoded test data...\n",
            "Size of encoded training data torch.Size([60000, 10000]) and test data torch.Size([10000, 10000])\n",
            "Epoch: 1\n",
            "49 1.1208 81.25\n",
            "99 0.8713 62.5\n",
            "149 0.8456 62.5\n",
            "199 0.6049 81.25\n",
            "249 0.4636 87.5\n",
            "299 0.4735 81.25\n",
            "349 0.3627 87.5\n",
            "399 0.4051 87.5\n",
            "449 0.4054 93.75\n",
            "499 0.1770 93.75\n",
            "549 0.3956 93.75\n",
            "599 0.3039 81.25\n",
            "649 0.1670 93.75\n",
            "699 0.2109 100.0\n",
            "749 0.5622 75.0\n",
            "799 0.4233 87.5\n",
            "849 0.0832 100.0\n",
            "899 0.3651 81.25\n",
            "949 0.1344 93.75\n",
            "999 0.4265 87.5\n",
            "1049 0.5765 75.0\n",
            "1099 0.5031 87.5\n",
            "1149 0.1599 87.5\n",
            "1199 0.2276 93.75\n",
            "1249 0.1941 87.5\n",
            "1299 0.1275 100.0\n",
            "1349 0.1697 87.5\n",
            "1399 0.2396 93.75\n",
            "1449 0.2182 87.5\n",
            "1499 0.3913 75.0\n",
            "1549 0.5966 87.5\n",
            "1599 0.4244 81.25\n",
            "1649 0.3110 81.25\n",
            "1699 0.3865 81.25\n",
            "1749 0.1692 93.75\n",
            "1799 0.2767 93.75\n",
            "1849 0.2255 93.75\n",
            "1899 0.4583 75.0\n",
            "1949 0.2945 87.5\n",
            "1999 0.3203 87.5\n",
            "2049 0.4123 87.5\n",
            "2099 0.4127 87.5\n",
            "2149 0.1529 100.0\n",
            "2199 0.4361 93.75\n",
            "2249 0.3397 81.25\n",
            "2299 0.3641 81.25\n",
            "2349 0.5809 75.0\n",
            "2399 0.1378 93.75\n",
            "2449 0.3891 81.25\n",
            "2499 0.4141 75.0\n",
            "2549 0.3432 87.5\n",
            "2599 0.5873 75.0\n",
            "2649 0.2514 87.5\n",
            "2699 0.1320 93.75\n",
            "2749 0.0676 100.0\n",
            "2799 0.0793 93.75\n",
            "2849 0.2437 87.5\n",
            "2899 0.5182 87.5\n",
            "2949 0.7358 75.0\n",
            "2999 0.3997 87.5\n",
            "3049 0.7712 87.5\n",
            "3099 0.2652 81.25\n",
            "3149 0.3411 75.0\n",
            "3199 0.2944 87.5\n",
            "3249 0.1713 93.75\n",
            "3299 0.1894 87.5\n",
            "3349 0.5877 75.0\n",
            "3399 0.4183 81.25\n",
            "3449 0.5943 81.25\n",
            "3499 0.3376 87.5\n",
            "3549 0.4037 93.75\n",
            "3599 0.1298 100.0\n",
            "3649 0.6117 75.0\n",
            "3699 0.0596 100.0\n",
            "3749 0.2147 87.5\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0215, Accuracy: 8716/10000 (87.16%)\n",
            "\n",
            "--- 17.082261562347412 seconds ---\n",
            "Epoch: 2\n",
            "49 0.2011 93.75\n",
            "99 0.1985 87.5\n",
            "149 0.2847 81.25\n",
            "199 0.2668 87.5\n",
            "249 0.3157 81.25\n",
            "299 0.1385 87.5\n",
            "349 0.7568 75.0\n",
            "399 0.1691 100.0\n",
            "449 0.3508 87.5\n",
            "499 0.3313 87.5\n",
            "549 0.1056 100.0\n",
            "599 0.4110 87.5\n",
            "649 0.3891 87.5\n",
            "699 0.2812 87.5\n",
            "749 0.3235 87.5\n",
            "799 0.3213 87.5\n",
            "849 0.2915 93.75\n",
            "899 0.4498 93.75\n",
            "949 0.5742 81.25\n",
            "999 0.3762 93.75\n",
            "1049 0.1083 100.0\n",
            "1099 0.2180 93.75\n",
            "1149 0.2085 87.5\n",
            "1199 0.1273 100.0\n",
            "1249 0.5861 81.25\n",
            "1299 0.3187 87.5\n",
            "1349 0.1852 93.75\n",
            "1399 0.0483 100.0\n",
            "1449 0.2309 93.75\n",
            "1499 0.0936 100.0\n",
            "1549 0.2504 87.5\n",
            "1599 0.7073 68.75\n",
            "1649 0.1165 93.75\n",
            "1699 0.0920 100.0\n",
            "1749 0.2833 87.5\n",
            "1799 0.1920 87.5\n",
            "1849 0.3503 75.0\n",
            "1899 0.2970 93.75\n",
            "1949 0.5440 81.25\n",
            "1999 0.4979 75.0\n",
            "2049 0.0900 100.0\n",
            "2099 0.0768 100.0\n",
            "2149 0.3290 93.75\n",
            "2199 0.2305 87.5\n",
            "2249 0.2097 87.5\n",
            "2299 0.1107 93.75\n",
            "2349 0.1671 93.75\n",
            "2399 0.3559 87.5\n",
            "2449 0.1410 93.75\n",
            "2499 0.4267 68.75\n",
            "2549 0.1135 100.0\n",
            "2599 0.0934 100.0\n",
            "2649 0.1839 87.5\n",
            "2699 0.1858 93.75\n",
            "2749 0.0676 100.0\n",
            "2799 0.1355 93.75\n",
            "2849 0.3499 81.25\n",
            "2899 0.4563 87.5\n",
            "2949 0.3620 81.25\n",
            "2999 0.4606 81.25\n",
            "3049 0.2033 93.75\n",
            "3099 0.3125 93.75\n",
            "3149 0.3690 87.5\n",
            "3199 0.0997 93.75\n",
            "3249 0.2244 93.75\n",
            "3299 0.0799 100.0\n",
            "3349 0.2912 87.5\n",
            "3399 0.1804 93.75\n",
            "3449 0.0896 100.0\n",
            "3499 0.2604 87.5\n",
            "3549 0.2244 93.75\n",
            "3599 0.1615 93.75\n",
            "3649 0.2182 93.75\n",
            "3699 0.3370 81.25\n",
            "3749 0.2022 93.75\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0212, Accuracy: 8749/10000 (87.49%)\n",
            "\n",
            "--- 17.93140697479248 seconds ---\n",
            "Epoch: 3\n",
            "49 0.2416 87.5\n",
            "99 0.1540 93.75\n",
            "149 0.2151 93.75\n",
            "199 0.1135 100.0\n",
            "249 0.0921 100.0\n",
            "299 0.3842 81.25\n",
            "349 0.0514 100.0\n",
            "399 0.1785 87.5\n",
            "449 0.1846 93.75\n",
            "499 0.0153 100.0\n",
            "549 0.0147 100.0\n",
            "599 0.1820 93.75\n",
            "649 0.0389 100.0\n",
            "699 0.0907 100.0\n",
            "749 0.1054 93.75\n",
            "799 0.2542 87.5\n",
            "849 0.0725 100.0\n",
            "899 0.2193 93.75\n",
            "949 0.0916 100.0\n",
            "999 0.1680 93.75\n",
            "1049 0.9975 81.25\n",
            "1099 0.1748 93.75\n",
            "1149 0.1521 93.75\n",
            "1199 0.3660 87.5\n",
            "1249 0.1678 93.75\n",
            "1299 0.1805 93.75\n",
            "1349 0.0242 100.0\n",
            "1399 0.1727 93.75\n",
            "1449 0.3455 87.5\n",
            "1499 0.5313 87.5\n",
            "1549 0.3212 81.25\n",
            "1599 0.1676 93.75\n",
            "1649 0.2425 81.25\n",
            "1699 0.3884 93.75\n",
            "1749 0.2281 93.75\n",
            "1799 0.4917 93.75\n",
            "1849 0.1212 93.75\n",
            "1899 0.0920 93.75\n",
            "1949 0.2160 87.5\n",
            "1999 0.0516 100.0\n",
            "2049 0.1901 93.75\n",
            "2099 0.0645 100.0\n",
            "2149 0.0222 100.0\n",
            "2199 0.3005 81.25\n",
            "2249 0.0980 100.0\n",
            "2299 0.0959 100.0\n",
            "2349 0.1099 100.0\n",
            "2399 0.0942 93.75\n",
            "2449 0.3468 87.5\n",
            "2499 0.2128 93.75\n",
            "2549 0.2822 87.5\n",
            "2599 0.1203 93.75\n",
            "2649 0.3343 87.5\n",
            "2699 0.0469 100.0\n",
            "2749 0.1238 93.75\n",
            "2799 0.1910 93.75\n",
            "2849 0.0164 100.0\n",
            "2899 0.5320 81.25\n",
            "2949 0.0788 93.75\n",
            "2999 0.1238 93.75\n",
            "3049 0.2357 93.75\n",
            "3099 0.3591 87.5\n",
            "3149 0.1150 100.0\n",
            "3199 0.4163 87.5\n",
            "3249 0.2784 81.25\n",
            "3299 0.2832 93.75\n",
            "3349 0.0200 100.0\n",
            "3399 0.1128 93.75\n",
            "3449 0.2528 87.5\n",
            "3499 0.1229 100.0\n",
            "3549 0.1258 100.0\n",
            "3599 0.2971 87.5\n",
            "3649 0.1049 93.75\n",
            "3699 0.2840 87.5\n",
            "3749 0.1595 87.5\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0225, Accuracy: 8744/10000 (87.44%)\n",
            "\n",
            "--- 17.116889476776123 seconds ---\n",
            "Epoch: 4\n",
            "49 0.0578 100.0\n",
            "99 0.0453 100.0\n",
            "149 0.5759 68.75\n",
            "199 0.0418 100.0\n",
            "249 0.6315 87.5\n",
            "299 0.0751 100.0\n",
            "349 0.0673 100.0\n",
            "399 0.0389 100.0\n",
            "449 0.0399 100.0\n",
            "499 0.0491 100.0\n",
            "549 0.0983 93.75\n",
            "599 0.1676 87.5\n",
            "649 0.0701 100.0\n",
            "699 0.0095 100.0\n",
            "749 0.0611 93.75\n",
            "799 0.3358 87.5\n",
            "849 0.0418 100.0\n",
            "899 0.1334 93.75\n",
            "949 0.1127 93.75\n",
            "999 0.0601 100.0\n",
            "1049 0.0660 93.75\n",
            "1099 0.3987 87.5\n",
            "1149 0.0891 93.75\n",
            "1199 0.0391 100.0\n",
            "1249 0.3690 87.5\n",
            "1299 0.0505 100.0\n",
            "1349 0.0649 100.0\n",
            "1399 0.3700 87.5\n",
            "1449 0.0689 100.0\n",
            "1499 0.0611 93.75\n",
            "1549 0.2590 75.0\n",
            "1599 0.2667 93.75\n",
            "1649 0.5387 81.25\n",
            "1699 0.0620 93.75\n",
            "1749 0.3992 93.75\n",
            "1799 0.1165 93.75\n",
            "1849 0.1209 100.0\n",
            "1899 0.1219 93.75\n",
            "1949 0.0624 100.0\n",
            "1999 0.1555 93.75\n",
            "2049 0.1225 93.75\n",
            "2099 0.2787 87.5\n",
            "2149 0.1503 93.75\n",
            "2199 0.1201 93.75\n",
            "2249 0.1558 93.75\n",
            "2299 0.0434 100.0\n",
            "2349 0.2272 87.5\n",
            "2399 0.1754 93.75\n",
            "2449 0.0856 93.75\n",
            "2499 0.1684 93.75\n",
            "2549 0.1021 93.75\n",
            "2599 0.3067 93.75\n",
            "2649 0.1145 93.75\n",
            "2699 0.2720 87.5\n",
            "2749 0.0931 100.0\n",
            "2799 0.0220 100.0\n",
            "2849 0.0606 93.75\n",
            "2899 0.3430 81.25\n",
            "2949 0.0935 100.0\n",
            "2999 0.0208 100.0\n",
            "3049 0.1367 93.75\n",
            "3099 0.2067 87.5\n",
            "3149 0.0679 100.0\n",
            "3199 0.0056 100.0\n",
            "3249 0.1598 93.75\n",
            "3299 0.0451 100.0\n",
            "3349 0.0700 100.0\n",
            "3399 0.1868 93.75\n",
            "3449 0.0528 100.0\n",
            "3499 0.2766 87.5\n",
            "3549 0.1393 87.5\n",
            "3599 0.1838 93.75\n",
            "3649 0.1820 93.75\n",
            "3699 0.0066 100.0\n",
            "3749 0.1642 93.75\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0229, Accuracy: 8765/10000 (87.65%)\n",
            "\n",
            "--- 17.214826107025146 seconds ---\n",
            "Epoch: 5\n",
            "49 0.0533 100.0\n",
            "99 0.0155 100.0\n",
            "149 0.3702 93.75\n",
            "199 0.0631 100.0\n",
            "249 0.0685 100.0\n",
            "299 0.6290 93.75\n",
            "349 0.0601 100.0\n",
            "399 0.0995 100.0\n",
            "449 0.1606 87.5\n",
            "499 0.0279 100.0\n",
            "549 0.0117 100.0\n",
            "599 0.1243 93.75\n",
            "649 0.1117 93.75\n",
            "699 0.1206 93.75\n",
            "749 0.3252 93.75\n",
            "799 0.1537 93.75\n",
            "849 0.0093 100.0\n",
            "899 0.0049 100.0\n",
            "949 0.4609 81.25\n",
            "999 0.2554 87.5\n",
            "1049 0.1665 93.75\n",
            "1099 0.3539 87.5\n",
            "1149 0.0680 93.75\n",
            "1199 0.2626 87.5\n",
            "1249 0.0924 100.0\n",
            "1299 0.0144 100.0\n",
            "1349 0.0581 100.0\n",
            "1399 0.0768 100.0\n",
            "1449 0.1296 100.0\n",
            "1499 0.1323 93.75\n",
            "1549 0.0361 100.0\n",
            "1599 0.5231 68.75\n",
            "1649 0.2140 93.75\n",
            "1699 0.0781 93.75\n",
            "1749 0.0430 100.0\n",
            "1799 0.1949 93.75\n",
            "1849 0.0847 100.0\n",
            "1899 0.1361 87.5\n",
            "1949 0.0308 100.0\n",
            "1999 0.0372 100.0\n",
            "2049 0.3403 87.5\n",
            "2099 0.0687 93.75\n",
            "2149 0.1254 93.75\n",
            "2199 0.1360 93.75\n",
            "2249 0.0254 100.0\n",
            "2299 0.1880 93.75\n",
            "2349 0.0297 100.0\n",
            "2399 0.0116 100.0\n",
            "2449 0.3608 87.5\n",
            "2499 0.0152 100.0\n",
            "2549 0.2830 87.5\n",
            "2599 0.0229 100.0\n",
            "2649 0.4924 87.5\n",
            "2699 0.0322 100.0\n",
            "2749 0.1729 93.75\n",
            "2799 0.0515 93.75\n",
            "2849 0.1035 100.0\n",
            "2899 0.3746 87.5\n",
            "2949 0.0901 93.75\n",
            "2999 0.2454 87.5\n",
            "3049 0.2104 93.75\n",
            "3099 0.1992 93.75\n",
            "3149 0.2869 93.75\n",
            "3199 0.0449 100.0\n",
            "3249 0.0081 100.0\n",
            "3299 0.1040 100.0\n",
            "3349 0.0064 100.0\n",
            "3399 0.0017 100.0\n",
            "3449 0.5285 81.25\n",
            "3499 0.1658 87.5\n",
            "3549 0.0093 100.0\n",
            "3599 0.0406 100.0\n",
            "3649 0.1641 93.75\n",
            "3699 0.0884 100.0\n",
            "3749 0.0436 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0247, Accuracy: 8761/10000 (87.61%)\n",
            "\n",
            "--- 18.05231785774231 seconds ---\n",
            "Epoch: 6\n",
            "49 0.1031 93.75\n",
            "99 0.2268 87.5\n",
            "149 0.1975 93.75\n",
            "199 0.0783 100.0\n",
            "249 0.0671 100.0\n",
            "299 0.0402 100.0\n",
            "349 0.0361 100.0\n",
            "399 0.0834 100.0\n",
            "449 0.0445 100.0\n",
            "499 0.2791 87.5\n",
            "549 0.0218 100.0\n",
            "599 0.0243 100.0\n",
            "649 0.1167 93.75\n",
            "699 0.1754 93.75\n",
            "749 0.1371 93.75\n",
            "799 0.1156 87.5\n",
            "849 0.2081 87.5\n",
            "899 0.1654 93.75\n",
            "949 0.1860 93.75\n",
            "999 0.0330 100.0\n",
            "1049 0.1958 93.75\n",
            "1099 0.0321 100.0\n",
            "1149 0.2048 93.75\n",
            "1199 0.0352 100.0\n",
            "1249 0.1667 93.75\n",
            "1299 0.0799 93.75\n",
            "1349 0.0919 93.75\n",
            "1399 0.1054 93.75\n",
            "1449 0.0093 100.0\n",
            "1499 0.2145 87.5\n",
            "1549 0.2331 87.5\n",
            "1599 0.1225 93.75\n",
            "1649 0.0708 93.75\n",
            "1699 0.0143 100.0\n",
            "1749 0.1482 93.75\n",
            "1799 0.1592 93.75\n",
            "1849 0.0740 93.75\n",
            "1899 0.0293 100.0\n",
            "1949 0.2019 93.75\n",
            "1999 0.3726 81.25\n",
            "2049 0.1247 93.75\n",
            "2099 0.0296 100.0\n",
            "2149 0.0017 100.0\n",
            "2199 0.3783 87.5\n",
            "2249 0.0981 100.0\n",
            "2299 0.2713 93.75\n",
            "2349 0.1460 93.75\n",
            "2399 0.0492 100.0\n",
            "2449 0.0638 100.0\n",
            "2499 0.0056 100.0\n",
            "2549 0.2367 87.5\n",
            "2599 0.1174 93.75\n",
            "2649 0.3217 93.75\n",
            "2699 0.0618 100.0\n",
            "2749 0.3513 93.75\n",
            "2799 0.0939 100.0\n",
            "2849 0.0289 100.0\n",
            "2899 0.1820 87.5\n",
            "2949 0.0530 100.0\n",
            "2999 0.0384 100.0\n",
            "3049 0.4996 87.5\n",
            "3099 0.0063 100.0\n",
            "3149 0.0332 100.0\n",
            "3199 0.1135 93.75\n",
            "3249 0.0324 100.0\n",
            "3299 0.1158 100.0\n",
            "3349 0.0609 100.0\n",
            "3399 0.0275 100.0\n",
            "3449 0.3471 81.25\n",
            "3499 0.0728 100.0\n",
            "3549 0.0667 100.0\n",
            "3599 0.0569 100.0\n",
            "3649 0.1712 87.5\n",
            "3699 0.0038 100.0\n",
            "3749 0.0676 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0264, Accuracy: 8784/10000 (87.84%)\n",
            "\n",
            "--- 17.327093839645386 seconds ---\n",
            "Epoch: 7\n",
            "49 0.0240 100.0\n",
            "99 0.0440 100.0\n",
            "149 0.0526 100.0\n",
            "199 0.1255 93.75\n",
            "249 0.1455 93.75\n",
            "299 0.0257 100.0\n",
            "349 0.0353 100.0\n",
            "399 0.0203 100.0\n",
            "449 0.0501 100.0\n",
            "499 0.0096 100.0\n",
            "549 0.2237 93.75\n",
            "599 0.1795 93.75\n",
            "649 0.0612 100.0\n",
            "699 0.0592 100.0\n",
            "749 0.1077 93.75\n",
            "799 0.1642 93.75\n",
            "849 0.3485 87.5\n",
            "899 0.2192 93.75\n",
            "949 0.0593 100.0\n",
            "999 0.0679 100.0\n",
            "1049 0.0163 100.0\n",
            "1099 0.0259 100.0\n",
            "1149 0.0182 100.0\n",
            "1199 0.0351 100.0\n",
            "1249 0.0797 93.75\n",
            "1299 0.1679 93.75\n",
            "1349 0.0353 100.0\n",
            "1399 0.0570 100.0\n",
            "1449 0.1444 93.75\n",
            "1499 0.0075 100.0\n",
            "1549 0.0206 100.0\n",
            "1599 0.1134 93.75\n",
            "1649 0.1108 93.75\n",
            "1699 0.0211 100.0\n",
            "1749 0.2907 93.75\n",
            "1799 0.0933 93.75\n",
            "1849 0.0203 100.0\n",
            "1899 0.1062 100.0\n",
            "1949 0.2346 87.5\n",
            "1999 0.1666 93.75\n",
            "2049 0.0298 100.0\n",
            "2099 0.0922 93.75\n",
            "2149 0.0065 100.0\n",
            "2199 0.0420 100.0\n",
            "2249 0.3453 87.5\n",
            "2299 0.0192 100.0\n",
            "2349 0.0176 100.0\n",
            "2399 0.0052 100.0\n",
            "2449 0.2612 93.75\n",
            "2499 0.2240 87.5\n",
            "2549 0.2314 87.5\n",
            "2599 0.1812 93.75\n",
            "2649 0.4663 81.25\n",
            "2699 0.2147 93.75\n",
            "2749 0.0030 100.0\n",
            "2799 0.0071 100.0\n",
            "2849 0.0992 93.75\n",
            "2899 0.3666 81.25\n",
            "2949 0.0356 100.0\n",
            "2999 0.0020 100.0\n",
            "3049 0.1186 93.75\n",
            "3099 0.0487 100.0\n",
            "3149 0.0159 100.0\n",
            "3199 0.1294 93.75\n",
            "3249 0.0836 100.0\n",
            "3299 0.2482 93.75\n",
            "3349 0.1407 100.0\n",
            "3399 0.2523 87.5\n",
            "3449 0.0539 100.0\n",
            "3499 0.1982 93.75\n",
            "3549 0.4189 75.0\n",
            "3599 0.0564 100.0\n",
            "3649 0.0242 100.0\n",
            "3699 0.1192 100.0\n",
            "3749 0.1858 93.75\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0313, Accuracy: 8683/10000 (86.83%)\n",
            "\n",
            "--- 17.52048349380493 seconds ---\n",
            "Epoch: 8\n",
            "49 0.0659 100.0\n",
            "99 0.0471 100.0\n",
            "149 0.0648 100.0\n",
            "199 0.0039 100.0\n",
            "249 0.0223 100.0\n",
            "299 0.0123 100.0\n",
            "349 0.0233 100.0\n",
            "399 0.0702 100.0\n",
            "449 0.0019 100.0\n",
            "499 0.0957 100.0\n",
            "549 0.0219 100.0\n",
            "599 0.1291 93.75\n",
            "649 0.0033 100.0\n",
            "699 0.1171 93.75\n",
            "749 0.0903 93.75\n",
            "799 0.0475 100.0\n",
            "849 0.1307 87.5\n",
            "899 0.0515 100.0\n",
            "949 0.0158 100.0\n",
            "999 0.2093 87.5\n",
            "1049 0.2440 87.5\n",
            "1099 0.1331 87.5\n",
            "1149 0.0108 100.0\n",
            "1199 0.0007 100.0\n",
            "1249 0.0372 100.0\n",
            "1299 0.0891 100.0\n",
            "1349 0.1030 93.75\n",
            "1399 0.0276 100.0\n",
            "1449 0.0924 93.75\n",
            "1499 0.0048 100.0\n",
            "1549 0.1535 93.75\n",
            "1599 0.0565 100.0\n",
            "1649 0.4061 93.75\n",
            "1699 0.0393 100.0\n",
            "1749 0.1424 93.75\n",
            "1799 0.0942 93.75\n",
            "1849 0.1452 93.75\n",
            "1899 0.0616 93.75\n",
            "1949 0.0416 100.0\n",
            "1999 0.0013 100.0\n",
            "2049 0.0395 100.0\n",
            "2099 0.0111 100.0\n",
            "2149 0.0155 100.0\n",
            "2199 0.0295 100.0\n",
            "2249 0.2511 93.75\n",
            "2299 0.0823 100.0\n",
            "2349 0.4882 87.5\n",
            "2399 0.0009 100.0\n",
            "2449 0.2649 87.5\n",
            "2499 0.0660 100.0\n",
            "2549 0.0897 93.75\n",
            "2599 0.0265 100.0\n",
            "2649 0.0645 100.0\n",
            "2699 0.2758 93.75\n",
            "2749 0.0056 100.0\n",
            "2799 0.0106 100.0\n",
            "2849 0.1465 93.75\n",
            "2899 0.0667 100.0\n",
            "2949 0.0749 93.75\n",
            "2999 0.0027 100.0\n",
            "3049 0.1028 93.75\n",
            "3099 0.0494 93.75\n",
            "3149 0.0800 93.75\n",
            "3199 0.0991 93.75\n",
            "3249 0.0252 100.0\n",
            "3299 0.6333 87.5\n",
            "3349 0.0186 100.0\n",
            "3399 0.1964 93.75\n",
            "3449 0.1129 93.75\n",
            "3499 0.0331 100.0\n",
            "3549 0.0386 100.0\n",
            "3599 0.0945 93.75\n",
            "3649 0.1291 93.75\n",
            "3699 0.0210 100.0\n",
            "3749 0.4742 87.5\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0354, Accuracy: 8615/10000 (86.15%)\n",
            "\n",
            "--- 18.1940975189209 seconds ---\n",
            "Epoch: 9\n",
            "49 0.0019 100.0\n",
            "99 0.0502 100.0\n",
            "149 0.0372 100.0\n",
            "199 0.2548 93.75\n",
            "249 0.2124 87.5\n",
            "299 0.0184 100.0\n",
            "349 0.0689 100.0\n",
            "399 0.0236 100.0\n",
            "449 0.1226 87.5\n",
            "499 0.0001 100.0\n",
            "549 0.0259 100.0\n",
            "599 0.0210 100.0\n",
            "649 0.0211 100.0\n",
            "699 0.0208 100.0\n",
            "749 0.0021 100.0\n",
            "799 0.1070 93.75\n",
            "849 0.0207 100.0\n",
            "899 0.0571 100.0\n",
            "949 0.0242 100.0\n",
            "999 0.0130 100.0\n",
            "1049 0.0947 93.75\n",
            "1099 0.0114 100.0\n",
            "1149 0.1324 93.75\n",
            "1199 0.0307 100.0\n",
            "1249 0.0998 93.75\n",
            "1299 0.0337 100.0\n",
            "1349 0.0593 93.75\n",
            "1399 0.1098 87.5\n",
            "1449 0.0077 100.0\n",
            "1499 0.0023 100.0\n",
            "1549 0.0219 100.0\n",
            "1599 0.1734 93.75\n",
            "1649 0.1913 87.5\n",
            "1699 0.0526 100.0\n",
            "1749 0.1779 87.5\n",
            "1799 0.1093 93.75\n",
            "1849 0.2401 93.75\n",
            "1899 0.0094 100.0\n",
            "1949 0.0153 100.0\n",
            "1999 0.0039 100.0\n",
            "2049 0.0181 100.0\n",
            "2099 0.0275 100.0\n",
            "2149 0.2130 93.75\n",
            "2199 0.2978 93.75\n",
            "2249 0.3129 81.25\n",
            "2299 0.2561 87.5\n",
            "2349 0.0104 100.0\n",
            "2399 0.3859 87.5\n",
            "2449 0.2149 93.75\n",
            "2499 0.0487 100.0\n",
            "2549 0.0686 93.75\n",
            "2599 0.0195 100.0\n",
            "2649 0.2758 93.75\n",
            "2699 0.0045 100.0\n",
            "2749 0.1992 93.75\n",
            "2799 0.2650 93.75\n",
            "2849 0.0437 93.75\n",
            "2899 0.3284 81.25\n",
            "2949 0.1442 93.75\n",
            "2999 0.0148 100.0\n",
            "3049 0.0937 100.0\n",
            "3099 0.0145 100.0\n",
            "3149 0.0050 100.0\n",
            "3199 0.1911 93.75\n",
            "3249 0.0021 100.0\n",
            "3299 0.0019 100.0\n",
            "3349 0.0174 100.0\n",
            "3399 0.0058 100.0\n",
            "3449 0.0353 100.0\n",
            "3499 0.1458 93.75\n",
            "3549 0.0570 93.75\n",
            "3599 0.1695 93.75\n",
            "3649 0.1579 93.75\n",
            "3699 0.0105 100.0\n",
            "3749 0.0919 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0367, Accuracy: 8620/10000 (86.20%)\n",
            "\n",
            "--- 17.342796802520752 seconds ---\n",
            "Epoch: 10\n",
            "49 0.0201 100.0\n",
            "99 0.0062 100.0\n",
            "149 0.0137 100.0\n",
            "199 0.0007 100.0\n",
            "249 0.0113 100.0\n",
            "299 0.0194 100.0\n",
            "349 0.1057 93.75\n",
            "399 0.0553 93.75\n",
            "449 0.0907 93.75\n",
            "499 0.0226 100.0\n",
            "549 0.1241 93.75\n",
            "599 0.0047 100.0\n",
            "649 0.0381 100.0\n",
            "699 0.0285 100.0\n",
            "749 0.0850 93.75\n",
            "799 0.0588 93.75\n",
            "849 0.1153 93.75\n",
            "899 0.0550 93.75\n",
            "949 0.0231 100.0\n",
            "999 0.0101 100.0\n",
            "1049 0.0493 100.0\n",
            "1099 0.0750 93.75\n",
            "1149 0.0774 93.75\n",
            "1199 0.0914 93.75\n",
            "1249 0.0038 100.0\n",
            "1299 0.0016 100.0\n",
            "1349 0.0514 100.0\n",
            "1399 0.0200 100.0\n",
            "1449 0.0161 100.0\n",
            "1499 0.0104 100.0\n",
            "1549 0.0189 100.0\n",
            "1599 0.2778 75.0\n",
            "1649 0.0147 100.0\n",
            "1699 0.2821 93.75\n",
            "1749 0.0659 100.0\n",
            "1799 0.0665 100.0\n",
            "1849 0.0352 100.0\n",
            "1899 0.0310 100.0\n",
            "1949 0.0007 100.0\n",
            "1999 0.0582 100.0\n",
            "2049 0.0046 100.0\n",
            "2099 0.0053 100.0\n",
            "2149 0.0001 100.0\n",
            "2199 0.0002 100.0\n",
            "2249 0.0747 93.75\n",
            "2299 0.0099 100.0\n",
            "2349 0.0274 100.0\n",
            "2399 0.0272 100.0\n",
            "2449 0.0440 100.0\n",
            "2499 0.0671 100.0\n",
            "2549 0.0211 100.0\n",
            "2599 0.0377 100.0\n",
            "2649 0.0218 100.0\n",
            "2699 0.0856 93.75\n",
            "2749 0.0456 100.0\n",
            "2799 0.0477 100.0\n",
            "2849 0.0088 100.0\n",
            "2899 0.0451 100.0\n",
            "2949 0.0439 100.0\n",
            "2999 0.0093 100.0\n",
            "3049 0.0474 93.75\n",
            "3099 0.0656 100.0\n",
            "3149 0.1620 93.75\n",
            "3199 0.0201 100.0\n",
            "3249 0.3230 93.75\n",
            "3299 0.0788 100.0\n",
            "3349 0.0521 100.0\n",
            "3399 0.0060 100.0\n",
            "3449 0.0446 100.0\n",
            "3499 0.0656 100.0\n",
            "3549 0.1193 93.75\n",
            "3599 0.0699 100.0\n",
            "3649 0.0333 100.0\n",
            "3699 0.1975 93.75\n",
            "3749 0.0375 100.0\n",
            "Start testing on test set\n",
            "\n",
            "Testing Average loss: 0.0377, Accuracy: 8686/10000 (86.86%)\n",
            "\n",
            "--- 17.809141635894775 seconds ---\n"
          ]
        }
      ],
      "source": [
        "#fminst epoch 10 rffg32vsa\n",
        "class ArgumentParser:\n",
        "    def __init__(self):\n",
        "        self.args = {\n",
        "            'lr': 0.01,            # Learning rate for optimizing class representative\n",
        "            'gamma': 0.3,          # Kernel parameter for computing covariance\n",
        "            'epoch': 10,            # Epochs of training\n",
        "            'gorder': 32,           # Order of the cyclic group required for G-VSA (8 ou 16)\n",
        "            'dim': 10000,          # Dimension of hypervectors c le grand D\n",
        "            'seed': 43,            # Random seed for reproducing results\n",
        "            'resume': True,       # Resume from existing encoded hypervectors\n",
        "            'data_dir': './encoded_data', # Directory to save encoded data (hypervectors)\n",
        "            'dataset': 'fmnist',    # Dataset choice choices=['mnist', 'fmnist', 'cifar', 'isolet', 'ucihar']\n",
        "            'raw_data_dir': './dataset',   # Raw data directory\n",
        "            'model': 'rff-gvsa'    # Model type choices=['rff-hdc', 'linear-hdc', 'rff-gvsa']\n",
        "        }\n",
        "        self.seed = self.args['seed']\n",
        "        self.lr = self.args['lr']\n",
        "        self.gamma = self.args['gamma']\n",
        "        self.epoch = self.args['epoch']\n",
        "        self.gorder = self.args['gorder']\n",
        "        self.dim = self.args['dim']\n",
        "        self.resume = self.args['resume']\n",
        "        self.data_dir = self.args['data_dir']\n",
        "        self.dataset = self.args['dataset']\n",
        "        self.raw_data_dir = self.args['raw_data_dir']\n",
        "        self.model = self.args['model']\n",
        "\n",
        "\n",
        "    def get_args(self):\n",
        "        return self.args\n",
        "args = ArgumentParser()\n",
        "torch.manual_seed(args.seed)\n",
        "np.random.seed(args.seed)\n",
        "if 'hdc' in args.model:\n",
        "    args.gorder = 2\n",
        "    print(\"Use binary HDC with random fourier features, ignoring gorder, set to 2.\")\n",
        "args.data_dir = f'{args.data_dir}/{args.dataset}_{args.model}_order{args.gorder}_gamma{args.gamma}_dim{args.dim}'\n",
        "try:\n",
        "    os.makedirs(args.data_dir)\n",
        "except FileExistsError:\n",
        "    print('Encoded data folder already exists')\n",
        "if not args.resume:\n",
        "    print('Encode the dataset into hypervectors and save')\n",
        "    encode_and_save(args)\n",
        "    print('Finish encoding and saving')\n",
        "print(f'Optimizing class representatives for {args.epoch} epochs')\n",
        "train(args)"
      ]
    }
  ]
}